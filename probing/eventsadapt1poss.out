node012.cm.cluster
Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
layer 0 0.5
[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
layer 1 0.6083333333333333
[0.6666666666666666, 0.6083333333333333, 0.6666666666666666, 0.575, 0.5833333333333334, 0.5666666666666667, 0.6333333333333333, 0.6166666666666667, 0.6333333333333333, 0.5333333333333333]
layer 2 0.63
[0.6166666666666667, 0.6083333333333333, 0.65, 0.5916666666666667, 0.6, 0.6583333333333333, 0.6583333333333333, 0.625, 0.675, 0.6166666666666667]
layer 3 0.6716666666666666
[0.675, 0.675, 0.7083333333333334, 0.7083333333333334, 0.6666666666666666, 0.6666666666666666, 0.6833333333333333, 0.625, 0.7083333333333334, 0.6]
layer 4 0.6525
[0.625, 0.6583333333333333, 0.6416666666666667, 0.6416666666666667, 0.625, 0.65, 0.6916666666666667, 0.6916666666666667, 0.6833333333333333, 0.6166666666666667]
layer 5 0.6758333333333333
[0.6916666666666667, 0.675, 0.7166666666666667, 0.65, 0.6, 0.6916666666666667, 0.6416666666666667, 0.6666666666666666, 0.7166666666666667, 0.7083333333333334]
layer 6 0.6741666666666667
[0.6583333333333333, 0.7166666666666667, 0.675, 0.6666666666666666, 0.6416666666666667, 0.65, 0.6916666666666667, 0.675, 0.6833333333333333, 0.6833333333333333]
layer 7 0.7225
[0.75, 0.7333333333333333, 0.7583333333333333, 0.725, 0.675, 0.6833333333333333, 0.725, 0.7083333333333334, 0.7333333333333333, 0.7333333333333333]
layer 8 0.7558333333333334
[0.7833333333333333, 0.7583333333333333, 0.7166666666666667, 0.7333333333333333, 0.7916666666666666, 0.7166666666666667, 0.725, 0.8, 0.7833333333333333, 0.75]
layer 9 0.7591666666666667
[0.7666666666666667, 0.7416666666666667, 0.7666666666666667, 0.775, 0.7333333333333333, 0.7916666666666666, 0.7916666666666666, 0.7333333333333333, 0.7833333333333333, 0.7083333333333334]
layer 10 0.7825
[0.8, 0.7666666666666667, 0.7916666666666666, 0.775, 0.8166666666666667, 0.7666666666666667, 0.7833333333333333, 0.8166666666666667, 0.7583333333333333, 0.75]
layer 11 0.8091666666666667
[0.7583333333333333, 0.8916666666666667, 0.7833333333333333, 0.85, 0.7916666666666666, 0.7833333333333333, 0.825, 0.8083333333333333, 0.8333333333333334, 0.7666666666666667]
layer 12 0.7925
[0.7583333333333333, 0.7916666666666666, 0.825, 0.8, 0.7833333333333333, 0.7666666666666667, 0.8, 0.8333333333333334, 0.8166666666666667, 0.75]
layer 13 0.8075
[0.8083333333333333, 0.8166666666666667, 0.8083333333333333, 0.825, 0.775, 0.8, 0.8083333333333333, 0.8416666666666667, 0.8, 0.7916666666666666]
layer 14 0.8183333333333334
[0.825, 0.7916666666666666, 0.8083333333333333, 0.8166666666666667, 0.8416666666666667, 0.8166666666666667, 0.8666666666666667, 0.7916666666666666, 0.8083333333333333, 0.8166666666666667]
layer 15 0.8225
[0.8166666666666667, 0.8, 0.8333333333333334, 0.7833333333333333, 0.8583333333333333, 0.8833333333333333, 0.8, 0.7833333333333333, 0.825, 0.8416666666666667]
layer 16 0.8075
[0.875, 0.7916666666666666, 0.775, 0.7583333333333333, 0.8333333333333334, 0.8, 0.7833333333333333, 0.7916666666666666, 0.8333333333333334, 0.8333333333333334]
layer 17 0.8233333333333334
[0.85, 0.8083333333333333, 0.8333333333333334, 0.8, 0.7916666666666666, 0.8166666666666667, 0.8166666666666667, 0.85, 0.85, 0.8166666666666667]
layer 18 0.8125
[0.8083333333333333, 0.7916666666666666, 0.8416666666666667, 0.8, 0.825, 0.825, 0.8416666666666667, 0.8, 0.7916666666666666, 0.8]
layer 19 0.8116666666666666
[0.8166666666666667, 0.8333333333333334, 0.825, 0.85, 0.7833333333333333, 0.825, 0.8, 0.7916666666666666, 0.7583333333333333, 0.8333333333333334]
layer 20 0.7866666666666666
[0.8166666666666667, 0.7416666666666667, 0.7833333333333333, 0.8416666666666667, 0.7916666666666666, 0.825, 0.8, 0.775, 0.7416666666666667, 0.75]
layer 21 0.7941666666666667
[0.775, 0.7333333333333333, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.8333333333333334, 0.7083333333333334, 0.8083333333333333, 0.8166666666666667]
layer 22 0.77
[0.7166666666666667, 0.825, 0.775, 0.8083333333333333, 0.7416666666666667, 0.8333333333333334, 0.8083333333333333, 0.7333333333333333, 0.7416666666666667, 0.7166666666666667]
layer 23 0.7758333333333334
[0.7833333333333333, 0.75, 0.775, 0.7333333333333333, 0.75, 0.8166666666666667, 0.7916666666666666, 0.8083333333333333, 0.7583333333333333, 0.7916666666666666]
Traceback (most recent call last):
  File "bert_automatic.py", line 104, in <module>
  File "bert_automatic.py", line 95, in output
AttributeError: module 'seaborn' has no attribute 'set_theme'
