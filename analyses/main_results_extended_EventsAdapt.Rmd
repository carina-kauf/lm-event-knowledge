---
title: "results_main"
output: html_document
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(gtools)
library(cowplot)
library(png)

source('dataloader_utils.R') #includes normalizations, read_data functions
source('stats_utils.R')

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
```

### Print environment variables 
```{r}
#SET ENVIRONMENT VARIABLES

# can be: "EventsAdapt"
experiment <- "EventsAdapt"

#can be:
# llms_main = human + main LLMs
# all_main = human + main LLMs + baselines
# baselines = human + baselines
# llms_all = human + all MLMs 
which_models <- "all_main" 

#can be "min-max", "zscore", "none"
normalization_type <- "min-max" 

path <- paste("results/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste(path,experiment,"_Models=",which_models,sep='')
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

print(paste(savedir))
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization_type))

#for plotting the dotted reference line
if (grepl("min-max", normalization_type) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
normalization <- get_normalization_fn(normalization_type)
```

# READ DATA
```{r}
#created via preprocess_scores.Rmd
dat = read.csv('clean_data/clean_EventsAdapt_df.csv')
human_dat = read.csv('clean_data/clean_EventsAdapt_human_dat.csv')
```

# Select which models/metrics to plot

```{r, utils.choose_models, echo=FALSE}
human <- c("human")
llms_main <- c("RoBERTa-large.sentence-PLL", "BERT-large.sentence-PLL", "GPT-J.sentence-LL", "GPT-2-xl.sentence-LL", "MPT-30b.sentence-LL")
baselines <- c("tinyLSTM.surprisal","SDM", "thematicFit.prod", "syntax-PPMI")

llms_main <- c(human, llms_main)
all_main <- c(llms_main, baselines)
baselines <- c(human, baselines)

all_metrics = unique(dat$Metric) 
llm_metrics = as.character(all_metrics[str_detect(all_metrics, 'BERT|GPT|MPT')])
llms_all <- c(human,llm_metrics)

# select which to use
chosen_models = eval(parse(text=which_models))
dat = dat %>% filter(Metric %in% chosen_models)

if (grepl("llms_all", which_models) == FALSE){
# shorten model names
llms_main_short <- c("MPT", "GPT-J", "GPT-2", "RoBERTa", "BERT")
baselines_short <- c("tinyLSTM","SDM", "thematicFit", "syntax-PPMI")
models_short_order = c(human, llms_main_short, baselines_short)

shorten_metric_names <- function(col) { gsub("-large|-xl|-xxlarge", "", col) }

dat = dat %>%
  mutate(MetricModel = Metric, Metric=as.character(Metric)) %>%
  separate(Metric, into=c("Metric"), sep="\\.", extra="drop") %>%
  mutate(Metric = shorten_metric_names(Metric)) 
chosen_models = unique(dat$Metric)
chosen_models = chosen_models[order(match(chosen_models,models_short_order))]
dat$Metric = factor(dat$Metric, levels = chosen_models)
}else{
  dat$Metric = relevel(dat$Metric, ref="human")
  chosen_models = levels(dat$Metric)
}
message("Using these models/metrics:")
print(chosen_models)
```

# CONTRASTS
```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$TrialType = factor(dat$TrialType, levels=c("AAN", "AAR", "AI"))    # dummy coding by default
dat$Voice = as.factor(dat$Voice)
contrasts(dat$Voice) = c(0.5, -0.5)
colnames(attr(dat$Voice, "contrasts")) = c("A>P")

dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
nr_models = length(chosen_models)
ncols=nr_models #models plus human
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}

# color scheme
color_plaus = '#1b9e77'
color_voice = '#d95f02'
color_syn = '#7570b3'
cortext_size = 3

label_names <- c(
  "DTFit" = "animate-inanimate, unlikely",
  "AI" = "animate-inanimate, impossible",
  "AAN" = "animate-animate, unlikely",
  "AAR" = "animate-animate\n(control)"
)

# source: https://www.markhw.com/blog/logos

get_png <- function(filename) {
  grid::rasterGrob(png::readPNG(filename), interpolate = TRUE)
}

img_ai <- get_png("./animate-inanimate.png")
img_aa <- get_png("./animate-animate.png")
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

#add Category color
if (which_models == "llms_all") {
  dat.binchoice = dat %>%
    mutate(Metric = as.character(Metric)) %>%
    mutate(Category = ifelse(startsWith(Metric, "BERT"), "BERT", "RoBERTa")) %>%
    mutate(Category = ifelse(startsWith(Metric, "GPT"), "GPT", Category)) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category))
} else {
  dat.binchoice = dat %>%
    mutate(Category = ifelse(Metric%in%llms_main_short, "LLMs", "baselines")) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category)) 
}

dat.binchoice = dat.binchoice %>%
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"],
            Sentence=Sentence) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup()

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=chosen_models)
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("DTFit", "AI", "AAN", "AAR"))

if (which_models=="llms_all") {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "RoBERTa", "BERT", "GPT"))
} else {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "LLMs", "baselines"))
}
```

# ACTIVE VS PASSIVE

### Prep data

```{r, echo=FALSE}
if (grepl("all_main", which_models) == TRUE){
plotting_models = c('human',llms_main_short)
}else{
  plotting_models = chosen_models
}

dat.voice = dat %>%
  filter(Experiment=='EventsAdapt') %>%
  filter(Metric %in% plotting_models) %>%
  select(ItemNum, TrialType, Plausibility, Voice, Metric, FinalNormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(Voice, meanScore)
dat.voice$Metric = factor(dat.voice$Metric, levels=plotting_models)
```

### Calculate correlations

```{r}
df_correlation.voice = get_correlation_df("voice", "human", dat.voice, plotting_models)
```

### Plot

```{r}
df_correlation.voice$Metric = factor(df_correlation.voice$Metric, levels=plotting_models)
dat.voice$Metric = factor(dat.voice$Metric, levels=plotting_models)

plot.voice = ggplot(data=dat.voice)+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=active, y=passive), size=0.2, color=color_voice)+
  geom_text(mapping=aes(x=0, y=1.10, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.voice, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1), labels=c(0,0.5,1))+
  theme_classic()+
  facet_grid(~Metric)+
  ggtitle('VOICE')+
  xlab("Active sentence score")+
  ylab("Passive sentence score")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))
plot.voice
```


# SYNONYMOUS SENTENCES

### Prep data

```{r, echo=FALSE}
dat.syn = dat %>%
  filter(!is.na(SynonymPair)) %>%
  filter(Metric %in% plotting_models) %>%
  filter(Voice=="active") %>%
  dplyr::select(-ItemNum) %>%
  group_by(TrialType, NumSyn, SynonymPair, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(NumSyn, meanScore)

# remove the sentences that don't have synonyms
dat.syn = dat.syn %>%
  filter(!is.na(Version1)) %>%
  filter(!is.na(Version2))

dat.syn$Metric = factor(dat.syn$Metric, levels=plotting_models)
```

### Calculate correlations

```{r}
df_correlation.syn = get_correlation_df("synonym", "human", dat.syn, plotting_models)
```

### Plot


```{r}
df_correlation.syn$Metric = factor(df_correlation.syn$Metric, levels=plotting_models)
dat.syn$Metric = factor(dat.syn$Metric, levels=plotting_models)

plot.syn = ggplot(data=dat.syn)+
  #facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=Version1, y=Version2), size=0.2, color=color_syn)+
  geom_text(mapping=aes(x=0, y=1.10, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.syn, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1), labels=c(0,0.5,1))+
  xlab("Version 1 score")+
  ylab("Version 2 score")+
  theme_classic()+
  facet_grid(~Metric)+
  ggtitle("SYNONYMOUS SENTENCES")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))
plot.syn
```


# COMBINE

```{r}
bottom_row <- plot_grid(plot.voice, plot.syn, labels = c('A', 'B'), label_size = 15)

title <- ggdraw() + draw_label("GENERALIZATION", fontface='bold') + theme(plot.background = element_rect(fill="#F5F5F5", color=NA))
bottom_row_title <- plot_grid(title, bottom_row, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins
bottom_row_title
savename <- "voice_syn.png"
ggsave(paste(savedir,savename,sep="/"), height=6.5,width=30, units='cm')
```


# SI - passives binary accuracy

### Binary accuracy plot for passive sentences (for SI)
```{r}
# get p values
dat.binchoice.summary.passive = dat.binchoice %>%
  filter(TrialType %in% c("AI", "AAN"), Voice=="passive") %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary.passive = dat.binchoice.summary.passive %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  ungroup()
```

```{r}
# get human responses separately
human.results = dat.binchoice.summary.passive %>%
  filter(Metric=='human') %>%
  select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)

dat.binchoice.summary.withchisq = merge(dat.binchoice.summary.passive, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = plabel(pVal2humansAdjusted)) 

# print the result
for (i in seq_along(dat.binchoice.summary.withchisq$Metric)) {
  print(paste(dat.binchoice.summary.withchisq$Metric[i], ": ",
        round(dat.binchoice.summary.withchisq$AccuracyScore[i],2), 
        ", Ï‡2=", round(dat.binchoice.summary.withchisq$ChiSq[i],2), 
        ", p=", round(dat.binchoice.summary.withchisq$pVal2humansAdjusted[i],3),
        ";", sep=""))
}
```

```{r}
plot.binacc.passive = ggplot(data=dat.binchoice %>%
                                         filter(TrialType%in%c("AI","AAN"), Voice=='passive'), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 2, labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary.passive)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
plot.binacc.passive

savename <- "1_binaryAccuracy_split_passive.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=10, units='cm')
```


# SI - MODEL HUMAN COMPARISON

## Calculate human 2 human response correlation 
(each human with the average of all the rest)
```{r}
human_dat.comp = human_dat %>%
  filter(!is.na(Score)) %>%
  mutate(FinalNormScore = normalization(Score)) %>%
  select(WorkerId, ItemNum, TrialType, Plausibility, Voice, Sentence, FinalNormScore)

workerIDs = unique(human_dat.comp$WorkerId)
df_correlation = data.frame()

for (i in seq_along(workerIDs)) {
  if (i%%50==0) {    # just to keep track (n=511 for EventsAdapt)
    print(i)
  }
  human_dat.this = human_dat.comp %>% filter(WorkerId==workerIDs[i])
  human_dat.rest = human_dat.comp %>% filter(WorkerId!=workerIDs[i]) %>%
    filter(ItemNum %in% unique(human_dat.this$ItemNum)) %>%
    group_by(ItemNum, Plausibility, Voice, Sentence) %>% 
    summarize(MeanNormScore=mean(FinalNormScore))
  
  human_dat.side2side = merge(human_dat.this, human_dat.rest,
                              by=c("ItemNum", "Plausibility", "Voice"))
  corval= cor(human_dat.side2side$FinalNormScore, human_dat.side2side$MeanNormScore)
  # add vector to a dataframe
  df <- data.frame(workerIDs[i], corval)
  df_correlation <- rbind(df_correlation,df)
}

human2human_corr = mean(df_correlation$corval)
print(paste("The human2human overall correlation value is", human2human_corr))
```

## Sample a random human response for each item 
And compute the avg of all other responses 

```{r}
set.seed(21)
human_dat.sampled = human_dat.comp %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Sentence) %>%
  summarize(RandomScore = sample(FinalNormScore,1),
            MeanScoreRest = (sum(FinalNormScore)-RandomScore)/(length(FinalNormScore)-1)) %>%
  ungroup()
```

## Prep data

```{r}
dat.human = dat %>% filter(Metric=="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore)
dat.model = dat %>% filter(Metric!="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore, Metric)
dat.model2human = merge(dat.human, dat.model, 
                        by=c("ItemNum", "TrialType", "Sentence", "Plausibility")) %>%
  rename(HumanScore=FinalNormScore.x, ModelScore=FinalNormScore.y)
```

```{r}
# add human2human data
human_dat.sampled = human_dat.sampled %>%
  select(-Voice) %>%
  filter(Sentence %in% unique(dat.model2human$Sentence)) %>%  # make sure it's the same sents
  rename(HumanScore=MeanScoreRest, ModelScore=RandomScore)    # renaming for convenience 
human_dat.sampled$Metric = 'singleHuman'

dat.model2human = rbind(dat.model2human, human_dat.sampled)
```

Formatting:

```{r, echo=FALSE, fig.height=12, fig.width=15}
dat.model2human = dat.model2human %>%
  #add color info
  mutate(Category = ifelse(Metric%in%llms_main_short, "LLMs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="singleHuman", "human", Category))

# specify order for plotting
metric_names = chosen_models
metric_names[1] = "singleHuman"
dat.model2human$Metric = factor(dat.model2human$Metric, levels=metric_names)
dat.model2human$Category = factor(dat.model2human$Category, levels=c("human", "LLMs", "baselines"))
dat.model2human$TrialType = factor(dat.model2human$TrialType, levels=c("AI", "AAN", "AAR"))
```

## Calculate correlations for the plot

```{r}
trialTypes = c("AI", "AAN", "AAR")
df_correlation = get_correlation_df_tt("model2human", "singleHuman", trialTypes, dat.model2human, metric_names)
```

## Plot

```{r}

dat.model2human$TrialType = factor(dat.model2human$TrialType, levels=c("AI","AAN","AAR"))
df_correlation$Metric = factor(df_correlation$Metric, levels=metric_names)
df_correlation$TrialType = factor(df_correlation$TrialType, levels=c("AI","AAN","AAR"))
df_correlation$Metric = factor(df_correlation$Metric, levels=metric_names)

# text size 
cortext_size = 2.5
label_names <- c(
  "AI" = "animate-inanimate",
  "AAN" = "animate-animate",
  "AAR" = "animate-animate\ncontrol"
)

plot.model2human = ggplot(dat=dat.model2human)+  
  geom_point(mapping=aes(x=ModelScore, y=HumanScore, color=Category), 
             size=0.1, position=position_jitter(width=0.01))+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=ModelScore, y=HumanScore, color=Category), 
             size=0.1, position=position_jitter(width=0.01))+
  geom_text(mapping=aes(x=0.2, y=1.10, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(-0.15,1))+
  scale_x_continuous(breaks=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  theme_classic()+
  facet_grid(TrialType~Metric, labeller = labeller(TrialType = as_labeller(label_names)))
plot.model2human

savename <- "model2human_byTrialType.png"
ggsave(paste(savedir,savename,sep="/"), height=12,width=30, units='cm')
```


# SI CONTROL PLOTS

```{r, fig.height=2,fig.width=5}
dat.sub.aar = dat %>%
  filter(Voice=="active") %>%
  filter(TrialType%in%c("AAR"))

dat.sub.aar$Metric = factor(dat.sub.aar$Metric, levels=chosen_models)

plot.plaus.density.sub.aar = ggplot()+
  geom_density(data=dat.sub.aar, mapping=aes(FinalNormScore, fill = factor(Plausibility, levels=c("Implausible","Plausible"))), alpha = 0.2)+
  facet_grid(TrialType~Metric, labeller = labeller(TrialType = as_labeller(label_names)))+
  ggtitle('PLAUSIBILITY (BOTH VERSIONS PLAUSIBLE)')+
  geom_vline(xintercept=reference_value, linetype='dotted')+
  #coord_flip()+
  scale_x_continuous(breaks=c(0,0.5,1))+
  theme_classic()+
  xlab("Normed score") +
  ylab(" ")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title=element_blank(),
        legend.position="none")
plot.plaus.density.sub.aar
```

## get p values
```{r}
dat.binchoice.summary.aar = dat.binchoice %>%
  filter(Voice=="active", TrialType %in% c("AAR")) %>%
  group_by(Category, Metric) %>%
  summarize(NumCorrect=sum(Accuracy[!is.na(Accuracy)]), NumTotal=length(Accuracy[!is.na(Accuracy)])) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary.aar = dat.binchoice.summary.aar %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  ungroup()
```

## Prep data

```{r, echo=FALSE}
plotting_models = chosen_models

dat.sub = dat %>%
  filter(Metric%in%plotting_models)

dat.plaus = dat.sub %>%
  filter(Metric %in% plotting_models) %>%
  select(ItemNum, TrialType, Plausibility, Voice, Metric, FinalNormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(Plausibility, meanScore)
dat.plaus$Metric = factor(dat.plaus$Metric, levels=plotting_models)
```

## Plot

```{r, fig.height=2,fig.width=5}
trialTypes = c("AAR")
dat.sub.aar = dat.plaus %>%
  filter(TrialType%in%c(trialTypes))

df_correlation.plaus.all = get_correlation_df("plausibility", "human", dat.sub.aar, plotting_models)
df_correlation.plaus.all$Metric = factor(df_correlation.plaus.all$Metric, levels=plotting_models)

dat.sub.aar$Metric = factor(dat.sub.aar$Metric, levels=plotting_models)

plot.plaus.control = ggplot(data=dat.sub.aar)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  facet_grid(TrialType~Metric, labeller = labeller(TrialType = as_labeller(label_names)))+
  geom_hline(yintercept=reference_value, linetype='dotted', lwd=0.2)+
  geom_vline(xintercept=reference_value, linetype='dotted', lwd=0.2)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  geom_text(mapping=aes(x=0, y=1.10, label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.plaus.all, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  ggtitle(' ')+
  theme_classic()+
  xlab("Plausible V1") +
  ylab("Plausible V2") +
  theme(plot.title = element_text(hjust=0.5,face='bold'))
plot.plaus.control

savename <- "plausplot_AAR.png"
ggsave(paste(savedir,savename,sep="/"), height=6,width=25, units='cm')
```

```{r}
plot.score_difference.aar = ggplot(data=dat.binchoice %>% filter(Voice=="active", TrialType=="AAR"),mapping=aes(x=TrialType, y=FinalScoreDiff, fill=Category))+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=1, y=0.02, label=pLabel), data=dat.binchoice.summary.aar, size=cortext_size)+
  geom_hline(yintercept=0, linetype='dotted')+
  ggtitle('SCORE DIFFERENCE')+
  theme_classic()+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  xlab("")+
  ylab("Difference (V1 - V2)")+
  coord_cartesian(ylim=c(-0.02,0.02))
plot.score_difference.aar
```


```{r}
control_plot.aar <- plot_grid(plot.plaus.density.sub.aar, plot.score_difference.aar, labels = c('A', 'B'), label_size = 15, ncol = 1)
control_plot.aar

savename <- "controlplot_aar.png"
ggsave(paste(savedir,savename,sep="/"), height=15,width=25, units='cm')
```


