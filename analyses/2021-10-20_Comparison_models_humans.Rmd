---
title: "2021-10-20_Comparison_models_humans"
output: html_document
---


```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)


#SET ENVIRONMENT VARIABLES
#use BOTH EventsAdapt AND EventsRev OR only EventsAdapt
use_eventsrev <- TRUE #set to FALSE if you don't want to also use EventsRev data for analyses
which_models <- "all-main" #can be "anns-all", "anns-main", "baselines", "all", "all-main"
normalization <- "zscore" #can be "min-max", "zscore"


remove_axis <- theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#uppercase first letter of string (needed for fast_vector_sum)
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

#function for sharing legends #https://github.com/tidyverse/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)

  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))

  grid.newpage()
  grid.draw(combined)

  # return gtable invisibly
  invisible(combined)

}



# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on EventsRev dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  ## Exclude fast vector sum model for mismatch of sentences >> to be fixed! (exlusion list excludes more than in other sentences)
  if (grepl("fast_vector_sum", filename, fixed = TRUE)  == TRUE) {
    message(paste(">> Model", filename, "is mismatched in sentences >> excluding it!!"))
    return(NULL)
  }
  
  #ROWS
  #set target number of *rows* in file
  if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
    target_rownumber = 1648
  } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
    target_rownumber = 80
  } else {
    print(paste("unknown experiment for file: ", filename))
    target_rownumber = 0
  }
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==6) {
      
      # 0. PREPARATION (set #of trials per item)
      if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
        target_trialnr = 4
      } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
        target_trialnr = 2
      } else {
        print(paste("unknown experiment for file: ", filename))
        target_trialnr = 0
      }
    
    #streamline input format
    if (ncol(d)==3){
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    else if (ncol(d)==4){
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Plausibility column to streamline how it's assigned
                           #(e.g. all lowercase etc))
      rename(SentenceNum=V1, Sentence=V2, Score=V4) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == FALSE)) { #If surprisal_scores_tinylstm
      #Add sentence number from 0 to len(dataset) -1
      tgt_len = nrow(d)-1
      sentnums = c(0:tgt_len)
      d = d  %>%
      select(V1,V5) %>% #select relevant columns (others include UNKification for EventsAdapt dataset)
      rename(Sentence=V1, Score=V5) %>%
      mutate(SentenceNum = sentnums) %>% #This adds a column SentenceNum from 0 to len(dataframe)-1
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == TRUE)) {
      d = d  %>%
      select(V1,V3,V6) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V6) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    d = d %>%
      mutate(Plausibility = ifelse(SentenceNum%%2==0, 'Plausible', 'Implausible')) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip EventsAdapt dataset prefix
      mutate(Metric = str_replace(Metric, "new[_-][Ee]ventsAdapt[_\\.]", "")) %>%
      mutate(Metric = str_replace(Metric, "newsentences[_-]EventsAdapt[_\\.]", "")) %>%
      #strip EventsRev dataset prefix
      mutate(Metric = str_replace(Metric, "ev1[_\\.]", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline1_ppmi", "syntax-based_PPMI")) %>% #EventsAdapt
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline2_ppmi", "order-based_PPMI")) %>% #EventsAdapt
      ##Assimilate thematic fit model names between datasets
      mutate(Metric = str_replace(Metric, "update-model.TF-add", "thematicFit_addition")) %>%
      mutate(Metric = str_replace(Metric, "update-model.TF-prod", "thematicFit_product")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale
      #Note: One entire quadruplet (SentenceNum 1516, The editor overheard the proofreader .) has score 0.0
      # >> add 1e-30 to enable log scaling & ask Emmanuele why this is 0!
    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all models
    if (grepl("min-max", normalization) == TRUE){
      d = d %>%
      mutate(NormScore = min_max(Score))
    } else {
      d = d %>%
      mutate(NormScore = scale_this(Score))
    }

      # 4. PROCESS SENTENCES
      d = d  %>%
      #strip space before final period for alignment with TrialTypes etc below
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", ".")) %>%
      #for some reason fast_vector_sum does not have final periods now.
      mutate(Sentence = ifelse(endsWith(Sentence, "."),Sentence,paste(Sentence, ".", sep=""))) %>%
      #uppercase first word in sentence to align with other model sentence sets
      mutate(Sentence = ifelse(Metric == "fast_vector_sum",firstup(Sentence),Sentence))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

##### Print environment variables #####
```{r}
message("Running with the following environment variables:")
message(paste("use_eventsrev: ", use_eventsrev))
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization))

#for plotting the dotted reference line
if (grepl("min-max", normalization) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
```

###### HUMAN ######

```{r}

# READ
dirname = '../human_ratings'
ratings_eventsRev = paste(dirname, '1_EventsRev_plausibility', 'analyses', 'longform_data.csv', sep='/')
ratings_eventsAdapt = paste(dirname, '2_EventsAdapt_plausibility', 'analyses', 'longform_data.csv', sep='/')

dat_er = read.csv(ratings_eventsRev) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence) %>%
  filter(ItemNum<41)    # exclude attention checks

dat_ea = read.csv(ratings_eventsAdapt) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence, TrialType, Voice)

# CLEAN
dat_er$Plausibility = recode(dat_er$Plausibility, implaus='Implausible', plaus='Plausible')
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'
dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap

# arbitrarily assign plausibility labels to AAR
dat_ea$Plausibility = recode(dat_ea$Plausibility, plausible0='Implausible', plausible1='Plausible')    
dat_ea$Plausibility = recode(dat_ea$Plausibility, implausible='Implausible', plausible='Plausible') 
dat_ea$Experiment = 'EventsAdapt'

# COMBINE
dat = rbind(dat_er, dat_ea)
dat$TrialType = factor(dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
```

#Initial mean, but to be filtered later on!

```{r pressure, echo=FALSE}
human_dat.mean = dat %>% 
  group_by(ItemNum, Sentence, Plausibility, TrialType, Voice, Experiment) %>% #, SynonymPair, NumSyn) %>% 
  summarize(MeanScore = mean(Score, na.rm=TRUE)) %>%
  ungroup() 

#scale scores

#3.2 Normalize scores for all models
if (grepl("min-max", normalization) == TRUE){
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = min_max(MeanScore))
} else {
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = scale_this(MeanScore))
}
```

###### MODELS ######


```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_eventsAdapt = paste(dirname, 'new-EventsAdapt', sep='/')
dirname_eventsRev = paste(dirname, 'EventsRev', sep='/')

message("Loading EventsAdapt data ...")
files_ea = list.files(path=dirname_eventsAdapt, pattern="*.txt")
dat_ea = do.call(rbind, lapply(files_ea, function(x) read_data(dirname_eventsAdapt, x)))

cat("\n")
message("Loading EventsRev data ...")
files_er = list.files(path=dirname_eventsRev, pattern="*.txt")
dat_er = do.call(rbind, lapply(files_er, function(x) read_data(dirname_eventsRev, x)))
```

###### PREPROCESS DATA FILES ######

## EVENTSADAPT
##Load & merge with original sentence file for information about TrialType & Voice
NOTE: Some of the sentences were different from the human experiment! >> take from dropbox, NOT behavioral study!
Maybe we should also just exclude these sentences
The file we use is downloaded from Dropbox and named here as 'newsentences_EventsAdapt_smallfixes.csv' to avoid confusion with the 'newsentences_EventsAdapt.csv' file in the behavioral study folder
```{r read data, echo=FALSE}
sentence_data = read.csv('newsentences_EventsAdapt_smallfixes.csv') %>%
rename(TrialType=EvType) %>%
rename(SentenceNum=X) %>%
rename(ItemNum=ItemNumber) %>% 
rename(Sentence=Stimulus) %>%
select(Sentence, Voice, TrialType)

#merge
dat_ea = merge(dat_ea, sentence_data)
dat_ea = arrange(dat_ea, desc(Metric), ItemNum, SentenceNum)

dat_ea$Experiment = 'EventsAdapt'

#align ItemNumber with numbers from behavioral analysis
dat_ea$ItemNum = dat_ea$ItemNum+1
```

## EVENTSREV
```{r read data, echo=FALSE}

dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'

#align ItemNumber with numbers from behavioral analysis
dat_er$ItemNum = dat_er$ItemNum + 1

dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap
```

## COMBINE OR NOT
```{r read data, echo=FALSE}
if (use_eventsrev == TRUE){
  dat = rbind(dat_er, dat_ea)
} else if (use_eventsrev == FALSE){
  dat = dat_ea
} else {
  stop("Please specify whether to use EventsRev for analysis or not!")
}

dat$TrialType = factor(dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
```

### Filter out non-ANN models
### below yields 24192 (lines) /(1648+80=1728) (sentences) = 14 (models) for EventsAdapt + EventsRev
```{r, echo=FALSE}

print(unique(dat$Metric))
anns <- c("gpt2","bert","xlnet","roberta")
anns_main <- c("gpt2-xl_full","bert-large-cased.sentence-probs-pseudolog","roberta-large.sentence-probs-pseudolog","xlnet-large-cased.sentence-probs-pseudolog")
baselines <- c("syntax-based_PPMI","order-based_PPMI","thematicFit_addition","thematicFit_product","v2_sdm","fast_vector_sum","surprisal_scores_tinylstm")
all_main <- append(baselines, anns_main)


#filter out non-ANN models
if (which_models == "anns-all"){
  model_dat = dat %>%
  filter(grepl(paste(anns, collapse = "|"),dat$Metric))
  message("Using only ANN models for analysis")
  chosen_models = anns
} else if (which_models == "anns-main"){
  model_dat = dat %>%
  filter(grepl(paste(anns_main, collapse = "|"),dat$Metric))
  message("Using only main ANN models for analysis")
  chosen_models = anns_main
} else if (which_models == "baselines"){
  model_dat = dat %>%
  filter(grepl(paste(baselines, collapse = "|"),dat$Metric))
  #model_dat$Metric = factor(dat$Metric, levels=c("syntax-based_PPMI","order-based_PPMI","thematicFit_addition",
  #  "thematicFit_product","v2_sdm","fast_vector_sum","surprisal_scores_tinylstm"))
  chosen_models = baselines
  message("Using only baseline models for analysis")
} else if (which_models == "all-main"){
  model_dat = dat %>%
  filter(grepl(paste(all_main, collapse = "|"),dat$Metric))
  message("Using all main models for analysis")
  chosen_models = all_main
} else {
  message("Using all models for analysis")
  chosen_models = append(anns,baselines)
}

print(unique(model_dat$Metric))
```

# PLAUSIBILITY

We need to define for which models a higher score for plausible sentences is desirable:

1. HIGHER = MORE PLAUSIBLE
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a plausible sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE PLAUSIBLE
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

model_dat.binchoice = model_dat %>%
  #create filter criterion
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  group_by(ItemNum, TrialType, Voice, Metric,LowerBetter) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0))

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Plausible is more positive!)
# Take out/plot NormScore below if confusing!
model_dat = model_dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore, NormScore))
```

## MAKE SURE TO KEEP SAME SENTENCES FOR HUMANS AND MODELS & RE-ORDER
We have to exclude certain items because there's not a full overlap of stimuli between human and model data. We cannot exclude based on item number or synonym pair number because of general misalignment (some numbers are missing)
The below exclusion keywords are obtained from running the check that finds differences in the datasets for models and humans below

```{r}
exclusion_keywords = c('designer', 
                       'decorator',
                       #
                       'pants',
                       'trousers',
                       #
                       'owner',
                       'proprietor',
                       #
                       'spectators',
                       'audience',
                       #
                       'illusionist',
                       'magician',
                       #
                       'ancestors',
                       'serf',
                       #
                       'dictator',
                       'townspeople',
                       #
                       'runner',
                       'jogger',
                       #
                       'officer',
                       'deputy',
                       #
                       'hunter',
                       'perpetrator',
                       #
                       'closed', # doesn't come with a synonym
                       'cushion', # doesn't come with a synonym
                       'pullover',  # doesn't come with a synonym
                       'soda', # doesn't come with a synonym
                       'woodworker', # doesn't come with a synonym
                       'target', # doesn't come with a synonym
                       'pine',  # doesn't come with a synonym
                       'smuggler',  # doesn't come with a synonym
                       'fired',  # doesn't come with a synonym
                       'beverage', #7 doesn't come with a synonym
                       'box-office', #282 no synonym
                       'counselor', #293 no synonym
                       'laundress', #306 no synonym
                       'twins', #328 no synonym
                       'fashionista', #379 no synonym
                       'sportsstar', #922 no synonym
                       'applauding' #927 no synonym
                       )
```

#exclude from model dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset
message(paste(length(unique(model_dat$Sentence))))
model_dat = model_dat %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(model_dat$Sentence))))

#order by itemNum for correct assignment
model_dat = arrange(model_dat, ItemNum)

models = unique(model_dat$Metric)
df = NA

for(i in seq_along(models)){ 
  print(paste(models[i]))
  curr_dataset = subset(model_dat, Metric == models[i])
  # add info about synonym pairs for dat_ea items
  num_EA = length(subset(curr_dataset, Experiment=='EventsAdapt')$Sentence) %/% 4
  print(paste(num_EA, ',', num_EA%*%4))
  num_ER = length(subset(curr_dataset, Experiment=='EventsRev')$Sentence) %/% 2
  print(paste(num_ER, ',', num_ER%*%2))
  itemNums_forSynsets <- c(rep(1:num_EA, each=4), rep(901:(900+num_ER), each=2))
  
  curr_dataset = curr_dataset %>%
    rename(ItemNum_OLD=ItemNum) %>%
    mutate(ItemNum = itemNums_forSynsets) %>%
    mutate(SynonymPair = ifelse(ItemNum>248, NA, (ItemNum+1)%/%2)) %>% #changed from 900 to 248 CK
    mutate(NumSyn = ifelse(ItemNum>248, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) #changed from 900 to 248 CK

  if (is.na(df)){
    df = curr_dataset}
  else{
    df = merge(df,curr_dataset, all=TRUE)
  }
}
```

#exclude from human dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset
message(paste(length(unique(human_dat.mean$Sentence))))
human_dat.mean = human_dat.mean %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(human_dat.mean$Sentence))))

# add info about synonym pairs for dat_ea items
num_EA = length(subset(human_dat.mean, Experiment=='EventsAdapt')$Sentence) %/% 4
print(paste(num_EA, ',', num_EA%*%4))
num_ER = length(subset(human_dat.mean, Experiment=='EventsRev')$Sentence) %/% 2
print(paste(num_ER, ',', num_ER%*%2))
itemNums_forSynsets <- c(rep(1:num_EA, each=4), rep(901:(900+num_ER), each=2))

human_dat.mean = human_dat.mean %>%
  rename(ItemNum_OLD=ItemNum) %>%
  mutate(ItemNum = itemNums_forSynsets) %>%
  mutate(SynonymPair = ifelse(ItemNum>248, NA, (ItemNum+1)%/%2)) %>% #changed from 900 to 248 CK
  mutate(NumSyn = ifelse(ItemNum>248, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) %>% #changed from 900 to 248 CK
  mutate(Metric = "human")
```
#Check that same sentences are in datasets for models and humans
```{r}
sentence_set_humans = unique(human_dat.mean$Sentence)
sentence_set_models = unique(model_dat$Sentence)
message(paste(length(sentence_set_humans)))
message(paste(length(sentence_set_models)))

itemNums_toExclude = list()

message(paste("Not in model sentence set but in human sentence set"))
for(i in seq_along(sentence_set_humans)){ 
  if (!sentence_set_humans[i] %in% sentence_set_models){
    #print(paste(i,sentence_set_humans[i]))
    curr_dat = subset(human_dat.mean, Sentence==sentence_set_humans[i])
    print(paste(i,sentence_set_humans[i],list(unique(curr_dat$ItemNum))))
    itemNums_toExclude <- append(itemNums_toExclude, list(unique(curr_dat$ItemNum)))
  }
}

message(paste("Not in human sentence set but in models sentence set"))
for(j in seq_along(sentence_set_models)){ 
  if (!sentence_set_models[j] %in% sentence_set_humans){
    #print(paste(j,sentence_set_models[j] ))
    curr_dat = subset(model_dat, Sentence==sentence_set_models[j])
    print(paste(j,sentence_set_models[j],list(unique(curr_dat$ItemNum))))
    itemNums_toExclude <- append(itemNums_toExclude, list(unique(curr_dat$ItemNum)))
    }
}

final_itemNums_toExclude <- unique(itemNums_toExclude)
message(paste(length(final_itemNums_toExclude)))
message(paste(final_itemNums_toExclude))
```

#reorder human dataset in the same way as model dataframes
# > order by Plausibility (reverse), then Voice, then NumSyn, then SynonymPair
# for some reason this has to be sequential >> doesn't work if I put it all into one line.

#reorder human dataset by ItemNum & models as well to be able to ensure same order of scores

```{r}
human_dat.mean_ordered = human_dat.mean %>%
  select(ItemNum, Sentence, Plausibility, Metric, TrialType, Voice, SynonymPair, NumSyn, NormScore, MeanScore, ItemNum_OLD)

human_dat.mean_ordered = arrange(human_dat.mean_ordered, ItemNum)
#human_dat.mean_ordered = arrange(human_dat.mean_ordered, Plausibility)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, desc(Plausibility))
human_dat.mean_ordered = arrange(human_dat.mean_ordered, Voice)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, SynonymPair)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, NumSyn)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, SynonymPair)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, ItemNum)

model_dat = arrange(df, Metric)
model_dat = arrange(df, ItemNum)
```

#Check that order is the same for one model metric and human data
For the AAR conditions, different orders were assigned plausible/implausible labels.
NOTE: I recoded all

```{r}
mismatched_plausibility = list()

human_final_sent = human_dat.mean_ordered$Sentence
human_item_num = human_dat.mean_ordered$ItemNum
#print(paste(human_final_sent[1:5]))
if (which_models == "baselines"){
  model_df = subset(model_dat, Metric=='v2_sdm')
} else {
  model_df = subset(model_dat, Metric=='gpt2-xl_full')
}
model_final_sent = model_df$Sentence
model_item_num = model_df$ItemNum
#print(paste(model_final_sent[1:5]))

print(paste(length(human_final_sent), length(model_final_sent)))

for(i in seq_along(human_final_sent)){ 
  if (human_final_sent[i] != model_final_sent[i]){
    #print('MISMATCH')
    #print(paste(human_item_num[i], '| human: ', human_final_sent[i], '| model: ', model_final_sent[i]))
    mismatched_plausibility <- append(mismatched_plausibility,list(human_item_num[i]))
  }
}

mismatched_plausibility = unique(mismatched_plausibility)
print(paste(mismatched_plausibility))
```

```{r}
#recode plausibility for AAR conditions, which are different between human & model dataset

human_dat.mean_ordered <- human_dat.mean_ordered %>%
    mutate(Plausibility =
      case_when(
        ItemNum %in% mismatched_plausibility == TRUE & Plausibility == 'Plausible' ~ "Implausible",
        ItemNum %in% mismatched_plausibility == TRUE & Plausibility == 'Implausible' ~ "Plausible",
        TRUE ~ as.character(Plausibility)
        )
    )

#reorder
human_dat.mean_ordered = arrange(human_dat.mean_ordered, ItemNum)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, desc(Plausibility))
human_dat.mean_ordered = arrange(human_dat.mean_ordered, Voice)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, SynonymPair)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, NumSyn)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, SynonymPair)
human_dat.mean_ordered = arrange(human_dat.mean_ordered, ItemNum)
```

#test
```{r}
mismatched_plausibility = list()

human_final_sent = human_dat.mean_ordered$Sentence
human_item_num = human_dat.mean_ordered$ItemNum
human_item_plaus = human_dat.mean_ordered$Plausibility
human_item_voice = human_dat.mean_ordered$Voice
human_item_synset = human_dat.mean_ordered$SynonymPair
human_item_numsyn = human_dat.mean_ordered$NumSyn
#print(paste(human_final_sent[1:5]))

if (which_models == "baselines"){
  model_df = subset(model_dat, Metric=='v2_sdm')
} else {
  model_df = subset(model_dat, Metric=='gpt2-xl_full')
}

model_final_sent = model_df$Sentence
model_item_num = model_df$ItemNum
model_item_plaus = model_df$Plausibility
model_item_voice = model_df$Voice
model_item_synset = model_df$SynonymPair
model_item_numsyn = model_df$NumSyn
#print(paste(model_final_sent[1:5]))

for(i in seq_along(human_final_sent)){ 
  if (human_final_sent[i] != model_final_sent[i]){
    print('wrong sent. order')
    print(paste(human_item_num[i], '| human: ', human_final_sent[i], '| model: ', model_final_sent[i]))
    mismatched_plausibility <- append(mismatched_plausibility,list(human_item_num[i]))
  }
  else if (human_item_num[i] != model_item_num[i]){
    print('wrong number')
    }
  else if (human_item_plaus[i] != model_item_plaus[i]){
    print('wrong plaus')
  }
    else if (human_item_voice[i] != model_item_voice[i]){
    print('wrong voice')
      }
  }
print('No more mismatches!')
```

```{r}
human_dat.binchoice = human_dat.mean_ordered %>%
  group_by(ItemNum, TrialType, Voice) %>%
  summarize(ScoreDiff = MeanScore[Plausibility=="Plausible"]-MeanScore[Plausibility=="Implausible"]) %>%
  mutate(Accuracy = ifelse(ScoreDiff>0, 1, 0))
```


## COMBINE WITH HUMAN DATA
```{r pressure, echo=FALSE}
model_dat = model_dat %>%
  select(ItemNum, Sentence, Plausibility, Metric, TrialType, Voice, SynonymPair, NumSyn, NormScore)

human_dat.mean = human_dat.mean %>%
  select(ItemNum, Sentence, Plausibility, Metric, TrialType, Voice, SynonymPair, NumSyn, NormScore)

dat_scores = rbind(human_dat.mean, model_dat)
```


##CORRELATION ANALYSIS
NOTE: If I run this with the dat as defined in the regression part I get way higher correlations > check!
```{r correlation, echo=FALSE}
dat = dat_scores #overwrite dat for rest of script again!
models_metrics = unique(dat$Metric)
print(paste(models_metrics))

df_correlation = data.frame()

for(i in seq_along(models_metrics)){
  message(paste(models_metrics[i]))
  message(paste(length(dat$NormScore[dat$Metric=="human"]), length(dat$NormScore[dat$Metric==models_metrics[i]])))
  correlation = cor(dat$NormScore[dat$Metric=="human"], dat$NormScore[dat$Metric==models_metrics[i]], method="pearson")
  #correlation = cor.test(dat$NormScore[dat$Metric=="human"], dat$NormScore[dat$Metric==models_metrics[i]], method="pearson")
  message(paste(correlation,"\n"))
  
  # add vector to a dataframe
  df <- data.frame(models_metrics[i], correlation)
  df_correlation <- rbind(df_correlation,df)
}
write.csv(df_correlation, "correlation_humans_models.csv")
```
```{r plot}
#matplot(dat$NormScore[dat$Metric=="human"],dat$NormScore[dat$Metric=="gpt2-xl_full"], type = "p",xlab="human",ylab="gpt2-xl_full")

scatter_dat_human = subset(dat, Metric=="human") %>%
  select(NormScore) %>%
  rename(human = NormScore)

scatter_dat_gpt = subset(dat, Metric=="gpt2-xl_full") %>%
  select(NormScore) %>%
  rename(gpt2 = NormScore)

scatter_dat = cbind(scatter_dat_human, scatter_dat_gpt)

ggplot(data=scatter_dat)+
    geom_point(mapping = aes(x=human, y=gpt2),
             shape=21, size=0.75, color='black',
             show.legend = TRUE)+
  geom_abline(h = 0, lty = 2, col="blue")+
  theme_classic()

savedir = './correlation_figs'
savename <- "scatter_human_gpt2.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

```{r plot}

scatter_dat_roberta = subset(dat, Metric=="roberta-large.sentence-probs-pseudolog") %>% #best model as per correlation analysis
  select(NormScore) %>%
  rename(roberta = NormScore)

scatter_dat = cbind(scatter_dat_human, scatter_dat_roberta)

ggplot(data=scatter_dat)+
    geom_point(mapping = aes(x=human, y=roberta),
             shape=21, size=0.75, color='black',
             show.legend = TRUE)+
  geom_abline(h = 0, lty = 2, col="blue")+
  theme_classic()

savename <- "scatter_human_roberta.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```


```{r, fig.height=12, fig.width=15}

datAI = subset(dat, TrialType=='AI')

ggplot(data=datAI)+
    geom_violin(mapping = aes(x=factor(Metric, levels=rev(append(c('human'),chosen_models))), y=NormScore, fill=Plausibility), 
    scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=Metric, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  ggtitle("AI") 

savename <- "violin_AI.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=20, units='cm')
```

```{r, fig.height=12, fig.width=15}

datAAN = subset(dat, TrialType=='AAN')

ggplot(data=datAAN)+
    geom_violin(mapping = aes(x=factor(Metric, levels=rev(append(c('human'),chosen_models))), y=NormScore, fill=Plausibility), 
                scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=Metric, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  ggtitle("AAN") 

savename <- "violin_AAN.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=20, units='cm')
```

```{r, fig.height=12, fig.width=15}

datAAR = subset(dat, TrialType=='AAR')

ggplot(data=datAAR)+
    geom_violin(mapping = aes(x=factor(Metric, levels=rev(append(c('human'),chosen_models))), y=NormScore, fill=Plausibility), 
                scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=Metric, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip() +
  ggtitle("AAR") 

savename <- "violin_AAR.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=20, units='cm')
```

```{r, fig.height=20, fig.width=20}

trialtypes = unique(dat$TrialType)
for(i in seq_along(trialtypes)){
  nam <- paste("A", i, sep = "")
  #print(models_metrics[i])
  
  curr_data = dat %>%
    filter(TrialType==trialtypes[i])
  
  p <- ggplot(data=curr_data) +
    geom_violin(mapping = aes(x=factor(Metric, levels=rev(append(c('human'),chosen_models))), y=NormScore, fill=Plausibility), scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=Metric, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  ggtitle(paste(trialtypes[i])) #+
  #facet_wrap(~TrialType)
  #ggtitle(paste(trialtypes[i]))
  
  assign(nam, p)
}

grid_arrange_shared_legend(A1, A2, A3, ncol = 2, nrow = 2, position='bottom')
#grid.arrange(A1, A2, A3, ncol=2, nrow=2)
g <- arrangeGrob(A1, A2, A3)

savename <- "violin_all.png"
ggsave(paste(savedir,savename,sep="/"), width=30, height=30, units='cm', g)
```

```{r, fig.height=8, fig.width=12}

curr_metrics = c("human", "gpt2-xl_full")
for(i in seq_along(curr_metrics)){
  nam <- paste("A", i, sep = "")
  #print(models_metrics[i])
  
  curr_data = dat %>%
    filter(Metric==curr_metrics[i])
  
  p <- ggplot(data=curr_data) +
    geom_violin(mapping = aes(x=factor(TrialType, levels=rev(c('AI', "AAN", "AAR"))), y=NormScore, fill=Plausibility), scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=TrialType, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  facet_wrap(~TrialType)+
  ggtitle(paste(curr_metrics[i]))
  
  assign(nam, p)
}

grid.arrange(A1, A2)
g <- arrangeGrob(A1, A2)

savename <- "violin_gpt2_human.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=20, units='cm', g)
```

```{r, fig.height=8, fig.width=12}

curr_metrics = c("human", "roberta-large.sentence-probs-pseudolog") #best model as per correlation analysis
for(i in seq_along(curr_metrics)){
  nam <- paste("A", i, sep = "")
  #print(models_metrics[i])
  
  curr_data = dat %>%
    filter(Metric==curr_metrics[i])
  
  p <- ggplot(data=curr_data) +
    geom_violin(mapping = aes(x=factor(TrialType, levels=rev(c('AI', "AAN", "AAR"))), y=NormScore, fill=Plausibility), scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=TrialType, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  facet_wrap(~TrialType)+
  ggtitle(paste(curr_metrics[i]))
  
  assign(nam, p)
}

grid.arrange(A1, A2)
g <- arrangeGrob(A1, A2)

savename <- "violin_roberta_human.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=20, units='cm', g)
```

```{r, fig.height=4, fig.width=12}

#Only EventsRev

curr_metrics = c("human", "gpt2-xl_full")
for(i in seq_along(curr_metrics)){
  nam <- paste("A", i, sep = "")
  #print(models_metrics[i])
  
  curr_data = dat %>%
    filter(ItemNum>900)%>%
    filter(Metric==curr_metrics[i])
  
  p <- ggplot(data=curr_data) +
    geom_violin(mapping = aes(x=factor(TrialType, levels=rev(c('AI', "AAN", "AAR"))), y=NormScore, fill=Plausibility), scale='width', width=0.7, alpha=0.5, position=position_dodge(width=0.5))+
    geom_point(mapping = aes(x=TrialType, y=NormScore, fill=Plausibility),
             shape=21, size=1.2, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='TrialType')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()+
  facet_wrap(~TrialType)+
  ggtitle(paste(curr_metrics[i]))
  
  assign(nam, p)
}

grid.arrange(A1, A2, ncol=2, nrow=1,
     top = "EventsRev")
g <- arrangeGrob(A1, A2)

savename <- "violin_gpt2_EventsRev.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm', g)
```


```{r plot scores, echo=FALSE}
curr_dat = subset(dat, Metric%in%c("gpt2-xl_full","human"))
curr_dat = subset(curr_dat, TrialType=="AI")
ggplot(data=curr_dat)+
    geom_violin(mapping = aes(x=Metric, y=NormScore, fill=Plausibility), 
                scale='width', width=0.6, alpha=0.5, position=position_dodge(width=0.6))+
    geom_point(mapping = aes(x=Metric, y=NormScore, fill=Plausibility),
             shape=21, size=0.75, color='black',
             position=position_jitterdodge(jitter.width=0.15, jitter.height=0, dodge.width=.6),
             show.legend = TRUE)+
  labs(x='Model')+
  labs(y='Standardized score (AI)')+
  geom_hline(yintercept=reference_value, linetype='dotted')+theme_classic()+coord_flip()
savename <- "violin_gpt2_AI.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

##REGRESSION

### Sentence freq info

```{r}
dirname = '../word_frequency_info'

sentence_file_er = paste(dirname, 'EventsRev_freqs.csv', sep='/')
sentence_file_ea = paste(dirname, 'EventsAdapt_freqs.csv', sep='/')

# read in & normalize predictors
dat_er_sentence = read.csv(sentence_file_er) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))
dat_er_sentence$ItemNum = dat_er_sentence$ItemNum + 900 
dat_er_sentence$TrialType = 'AAN'         
dat_er_sentence$Voice = 'active'
dat_ea_sentence = read.csv(sentence_file_ea) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))
dat_sentence = rbind(dat_er_sentence, dat_ea_sentence) 
dat_sentence$Plausibility = recode(dat_sentence$Plausibility, 
                             implausible='Implausible', plausible='Plausible',
                             implaus="Implausible", plaus="Plausible") 

# get updated item numbers
dat_sentence$ItemNum_OLD = dat_sentence$ItemNum 
dat_sentence = dat_sentence %>%
  filter(ItemNum_OLD %in% unique(human_dat.mean_ordered$ItemNum_OLD)) 
dat_sentence = arrange(dat_sentence, ItemNum_OLD)
dat_sentence$ItemNum = itemNums_forSynsets

# randomly assign plausibility values to AAR, use even/odd heuristic for the rest
dat_sentence$Index = c(1:nrow(dat_sentence))
dat_sentence = dat_sentence %>%
  mutate(Plausibility = ifelse((Index%%2==0), 'Implausible', 'Plausible'))

dat = merge(dat_scores, dat_sentence %>% select(-Plausibility), 
               by=c("ItemNum", "Sentence", "Voice", "TrialType"))
#write.csv(dat, 'dat_all_models_EventsAdaptRev.csv', row.names=FALSE)
```

### Contrasts
```{r}
#dat = read.csv('dat_all_models_EventsAdaptRev.csv')
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Voice = as.factor(dat$Voice)
contrasts(dat$Voice) = c(0.5, -0.5)
colnames(attr(dat$Voice, "contrasts")) = c("A>P")

dat$TrialType = factor(dat$TrialType, levels=c("AAR", "AAN", "AI"))    # dummy coding by default
dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)

```

### Stats

```{r regression, echo=FALSE}
m = lmer(NormScore ~ Plausibility*TrialType*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1+Plausibility||ItemNum), 
         data=dat, REML=FALSE)
sink("regression_model.txt", append=FALSE)
options(max.print=1000000)
summary(m)
sink()
summary(m)
```

```{r regression, echo=FALSE}
m1 = lmer(NormScore ~ Plausibility*TrialType*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1+Plausibility|ItemNum), 
         data=dat, REML=FALSE)
sink("regression_model1.txt", append=FALSE)
options(max.print=1000000)
summary(m1)
sink()
summary(m1)

```
```{r}
anova(m,m1)  # result: no benefit of adding a correlation param there
```

```{r regression, echo=FALSE}
m2 = lmer(NormScore ~ Plausibility*TrialType*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1+Plausibility*Metric||ItemNum), 
         data=dat, REML=FALSE)
sink("regression_model2.txt", append=FALSE)
options(max.print=1000000)
summary(m2)
sink()
summary(m2)

```








