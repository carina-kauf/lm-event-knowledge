---
title: "model_analysis"
output: html_document
---

NOTE: Currently we standardize the data. We could also do min-max scaling.

NOTE: Synsets seem to only exist for items with ItemNums <= 274!
Some items do not have the correct SynSet associated (items 102 & 272 from human data is not in models)

FIX: aligned sentences with human sentences & excluded sentences without synonyms > after new ItemNum 248 there are no synonym sentences anymore.

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(gridExtra)
library(assertthat)

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on EventsRev dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  
#  ## Exclude outdated thematic fit files/files used to combine thematic fit sentences & scores >> moved to folder
#  if ((grepl("_OLD", filename, fixed = TRUE)  == TRUE) | 
#    (grepl("n200.m20", filename, fixed = TRUE)  == TRUE)) {
#    message(paste("Excluding old scores for thematic fit model!"))
#    return(NULL)
#  }
  
  #ROWS
  #set target number of *rows* in file
  if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
    target_rownumber = 1648
  } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
    target_rownumber = 80
  } else {
    print(paste("unknown experiment for file: ", filename))
    target_rownumber = 0
  }
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==6) {
      
      # 0. PREPARATION (set #of trials per item)
      if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
        target_trialnr = 4
      } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
        target_trialnr = 2
      } else {
        print(paste("unknown experiment for file: ", filename))
        target_trialnr = 0
      }
    
    #streamline input format
    if (ncol(d)==3){
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
      print(paste(filename, "here!"))
    }
    else if (ncol(d)==4){
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Plausibility column to streamline how it's assigned
                           #(e.g. all lowercase etc))
      rename(SentenceNum=V1, Sentence=V2, Score=V4) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == FALSE)) { #If surprisal_scores_tinylstm
      #Add sentence number from 0 to len(dataset) -1
      tgt_len = nrow(d)-1
      sentnums = c(0:tgt_len)
      d = d  %>%
      select(V1,V5) %>% #select relevant columns (others include UNKification for EventsAdapt dataset)
      rename(Sentence=V1, Score=V5) %>%
      mutate(SentenceNum = sentnums) %>% #This adds a column SentenceNum from 0 to len(dataframe)-1
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == TRUE)) {
      d = d  %>%
      select(V1,V3,V6) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V6) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    d = d %>%
      mutate(Plausibility = ifelse(SentenceNum%%2==0, 'Plausible', 'Implausible')) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip EventsAdapt dataset prefix
      mutate(Metric = str_replace(Metric, "new[_-][Ee]ventsAdapt[_\\.]", "")) %>%
      mutate(Metric = str_replace(Metric, "newsentences[_-]EventsAdapt[_\\.]", "")) %>%
      #strip EventsRev dataset prefix
      mutate(Metric = str_replace(Metric, "ev1[_\\.]", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline1_ppmi", "syntax-based_PPMI")) %>% #EventsAdapt
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline2_ppmi", "order-based_PPMI")) %>% #EventsAdapt
      ##Assimilate thematic fit model names between datasets
      mutate(Metric = str_replace(Metric, "update-model.TF-add", "thematicFit_addition")) %>%
      mutate(Metric = str_replace(Metric, "update-model.TF-prod", "thematicFit_product")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale

    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all model
    d = d %>%
      mutate(Score = replace(Score, Score==0.0, 1e-20)) %>% #Sentences 937 & 938 don't have a score for thematic fit. #NOTE: Manually changed the score "None" to 0.0 for these!
      mutate(NormScore = scale_this(Score)) %>%

      # 4. PROCESS SENTENCES
      #strip space before final period for alignment with TrialTypes etc below
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", "."))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

#SET ENVIRONMENT VARIABLES
#use BOTH EventsAdapt AND EventsRev OR only EventsAdapt
```{r, echo=FALSE}
use_eventsrev <- TRUE #set to FALSE if you don't want to also use EventsRev data for analyses
which_models <- "anns" #can be "anns", baselines" or "all"
message("Running with the following environment variables:")
message(paste("use_eventsrev: ", use_eventsrev))
message(paste("which_models: ", which_models))
```

```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_eventsAdapt = paste(dirname, 'new-EventsAdapt', sep='/')
dirname_eventsRev = paste(dirname, 'EventsRev', sep='/')

message("Loading EventsAdapt data ...")
files_ea = list.files(path=dirname_eventsAdapt, pattern="*.txt")
dat_ea = do.call(rbind, lapply(files_ea, function(x) read_data(dirname_eventsAdapt, x)))

cat("\n")
message("Loading EventsRev data ...")
files_er = list.files(path=dirname_eventsRev, pattern="*.txt")
dat_er = do.call(rbind, lapply(files_er, function(x) read_data(dirname_eventsRev, x)))
```

###### PREPROCESS DATA FILES ######

## EVENTSADAPT
##Load & merge with original sentence file for information about TrialType & Voice
NOTE: Some of the sentences were different from the human experiment! >> take from dropbox, NOT behavioral study!
Maybe we should also just exclude these sentences
The file we use is downloaded from Dropbox and named here as 'newsentences_EventsAdapt_smallfixes.csv' to avoid confusion with the 'newsentences_EventsAdapt.csv' file in the behavioral study folder
```{r read data, echo=FALSE}
sentence_data = read.csv('newsentences_EventsAdapt_smallfixes.csv') %>%
rename(TrialType=EvType) %>%
rename(SentenceNum=X) %>%
rename(ItemNum=ItemNumber) %>% 
rename(Sentence=Stimulus) %>%
select(Sentence, Voice, TrialType)

#merge
dat_ea = merge(dat_ea, sentence_data)
dat_ea = arrange(dat_ea, desc(Metric), ItemNum, SentenceNum)

dat_ea$Experiment = 'EventsAdapt'

#align ItemNumber with numbers from behavioral analysis
dat_ea$ItemNum = dat_ea$ItemNum+1
```

## EVENTSREV
```{r read data, echo=FALSE}

dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'

#align ItemNumber with numbers from behavioral analysis
dat_er$ItemNum = dat_er$ItemNum + 1

dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap
```

## COMBINE OR NOT
```{r read data, echo=FALSE}
if (use_eventsrev == TRUE){
  dat = rbind(dat_er, dat_ea)
} else if (use_eventsrev == FALSE){
  dat = dat_ea
} else {
  stop("Please specify whether to use EventsRev for analysis or not!")
}

dat$TrialType = factor(dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
```

## Need to filter out data to make sure the correct synsets are assigned (and humans & models are being evaluated on the same dataset)
### Note: The next two cells were taken from the script 2021-09-10_Comparison_models_humans
We have to exclude certain items because there's not a full overlap of stimuli between human and model data. We cannot exclude based on item number or synonym pair number because of general misalignment (some numbers are missing)
The below exclusion keywords are obtained from running the check that finds differences in the datasets for models and humans below

```{r}
exclusion_keywords = c('designer', 
                       'decorator',
                       #
                       'pants',
                       'trousers',
                       #
                       'owner',
                       'proprietor',
                       #
                       'spectators',
                       'audience',
                       #
                       'illusionist',
                       'magician',
                       #
                       'ancestors',
                       'serf',
                       #
                       'dictator',
                       'townspeople',
                       #
                       'runner',
                       'jogger',
                       #
                       'officer',
                       'deputy',
                       #
                       'hunter',
                       'perpetrator',
                       #
                       'closed', # doesn't come with a synonym
                       'cushion', # doesn't come with a synonym
                       'pullover',  # doesn't come with a synonym
                       'soda', # doesn't come with a synonym
                       'woodworker', # doesn't come with a synonym
                       'target', # doesn't come with a synonym
                       'pine',  # doesn't come with a synonym
                       'smuggler',  # doesn't come with a synonym
                       'fired',  # doesn't come with a synonym
                       'beverage', #7 doesn't come with a synonym
                       'box-office', #282 no synonym
                       'counselor', #293 no synonym
                       'laundress', #306 no synonym
                       'twins', #328 no synonym
                       'fashionista', #379 no synonym
                       'sportsstar', #922 no synonym
                       'applauding' #927 no synonym
                       )
```

#exclude from model dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset/which don't contain a second SynSet version
message(paste(length(unique(dat$Sentence))))
model_dat = dat %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(model_dat$Sentence))))

#order by itemNum for correct assignment
model_dat = arrange(model_dat, ItemNum)

models = unique(model_dat$Metric)
df = NA

for(i in seq_along(models)){ 
  curr_dataset = subset(model_dat, Metric == models[i])
  # add info about synonym pairs for dat_ea items
  num_EA = length(subset(curr_dataset, Experiment=='EventsAdapt')$Sentence) %/% 4
  print(paste(num_EA, ',', num_EA%*%4))
  num_ER = length(subset(curr_dataset, Experiment=='EventsRev')$Sentence) %/% 2
  print(paste(num_ER, ',', num_ER%*%2))
  itemNums_forSynsets <- c(rep(1:num_EA, each=4), rep(901:(900+num_ER), each=2))
  
  curr_dataset = curr_dataset %>%
    rename(ItemNum_OLD=ItemNum) %>%
    mutate(ItemNum = itemNums_forSynsets) %>%
    mutate(SynonymPair = ifelse(ItemNum>248, NA, (ItemNum+1)%/%2)) %>% #changed from 900 to 248 || Items after the new ItemNum 248 do not have synonyms!
    mutate(NumSyn = ifelse(ItemNum>248, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) #changed from 900 to 248

  if (is.na(df)){
    df = curr_dataset}
  else{
    df = merge(df,curr_dataset, all=TRUE)
  }
}

model_dat = arrange(df, Metric)
model_dat = arrange(model_dat, ItemNum)

dat = model_dat
```

### Filter out non-ANN models
### below yields 24192 (lines) /(1648+80=1728) (sentences) = 14 (models) for EventsAdapt + EventsRev
```{r, echo=FALSE}

#print(unique(dat$Metric))
anns <- c("gpt2","bert","xlnet","roberta")

#filter out non-ANN models
if (which_models == "anns"){
  dat = dat %>%
  filter(grepl(paste(anns, collapse = "|"),dat$Metric))
  message("Using only ANN models for analysis")
} else if (which_models == "baselines"){
  dat = dat %>%
  filter(!grepl(paste(anns, collapse = "|"),dat$Metric))
  dat$Metric = factor(dat$Metric, levels=c("syntax-based_PPMI","order-based_PPMI","thematicFit_addition",
    "thematicFit_product","v2_sdm","fast_vector_sum","surprisal_scores_tinylstm"))
  message("Using only baseline models for analysis")
} else {
  message("Using all models for analysis")
}

print(unique(dat$Metric))
```

# PLAUSIBILITY

We need to define for which models a higher score for plausible sentences is desirable:

1. HIGHER = MORE PLAUSIBLE
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a plausible sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE PLAUSIBLE
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

#TAKE OUT (for presentation only)!!
#dat = subset(dat, Metric%in%c("gpt2-xl_full","bert-large-cased.sentence-probs-pseudolog"))

dat.binchoice = dat %>%
  #create filter criterion
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  group_by(ItemNum, TrialType, Voice, Metric,LowerBetter) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0))

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Plausible is more positive!)
# Take out/plot NormScore below if confusing!
dat = dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore, NormScore))

```

# BINARY ACCURACY

For baselines: Thematic fit models have lots & lots of 0.0 scores, which drags performance as Accuracy is set to 1 if the ScoreDifference is positive.
## Overall ANN plausibility plot
```{r, echo=FALSE}

ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=Accuracy))+
  stat_summary(geom='col', fun.y='mean',
               color='black', fill='gray80', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  coord_cartesian(ylim=c(0,1))+
  theme_minimal()

savedir <- paste("plots/EventsRev=",use_eventsrev,"_Models=",which_models)
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

savename <- "1_binaryAccuracy_all.png"
print(paste(savedir,savename,sep="/"))

ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

# Split by model & metric
```{r}
#Set plotting options for grid plots

models_metrics = unique(dat$Metric)
nr_models = length(models_metrics)
ncols=3
nrows=round(nr_models/3)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}

```

```{r, echo=FALSE, fig.height=8, fig.width=6}
ggplot(data=dat.binchoice, mapping=aes(x=Metric, y=Accuracy))+
  facet_wrap(~TrialType, nrow = 3)+
  stat_summary(geom='col', fun.y='mean',
               color='black', fill='gray80', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  coord_cartesian(ylim=c(0,1))+
  theme_classic()

#savename <- "1_binaryAccuracy_split.png"
#ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm')
```


# SCORE DIFFERENCE

Note: This is a weird plot for baselines as the score differences come from wildly different metrics!
Ignore the plots for baselines, or split up to enable a good visualization?
Also: As for some models a negative score difference indicates a more plausible choice (e.g. surprisal models), and for some it indicates a more implausible choice (e.g. PPMI models), I opted to plot the FinalScoreDiff.
FinalScoreDiff looks at whether a lower score is better for plausible sentences and, if so, reverses the sign of the score difference (see above).
Note that we could also adjust the NormScore & reverse the sign, and then keep ScoreDiff. Haven't done that, because that is also not clean yet.

## Overall
```{r, echo=FALSE}
#ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=ScoreDiff))+
ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=FinalScoreDiff))+
  stat_summary(geom='col', fun.y='mean',
               color='black', fill='gray80', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  theme_minimal()

savename <- "2_scoreDifference_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

##Split by model & metric
```{r, echo=FALSE, fig.height=15, fig.width=15}
#ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=ScoreDiff))+
ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=FinalScoreDiff))+
  facet_wrap(~Metric, ncol = 3)+
  stat_summary(geom='col', fun.y='mean',
               color='black', fill='gray80', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  theme_minimal()

savename <- "2_scoreDifference_split.png"
ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm')
```


# SCORES by Plausibility

Note that NormScore is more negative for plausible sentences for some models (e.g. surprisal)
## Overall
```{r, echo=FALSE}
ggplot(data=dat, mapping=aes(x=TrialType, y=FinalNormScore, fill=Plausibility))+
  stat_summary(geom='col', fun.y='mean',
               color='black', position='dodge', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', position=position_dodge(width=0.5), size = 0.5, width=0.1)

savename <- "3_normScores_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

##Split by model & metric
```{r, echo=FALSE, fig.height=12, fig.width=15}

ggplot(data=dat, mapping=aes(x=TrialType, y=FinalNormScore, fill=Plausibility))+
  facet_wrap(~Metric, ncol = 3)+
  stat_summary(geom='col', fun.y='mean',
               color='black', position='dodge', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', position=position_dodge(width=0.5), size = 0.5, width=0.1)

savename <- "3_normScores_split.png"
ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm')
```

# Within-item scores for plausible vs. implausible versions of the sentence

## Overall
```{r, echo=FALSE}
dat.wide = dat %>% 
  dplyr::select(ItemNum, TrialType, Plausibility, Voice, Metric, FinalNormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(Plausibility, meanScore)  

ggplot(data=dat.wide)+
  geom_point(mapping=aes(x=Implausible, y=Plausible))+
  facet_wrap(~TrialType)+
  geom_abline()+
  theme_classic()

savename <- "4_withinItem_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

##Split by model & metric

```{r Fig1, echo=FALSE, fig.height=15, fig.width=15}
models_metrics = unique(dat$Metric)

for(i in seq_along(models_metrics)){
  nam <- paste("A", i, sep = "")
  #print(models_metrics[i])
  
  curr_data = dat.wide %>%
    filter(Metric==models_metrics[i])
  
  p <- ggplot(data=curr_data)+
  geom_point(mapping=aes(x=Implausible, y=Plausible))+
  facet_wrap(~Metric)+
  geom_abline()+
  theme_classic()+
  ggtitle(paste(models_metrics[i]))
  
  assign(nam, p)
}

if (which_models=="all") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows) #generates g
} else if (which_models=="anns") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
} else { #if which_models == "baselines"
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  }

#savename <- "4_withinItem_split.png"
#ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm', g)
```

# ACTIVE VS PASSIVE

## Overall

```{r, echo=FALSE}
#ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=ScoreDiff, fill=Voice))+
ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=FinalScoreDiff, fill=Voice))+
  stat_summary(geom='col', fun.y='mean', position='dodge', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se', position=position_dodge(width=0.5),
               color = 'black', size = 0.5, width=0.1)

savename <- "5_activePassive_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

##Split by model & metric

```{r, echo=FALSE, fig.height=12, fig.width=15}
#ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=ScoreDiff, fill=Voice))+
ggplot(data=dat.binchoice, mapping=aes(x=TrialType, y=FinalScoreDiff, fill=Voice))+
  facet_wrap(~Metric, ncol = 3)+
  stat_summary(geom='col', fun.y='mean', position='dodge', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se', position=position_dodge(width=0.5),
               color = 'black', size = 0.5, width=0.1)

savename <- "5_activePassive_split.png"
ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm')
```


## By item

```{r, echo=FALSE}
dat.voice = dat %>%
  filter(!is.na(SynonymPair)) %>%
  dplyr::select(ItemNum, TrialType, Plausibility, Voice, Metric, FinalNormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(Voice, meanScore)

ggplot(data=dat.voice)+
  geom_point(mapping=aes(x=active, y=passive))+
  facet_wrap(~TrialType)+
  geom_abline()+
  theme_classic()

savename <- "6_activePassive_byitem_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

```{r Fig2, echo=FALSE, fig.height=15, fig.width=15}
models_metrics = unique(dat$Metric)

for(i in seq_along(models_metrics)){
  nam <- paste("A", i, sep = "")
  print(models_metrics[i])
  
  curr_data = dat.voice %>%
    filter(Metric==models_metrics[i])
  
  p <- ggplot(data=curr_data)+
  geom_point(mapping=aes(x=active, y=passive))+
  facet_wrap(~TrialType)+
  geom_abline()+
  theme_classic()+
  ggtitle(paste(models_metrics[i]))
  
  assign(nam, p)
}

if (which_models=="all") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows) #generates g
} else if (which_models=="anns") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
} else { #if which_models == "baselines"
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  }
savename <- "6_activePassive_byitem_split.png"
ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm', g)
```

# SYNONYMS

## By item

```{r, echo=FALSE}
dat.syn = dat %>%
  filter(!is.na(SynonymPair)) %>%
  dplyr::select(-ItemNum) %>%
  group_by(TrialType, NumSyn, SynonymPair, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(NumSyn, meanScore)

ggplot(data=dat.syn)+
  geom_point(mapping=aes(x=Version1, y=Version2, color=Voice))+
  facet_wrap(~TrialType)+
  geom_abline()+
  theme_classic()

savename <- "7_synonyms_all.png"
ggsave(paste(savedir,savename,sep="/"), width=16, height=12, units='cm')
```

```{r, echo=FALSE, fig.height=15, fig.width=15}

for(i in seq_along(models_metrics)){
  nam <- paste("A", i, sep = "")
  print(models_metrics[i])
  
  curr_data = dat.syn %>%
    filter(Metric==models_metrics[i])
  
  p <- ggplot(data=curr_data)+
  geom_point(mapping=aes(x=Version1, y=Version2, color=Voice))+
  facet_wrap(~TrialType)+
  geom_abline()+
  theme_classic()+
  ggtitle(paste(models_metrics[i]))
  
  assign(nam, p)
}

if (which_models=="all") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, ncol=ncols, nrow=nrows) #generates g
} else if (which_models=="anns") {
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, ncol=ncols, nrow=nrows)
} else { #if which_models == "baselines"
  grid.arrange(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  g <- arrangeGrob(A1, A2, A3, A4, A5, A6, A7, ncol=ncols, nrow=nrows)
  }
savename <- "7_synonyms_split.png"
ggsave(paste(savedir,savename,sep="/"), width=ncols*10, height=nrows*7, units='cm', g)
```


