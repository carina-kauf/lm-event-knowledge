---
title: "2021-10-20_Comparison_models_humans"
output: html_document
---


# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)


#SET ENVIRONMENT VARIABLES
which_models <- "all_main" #can be "anns_all", "anns_main", "baselines", "all", "all_main"
normalization <- "min-max" #can be "min-max", "zscore"

path <- paste("results_October2021/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste("results_October2021/EventsRevONLY_Models=",which_models)
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

remove_axis <- theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#uppercase first letter of string (needed for fast_vector_sum)
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

#function for sharing legends #https://github.com/tidyverse/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)

  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))

  grid.newpage()
  grid.draw(combined)

  # return gtable invisibly
  invisible(combined)

}

# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on EventsRev dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  ## Exclude fast vector sum model for mismatch of sentences >> to be fixed! (exlusion list excludes more than in other sentences)
  if (grepl("fast_vector_sum", filename, fixed = TRUE)  == TRUE) {
    message(paste(">> Model", filename, "is mismatched in sentences >> excluding it!!"))
    return(NULL)
  }
  
  #ROWS
  #set target number of *rows* in file
  if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
    target_rownumber = 1648
  } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
    target_rownumber = 80
  } else {
    print(paste("unknown experiment for file: ", filename))
    target_rownumber = 0
  }
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==6) {
      
      # 0. PREPARATION (set #of trials per item)
      if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
        target_trialnr = 4
      } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
        target_trialnr = 2
      } else {
        print(paste("unknown experiment for file: ", filename))
        target_trialnr = 0
      }
    
    #streamline input format
    if (ncol(d)==3){
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    else if (ncol(d)==4){
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Plausibility column to streamline how it's assigned
                           #(e.g. all lowercase etc))
      rename(SentenceNum=V1, Sentence=V2, Score=V4) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == FALSE)) { #If surprisal_scores_tinylstm
      #Add sentence number from 0 to len(dataset) -1
      tgt_len = nrow(d)-1
      sentnums = c(0:tgt_len)
      d = d  %>%
      select(V1,V5) %>% #select relevant columns (others include UNKification for EventsAdapt dataset)
      rename(Sentence=V1, Score=V5) %>%
      mutate(SentenceNum = sentnums) %>% #This adds a column SentenceNum from 0 to len(dataframe)-1
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == TRUE)) {
      d = d  %>%
      select(V1,V3,V6) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V6) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    d = d %>%
      mutate(Plausibility = ifelse(SentenceNum%%2==0, 'Plausible', 'Implausible')) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip EventsAdapt dataset prefix
      mutate(Metric = str_replace(Metric, "new[_-][Ee]ventsAdapt[_\\.]", "")) %>%
      mutate(Metric = str_replace(Metric, "newsentences[_-]EventsAdapt[_\\.]", "")) %>%
      #strip EventsRev dataset prefix
      mutate(Metric = str_replace(Metric, "ev1[_\\.]", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline1_ppmi", "syntax-based_PPMI")) %>% #EventsAdapt
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline2_ppmi", "order-based_PPMI")) %>% #EventsAdapt
      ##Assimilate thematic fit model names between datasets
      mutate(Metric = str_replace(Metric, "update-model.TF-add", "thematicFit_addition")) %>%
      mutate(Metric = str_replace(Metric, "update-model.TF-prod", "thematicFit_product")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale
      #Note: One entire quadruplet (SentenceNum 1516, The editor overheard the proofreader .) has score 0.0
      # >> add 1e-30 to enable log scaling & ask Emmanuele why this is 0!
    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all models
    if (grepl("min-max", normalization) == TRUE){
      d = d %>%
      mutate(NormScore = min_max(Score))
    } else {
      d = d %>%
      mutate(NormScore = scale_this(Score))
    }

      # 4. PROCESS SENTENCES
      d = d  %>%
      #strip space before final period for alignment with TrialTypes etc below
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", ".")) %>%
      #for some reason fast_vector_sum does not have final periods now.
      mutate(Sentence = ifelse(endsWith(Sentence, "."),Sentence,paste(Sentence, ".", sep=""))) %>%
      #uppercase first word in sentence to align with other model sentence sets
      mutate(Sentence = ifelse(Metric == "fast_vector_sum",firstup(Sentence),Sentence))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

### Print environment variables 
```{r}
print(paste(savedir))
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization))

#for plotting the dotted reference line
if (grepl("min-max", normalization) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
```

# Read sentence freq data

```{r}
dirname = '../word_frequency_info'

sentence_file_er = paste(dirname, 'EventsRev_freqs.csv', sep='/')
sentence_file_ea = paste(dirname, 'EventsAdapt_freqs.csv', sep='/')

# read in & normalize predictors
dat_er_sentence = read.csv(sentence_file_er) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))
dat_er_sentence$ItemNum = dat_er_sentence$ItemNum + 900 
dat_er_sentence$TrialType = 'AAN'         
dat_er_sentence$Voice = 'active'

# combine
dat_sentence = dat_er_sentence
dat_sentence$Plausibility = recode(dat_sentence$Plausibility, 
                             implausible='Implausible', plausible='Plausible',
                             implaus="Implausible", plaus="Plausible") 
```

# Read human data

```{r}

# READ
dirname = '../human_ratings'
ratings_eventsRev = paste(dirname, '1_EventsRev_plausibility', 'analyses', 'longform_data.csv', sep='/')

dat_er = read.csv(ratings_eventsRev) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence) %>%
  filter(ItemNum<41)    # exclude attention checks

# CLEAN
dat_er$Plausibility = recode(dat_er$Plausibility, implaus='Implausible', plaus='Plausible')
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'
dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap

human_dat = dat_er

human_dat$TrialType = factor(human_dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
human_dat$
```

Initial mean, but to be filtered later on!

```{r pressure, echo=FALSE}
human_dat.mean = human_dat %>% 
  group_by(ItemNum, Sentence, Plausibility, TrialType, Voice, Experiment) %>% #, SynonymPair, NumSyn) %>% 
  summarize(MeanScore = mean(Score, na.rm=TRUE)) %>%
  ungroup() 
human_dat.mean$ItemNum = as.factor(human_dat.mean$ItemNum)

#scale scores

#3.2 Normalize scores for all models
if (grepl("min-max", normalization) == TRUE){
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = min_max(MeanScore))
} else {
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = scale_this(MeanScore))
}
```

# Read model info

```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_eventsRev = paste(dirname, 'EventsRev', sep='/')

message("Loading EventsRev data ...")
files_er = list.files(path=dirname_eventsRev, pattern="*.txt")
dat_er = do.call(rbind, lapply(files_er, function(x) read_data(dirname_eventsRev, x)))
dat_sentence$ItemNum = as.factor(dat_sentence$ItemNum)
```

## Merge with sentence info

Load & merge with original sentence file for information about TrialType & Voice

NOTE: Some of the sentences were different from the human experiment! >> take from dropbox, NOT behavioral study!
Maybe we should also just exclude these sentences
The file we use is downloaded from Dropbox and named here as 'newsentences_EventsAdapt_smallfixes.csv' to avoid confusion with the 'newsentences_EventsAdapt.csv' file in the behavioral study folder

ANYA: currently excluding all the sentences where the match isn't exact

## EventsRev
```{r read data, echo=FALSE}
dat_er = merge(dat_er, dat_sentence, by=c("Sentence", "Plausibility"))
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'          
dat_er$Voice = 'active'
dat_er$ItemNum = dat_er$ItemNum.y
dat_er = dat_er %>% select(-ItemNum.x, ItemNum.y)

# check which sentences don't have a plausibility pair
sentence_pairnum = dat_er %>%
  group_by(ItemNum, TrialType, Voice, Metric) %>%
  summarize(NumSentences = length(ItemNum))
single_sentences = sentence_pairnum %>%
  filter(NumSentences==1)

# filter them out
dat_er = dat_er %>%
  filter(!(ItemNum %in% single_sentences$ItemNum))
dat = dat_er
```

## Add human data
```{r}
# only include sentences present in model data
human_dat.mean = human_dat.mean %>%
  filter(ItemNum %in% unique(dat$ItemNum)) %>%
  select(Sentence, Experiment, MeanScore, NormScore) %>%
  rename(Score=MeanScore)
human_dat.mean = merge(human_dat.mean, dat_sentence, by=c("Sentence"))

# Use ItemNum from model data
dat_sentencenum = dat %>%
  group_by(Sentence) %>%
  summarize(ItemNum = ItemNum[1])
human_dat.mean = merge(human_dat.mean %>% select(-ItemNum), dat_sentencenum)

# combine
human_dat.mean$Metric = "human"
dat = rbind(dat %>% select(-SentenceNum, -ItemNum.y), human_dat.mean)
```


# Select which models/metrics to plot

NOTE: the current version will fail to include all ANNs, might need to fix

```{r, echo=FALSE}

human <- c("human")
anns <- c("gpt2","bert","xlnet","roberta")
anns_main <- c("gpt2-xl_full","bert-large-cased.sentence-probs-pseudolog","roberta-large.sentence-probs-pseudolog","xlnet-large-cased.sentence-probs-pseudolog")
baselines <- c("syntax-based_PPMI","order-based_PPMI","thematicFit_addition","v2_sdm","surprisal_scores_tinylstm") #"fast_vector_sum", "thematicFit_product",
all_main <- c(human, anns_main, baselines)

# select which to use
chosen_models = eval(parse(text=which_models))
model_dat = dat %>% filter(Metric %in% chosen_models)

# simplify ANN names
if (which_models %in% c("all_main", "anns_main")){
  model_dat = model_dat %>% 
    mutate(Metric = str_replace(Metric, ".sentence-probs-pseudolog", ""))
}

message("Using these models/metrics:")
print(unique(model_dat$Metric))
```

# PLAUSIBILITY

We need to define for which models a higher score for plausible sentences is desirable:

1. HIGHER = MORE PLAUSIBLE
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a plausible sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE PLAUSIBLE
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

model_dat.binchoice = model_dat %>%
  #create filter criterion
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  group_by(ItemNum, TrialType, Voice, Metric,LowerBetter) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff+1, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0))

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Plausible is more positive!)
# Take out/plot NormScore below if confusing!
model_dat = model_dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore+1, NormScore))
```

# Sentence exclusion
We have to exclude certain items because there's not a full overlap of stimuli between human and model data. We cannot exclude based on item number or synonym pair number because of general misalignment (some numbers are missing)
The below exclusion keywords are obtained from running the check that finds differences in the datasets for models and humans below

```{r}
# NOTE: not all sentences are SVO; sometimes it's the agent and the recipient who are switched
exclusion_keywords = c(
                       'sportsstar', #922 no synonym
                       'applauding' #927 no synonym
                       )
```

## exclude from dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset
message(paste(length(unique(model_dat$Sentence))))
model_dat = model_dat %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(model_dat$Sentence))))

#order by itemNum for correct assignment
model_dat = arrange(model_dat, ItemNum)

models = unique(model_dat$Metric)
df = NA

for(i in seq_along(models)){ 
  print(paste(models[i]))
  curr_dataset = subset(model_dat, Metric == models[i])
  num_ER = length(subset(curr_dataset, Experiment=='EventsRev')$Sentence) %/% 2
  print(paste(num_ER, ',', num_ER%*%2))
  itemNums_forSynsets <- rep(901:(900+num_ER), each=2)
  
  curr_dataset = curr_dataset %>%
    rename(ItemNum_OLD=ItemNum) %>%
    mutate(ItemNum = itemNums_forSynsets) %>%
    mutate(SynonymPair = ifelse(ItemNum>248, NA, (ItemNum+1)%/%2)) %>% #changed from 900 to 248 CK
    mutate(NumSyn = ifelse(ItemNum>248, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) #changed from 900 to 248 CK

  if (is.na(df)){
    df = curr_dataset
  } else {
    df = merge(df,curr_dataset, all=TRUE)
  }
}
```

# CONTRASTS

```{r}
dat = model_dat
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
models_metrics = as.vector(sapply(chosen_models, 
                                  function(x) {str_replace(x, ".sentence-probs-pseudolog", "")}))
nr_models = length(models_metrics)
ncols=5
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

if (endsWith(paste(which_models),"_main")){
  aan_group = sapply(anns_main, function(x) {str_replace(x, ".sentence-probs-pseudolog", "")})
} else {
  aan_group = sapply(anns, function(x) {str_replace(x, ".sentence-probs-pseudolog", "")})
}

dat.binchoice = dat %>%
  #add color
  mutate(Category = ifelse(Metric%in%aan_group, "ANNs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category)) %>%
  #create filter criterion
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter,Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup()

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=models_metrics)
dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "ANNs", "baselines"))
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("AI", "AAN", "AAR"))
```

### Stats

```{r}
calculate_binom_pval <- function(numCorrect, numTotal) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ binom.test(numCorrect[i], numTotal[i])$p.value}))
}

# get p values
dat.binchoice.summary = dat.binchoice %>%
  filter(TrialType %in% c("AI", "AAN")) %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(Accuracy=NumCorrect/NumTotal) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  ungroup()
```

Compare with human data (Chi Square)

```{r}
# get the X-square and p-value of the X-square 2 sample test. Assume the total num is the same
calculate_chisq <- function(a, b, total) {
  result = prop.test(c(a, total), c(b, total))
  return(c(as.numeric(result$statistic), result$p.value))
}

calculate_chisq_vectorized_chi <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[1]}))
}

calculate_chisq_vectorized_p <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[2]}))
}

# get human responses separately
human.results = dat.binchoice.summary %>%
  filter(Metric=='human') %>%
  select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)
dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)

dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = ifelse(pVal2humansAdjusted<0.001, "***", 
                         ifelse(pVal2humansAdjusted<0.01, "**",
                                ifelse(pVal2humansAdjusted<0.05, "*", "")))) 
```

### Plot 

```{r, echo=FALSE}

plot.binacc = ggplot(data=subset(dat.binchoice, TrialType%in%c("AI","AAN")), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 1)+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun.y='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x='Model')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
plot.binacc

savename <- "1_binaryAccuracy_split.png"
ggsave(paste(savedir,savename,sep="/"), width=15, height=12, units='cm')
```


# MODEL HUMAN COMPARISON

## General settings

```{r}
# Plotting only humans and ANNs for now - will put baselines in the appendix
which_models = "anns_main"
metrics2plot = c(human, anns_main)
metrics2plot = as.vector(sapply(metrics2plot, 
                                  function(x) {str_replace(x, ".sentence-probs-pseudolog", "")}))

dat2plot = dat %>% 
  filter(Metric %in% metrics2plot)
```

## Calculate human 2 human response correlation 
(each human with the average of all the rest)
```{r}
human_dat.comp = human_dat %>%
  filter(!is.na(Score)) %>%
  mutate(FinalNormScore = min_max(Score)) %>%
  select(WorkerId, ItemNum, TrialType, Plausibility, Voice, Sentence, FinalNormScore)

workerIDs = unique(human_dat.comp$WorkerId)
df_correlation = data.frame()

for (i in seq_along(workerIDs)) {
  human_dat.this = human_dat.comp %>% filter(WorkerId==workerIDs[i])
  human_dat.rest = human_dat.comp %>% filter(WorkerId!=workerIDs[i]) %>%
    filter(ItemNum %in% unique(human_dat.this$ItemNum)) %>%
    group_by(ItemNum, Plausibility, Voice, Sentence) %>% 
    summarize(MeanNormScore=mean(FinalNormScore))
  
  human_dat.side2side = merge(human_dat.this, human_dat.rest,
                              by=c("ItemNum", "Plausibility", "Voice"))
  corval= cor(human_dat.side2side$FinalNormScore, human_dat.side2side$MeanNormScore)
  # add vector to a dataframe
  df <- data.frame(workerIDs[i], corval)
  df_correlation <- rbind(df_correlation,df)
}

human2human_corr = mean(df_correlation$corval)
print(paste("The human2human overall correlation value is", human2human_corr))
human2human_corr_sd = sd(df_correlation$corval)
print(paste("SD", human2human_corr_sd))
```

## Sample a random human response for each item 
And compute the avg of all other responses 

```{r}
set.seed(21)
human_dat.sampled = human_dat.comp %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Sentence) %>%
  summarize(RandomScore = sample(FinalNormScore,1),
            MeanScoreRest = (sum(FinalNormScore)-RandomScore)/(length(FinalNormScore)-1)) %>%
  ungroup()
```

## Prep data

```{r}
dat.human = dat2plot %>% filter(Metric=="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore)
dat.model = dat2plot %>% filter(Metric!="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore, Metric)
dat.model2human = merge(dat.human, dat.model, 
                        by=c("ItemNum", "TrialType", "Sentence", "Plausibility")) %>%
  rename(HumanScore=FinalNormScore.x, ModelScore=FinalNormScore.y)

# add human2human data
human_dat.sampled = human_dat.sampled %>%
  select(-Voice) %>%
  filter(Sentence %in% unique(dat.model2human$Sentence)) %>%  # make sure it's the same sents
  rename(HumanScore=MeanScoreRest, ModelScore=RandomScore)    # renaming for convenience 
human_dat.sampled$Metric = 'singleHuman'
dat.model2human = rbind(dat.model2human, human_dat.sampled)
```

Formatting:

```{r, echo=FALSE, fig.height=12, fig.width=15}
dat.model2human = dat.model2human %>%
  #add color info
  mutate(Category = ifelse(Metric%in%aan_group, "ANNs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="singleHuman", "human", Category))

# specify order for plotting
metric_names = metrics2plot
metric_names[1] = "singleHuman"
dat.model2human$Metric = factor(dat.model2human$Metric, levels=metric_names)
dat.model2human$Category = factor(dat.model2human$Category, levels=c("human", "ANNs", "baselines"))
dat.model2human$TrialType = factor(dat.model2human$TrialType, levels=c("AI", "AAN", "AAR"))
```

## Calculate correlations for the plot

```{r}
df_correlation = data.frame()

trialTypes = unique(dat.model2human$TrialType)
for (t in seq_along(trialTypes)){
  dat.model2human.tt = dat.model2human %>%
    filter(TrialType==trialTypes[t])
  
  # get pairwise correlation values for humans and each model
  val1.human = dat.model2human.tt$ModelScore[dat.model2human.tt$Metric=="singleHuman"]
  val2.human = dat.model2human.tt$HumanScore[dat.model2human.tt$Metric=="singleHuman"]
  
  for(i in seq_along(metric_names)){
      current_metric=metric_names[i]
      val1.model = dat.model2human.tt$ModelScore[dat.model2human.tt$Metric==current_metric]
      val2.model = dat.model2human.tt$HumanScore[dat.model2human.tt$Metric==current_metric]
      pval2zero = cor.test(val1.model, val2.model, method="pearson")$p.value
      corvals = c(
        cor(val1.human, val2.human, method="pearson"),
        cor(val1.model, val2.model, method="pearson"),
        cor(val1.human, val1.model, method="pearson"),
        cor(val1.human, val2.model, method="pearson"),
        cor(val2.human, val1.model, method="pearson"),
        cor(val2.human, val2.model, method="pearson"))
      test2humans = cocor.dep.groups.nonoverlap(corvals[1], corvals[2], corvals[3],
                                         corvals[4], corvals[5], corvals[6], 
                                         n=length(val1.model),
                                         test='raghunathan1996')
      pval2humans= test2humans@raghunathan1996$p.value
      
      # add vector to a dataframe
      df <- data.frame(current_metric, trialTypes[t], corvals[2], pval2zero, pval2humans)
      df_correlation <- rbind(df_correlation,df)
  }
}
colnames(df_correlation) = c("Metric", "TrialType", "Correlation", "pVal2zero", "pVal2humans")

# adjust for multiple comparisons
df_correlation = df_correlation %>%
  mutate(pVal2zeroAdjusted = p.adjust(pVal2zero, method="fdr", n=length(pVal2zero)),
         pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans))) %>%
  mutate(pVal2zeroLabel = ifelse(pVal2zeroAdjusted<0.001, "***", 
                                 ifelse(pVal2zeroAdjusted<0.01, "**",
                                        ifelse(pVal2zeroAdjusted<0.05, "*", ""))),
         pVal2humansLabel = ifelse(pVal2humansAdjusted<0.001, "***", 
                                 ifelse(pVal2humansAdjusted<0.01, "**",
                                        ifelse(pVal2humansAdjusted<0.05, "*", ""))))
```

## Plot
```{r}

# text size 
cortext_size = 3

plot.model2human = ggplot(dat=dat.model2human)+  
  geom_point(mapping=aes(x=ModelScore, y=HumanScore, color=Category), 
             size=0.2, position=position_jitter(width=0.01))+
  facet_wrap(~Metric, ncol=ncols)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_text(mapping=aes(x=-0.25, y=1, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1), xlim=c(-0.2,1))+
  theme_classic()+
  theme(legend.position="none")
plot.model2human

savename <- "model2human_byTrialType_ANNs.png"
ggsave(paste(savedir,savename,sep="/"), height=15,width=25, units='cm')
```


# SCATTERPLOTS

## General settings
```{r}
# Plotting only humans and ANNs for now - will put baselines in the appendix
which_models = "anns_main"
metrics2plot = c(human, anns_main)
metrics2plot = as.vector(sapply(metrics2plot, 
                                  function(x) {str_replace(x, ".sentence-probs-pseudolog", "")}))

dat2plot = dat %>% 
  filter(Metric %in% metrics2plot)

# color scheme
color_plaus = '#1b9e77'
color_voice = '#d95f02'
color_syn = '#7570b3'

```

## PLAUSIBLE VS IMPLAUSIBLE

### Prep data

```{r, echo=FALSE}
dat.plaus = dat2plot %>% 
  dplyr::select(ItemNum, TrialType, Plausibility, Voice, Metric, NormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(NormScore, na.rm=TRUE)) %>%
  spread(Plausibility, meanScore)  
```

### Calculate correlations

```{r}
df_correlation = data.frame()

# get pairwise correlation values for humans and each model
val1.human = dat.plaus$Plausible[dat.plaus$Metric=="human"]
val2.human = dat.plaus$Implausible[dat.plaus$Metric=="human"]

for(i in seq_along(metrics2plot)){
    val1.model = dat.plaus$Plausible[dat.plaus$Metric==metrics2plot[i]]
    val2.model = dat.plaus$Implausible[dat.plaus$Metric==metrics2plot[i]]
    pval2zero = cor.test(val1.model, val2.model, method="pearson")$p.value
    corvals = c(
      cor(val1.human, val2.human, method="pearson"),
      cor(val1.model, val2.model, method="pearson"),
      cor(val1.human, val1.model, method="pearson"),
      cor(val1.human, val2.model, method="pearson"),
      cor(val2.human, val1.model, method="pearson"),
      cor(val2.human, val2.model, method="pearson"))
    test2humans = cocor.dep.groups.nonoverlap(corvals[1], corvals[2], corvals[3],
                                       corvals[4], corvals[5], corvals[6], 
                                       n=length(val1.model),
                                       test='raghunathan1996')
    pval2humans= test2humans@raghunathan1996$p.value
    
    # add vector to a dataframe
    df <- data.frame(models_metrics[i], corvals[2], pval2zero, pval2humans)
    df_correlation <- rbind(df_correlation,df)
}
colnames(df_correlation) = c("Metric", "Correlation", "pVal2zero", "pVal2humans")

# adjust for multiple comparisons 
df_correlation = df_correlation %>%
  mutate(pVal2zeroAdjusted = p.adjust(pVal2zero, method="fdr", n=length(pVal2zero)),
         pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans))) %>%
  mutate(pVal2zeroLabel = ifelse(pVal2zeroAdjusted<0.001, "***", 
                                 ifelse(pVal2zeroAdjusted<0.01, "**",
                                        ifelse(pVal2zeroAdjusted<0.05, "*", ""))),
         pVal2humansLabel = ifelse(pVal2humansAdjusted<0.001, "***", 
                                 ifelse(pVal2humansAdjusted<0.01, "**",
                                        ifelse(pVal2humansAdjusted<0.05, "*", ""))))
```

### Plot
```{r}
plot.plaus = ggplot(data=dat.plaus)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_text(mapping=aes(x=0.2, y=1, label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation, size=cortext_size)+
  coord_cartesian(ylim=c(0,1), xlim=c(0,1))+
  theme_classic()+
  theme(plot.title = element_text(hjust=0.5,face='bold'))
plot.plaus
#paste("r",round(Correlation,2),sep=" = ")
```


## COMBINE

```{r}
plot.binacc /  (plot.model2human | plot.plaus)

savename <- "composite.png"
ggsave(paste(savedir,savename,sep="/"), height=24,width=28, units='cm')
```


#REGRESSION

## Contrasts

```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```

## Stats

```{r regression, echo=FALSE}

metrics = unique(dat$Metric)
results = data.frame()

for(i in seq_along(metrics)){
  metric = metrics[i]
  dat.metric = dat %>% filter(Metric==metric)
  m = lmer(NormScore ~ Plausibility + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + (1|ItemNum), 
           data=dat.metric, REML=FALSE, 
           control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))
 d = data.frame(coef(summary(m)))
 d = d %>% rename(pVal=Pr...t..)
 d$Metric = metric 
 d$Parameter = rownames(d)
 d = d[,c(6,7,1:5)]
 results = rbind(results, d)
}

# split up
results.ANN = results %>%
  filter(Metric %in% metrics2plot)
results.baselines = results %>%
  filter(!(Metric %in% metrics2plot))

# adjust for multiple comparisons
results.ANN = results.ANN %>%
  group_by(Parameter) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal))) %>%
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  mutate(pValFinal = ifelse(pValAdjusted<0.001, "<0.001", as.character(round(pValAdjusted,3)))) %>%
  ungroup()

results.baselines = results.baselines %>%
  group_by(Parameter) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)+1)) %>%   # account for humans too
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  mutate(pValFinal = ifelse(pValAdjusted<0.001, "<0.001", as.character(round(pValAdjusted,3)))) %>%
  ungroup()

savename <- "regression_by_metric.csv"
write.csv(results, paste(savedir,savename,sep="/"), row.names=FALSE)
```








