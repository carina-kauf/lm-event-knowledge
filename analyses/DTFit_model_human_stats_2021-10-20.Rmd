---
title: "DTFit_model_analysis_v2021-08-20"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(gridExtra)

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on DTFit dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  
  #ROWS
  #set target number of *rows* in file
  target_rownumber = 798
  
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==5 | ncol(d)==7) {
      
    target_trialnr = 2
    
    #streamline input format
    if (ncol(d)==3) {
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3)
    }
    else if (ncol(d)==4) {
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Typicality column to streamline how it's assigned)
      rename(SentenceNum=V1, Sentence=V2, Score=V4)
    }
    
    else if (ncol(d)==5) { #If surprisal_scores_tinylstm
      d = d  %>%
      select(V1,V2,V5) %>% 
      rename(SentenceNum=V1, Sentence=V2, Score=V5)
    }
    
    else if (ncol(d)==7) {
      d = d  %>%
      select(V1,V3,V7) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V7)
    }
    
    d = d %>%
      mutate(Typicality = ifelse(SentenceNum%%2==0, 'Atypical', 'Typical')) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip dataset prefix
      mutate(Metric = str_replace(Metric, "dtfit_vassallo_|dtfit.", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>%
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale
    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = replace(Score, Score == 0.0, 1e-20)) %>% #Take out once 0.0 is fixed!
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all models
    d = d %>%
      mutate(NormScore =min_max(Score)) %>%

      # 4. PROCESS SENTENCES
      #strip space before final period
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", "."))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

#SET ENVIRONMENT VARIABLES
```{r, echo=FALSE}
which_models <- "anns" #can be "anns", baselines" or "all"
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
```

## Below yields: 11172/798 = 14 models
```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_dtfit = paste(dirname, 'DTFit_scores_new_format', sep='/')

message("Loading DTFit data ...")
files = list.files(path=dirname_dtfit, pattern="*.txt")
dat = do.call(rbind, lapply(files, function(x) read_data(dirname_dtfit, x)))
```

###### PREPROCESS DATA FILES ######

## DTFIT
```{r read data, echo=FALSE}

dat$TrialType = 'DTFit'
```

### Filter out non-ANN models
```{r, echo=FALSE}

#print(unique(dat$Metric))
anns <- c("gpt2","bert","xlnet","roberta")

#filter out non-ANN models
if (which_models == "anns"){
  dat = dat %>%
  filter(grepl(paste(anns, collapse = "|"),dat$Metric))
  message("Using only ANN models for analysis")
} else if (which_models == "baselines"){
  dat = dat %>%
  filter(!grepl(paste(anns, collapse = "|"),dat$Metric))
  dat$Metric = factor(dat$Metric, levels=c("syntax-based_PPMI","order-based_PPMI","thematicFit_addition",
    "thematicFit_product","v2_sdm","fast_vector_sum","surprisal_scores_tinylstm"))
  message("Using only baseline models for analysis")
} else {
  message("Using all models for analysis")
}

print(unique(dat$Metric))
```

# TYPICALITY

We need to define for which models a higher score for typical sentences is desirable:

1. HIGHER = MORE TYPICAL
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a typical sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE TYPICAL
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

dat.binchoice = dat %>%
  #create filter criterion
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  group_by(ItemNum, Metric, LowerBetter, TrialType) %>%
  summarize(ScoreDiff = NormScore[Typicality=="Typical"]-NormScore[Typicality=="Atypical"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0))

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Typical is more positive!)
# Take out/plot NormScore below if confusing!
dat = dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore, NormScore))

# to be consistent with human data
dat$ItemNum = dat$ItemNum+1
```

# Combine with other data

## Sentence info
```{r}
dirname = '../word_frequency_info'

sentence_file_dtfit = paste(dirname, 'DTFit_freqs.csv', sep='/')

# read in & normalize predictors
dat_dtfit_sentence = read.csv(sentence_file_dtfit) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq)) %>%
  rename(Score=Rating, Plausibility=Typicality)
dat_dtfit_sentence$Plausibility = recode(dat_dtfit_sentence$Plausibility, 
                             AT='Implausible', T='Plausible') 
```

## Human ratings
```{r}
dirname = '../human_ratings'
ratings_dtfit = paste(dirname, '3_DTFIT', 'newformat_curated_human_ratings.csv', sep='/')

dat_human = read.csv(ratings_dtfit) %>%
  rename(Score=Rating, ItemNum=Item)
dat_human$Metric = 'human' 
dat_human$FinalNormScore = min_max(dat_human$Score)

# combine all
dat = rbind(dat %>% select(ItemNum, Sentence, Score, Typicality, Metric, FinalNormScore),
           dat_human %>% select(ItemNum, Sentence, Score, Typicality, Metric, FinalNormScore))

dat = dat %>% rename(Plausibility=Typicality)
dat$Plausibility = recode(dat$Plausibility, 
                             AT='Implausible', T='Plausible',
                             Atypical='Implausible', Typical='Plausible') 

dat = merge(dat, dat_dtfit_sentence, 
               by=c("Sentence", "Plausibility")) 

```

# STATS

### Contrasts
```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```

### Fit

```{r regression, echo=FALSE}
m = lmer(FinalNormScore ~ Plausibility*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1|ItemNum), 
         data=dat, REML=FALSE)
sink("regression_model_DTFit.txt", append=FALSE)
summary(m)
sink()
summary(m)
```
