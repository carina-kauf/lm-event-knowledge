---
title: "scaling"
output: html_document
date: '2023-08-18'
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(ggh4x)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(gtools)
library(cowplot)
library(png)

source('dataloader_utils.R') #includes normalizations, read_data functions
source('stats_utils.R')

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
```

### Print environment variables 
```{r}
#SET ENVIRONMENT VARIABLES

# can be: "EventsAdapt", "DTFit_EventsRev"
experiment <- "EventsAdapt"
which_models <- "scaling" 
normalization_type <- "min-max" 

path <- paste("results/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste(path,"Scaling",sep='')
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

print(paste(savedir))
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization_type))

#for plotting the dotted reference line
if (grepl("min-max", normalization_type) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
normalization <- get_normalization_fn(normalization_type)
```

# READ DATA
```{r}
#created via preprocess_scores.Rmd
if (grepl("EventsAdapt", experiment)) {
  dat = read.csv('clean_data/clean_EventsAdapt_df.csv')
  human_dat = read.csv('clean_data/clean_EventsAdapt_human_dat.csv')
} else {
  dat = rbind(read.csv('clean_data/clean_EventsRev_df.csv'), read.csv('clean_data/clean_DTFit_df.csv'))
  #human_dat = read.csv('clean_data/clean_EventsRev_human_dat.csv')
  #human_dat$TrialType = "AAN"
}

```

# Select which models/metrics to plot

```{r, utils.choose_models, echo=FALSE}
human <- c("human")
scaling <- c("DistilGPT-2.sentence-LL", "GPT-2.sentence-LL", "GPT-2-medium.sentence-LL", "GPT-2-large.sentence-LL", "GPT-2-xl.sentence-LL", "GPT-J.sentence-LL", "MPT-30b.sentence-LL")

scaling <- c(scaling, human)

all_metrics = unique(dat$Metric) 
llm_metrics = as.character(all_metrics[str_detect(all_metrics, 'GPT|MPT')])
llms_all <- c(human,llm_metrics)

# select which to use
chosen_models = eval(parse(text=which_models))
dat = dat %>% filter(Metric %in% chosen_models)

models_order = c("DistilGPT-2.sentence-LL", "GPT-2.sentence-LL", "GPT-2-medium.sentence-LL", "GPT-2-large.sentence-LL", "GPT-2-xl.sentence-LL", "GPT-J.sentence-LL", "MPT-30b.sentence-LL")
short_names <- str_replace(models_order, "\\.sentence-LL", "")
short_names <- c(short_names, human)
chosen_models = chosen_models[order(match(chosen_models,models_order))]
dat$Metric = relevel(factor(dat$Metric), levels = chosen_models, ref="human")
message("Using these models/metrics:")
print(chosen_models)
```

# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
nr_models = length(chosen_models)
ncols=nr_models #models plus human
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}

cortext_size = 4
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

#add Category color
dat.binchoice = dat %>%
  mutate(Category = ifelse(Metric%in%scaling, "LLMs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category))

dat.sentences = dat.binchoice %>%
  filter(Plausibility=="Plausible") %>% 
  dplyr::select(ItemNum, TrialType, Voice, Metric, LowerBetter, Category, Sentence)

dat.binchoice = dat.binchoice %>%
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup() %>%
  inner_join(dat.sentences)

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=chosen_models)
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("DTFit", "AI", "AAN", "AAR"))

dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("LLMs", "human"))
```


```{r}
# leave only active sentences here (since SDM & PPMI-syntax cannot deal with passive structures, put full plot in SI)
dat.binchoice.active = dat.binchoice %>% filter(Voice=="active")
```

## Stats

```{r}
# get p values
dat.binchoice.summary = dat.binchoice.active %>%
  filter(!(TrialType=="AAR")) %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  ungroup()
```

```{r}
# get human responses separately
human.results = dat.binchoice.summary %>%
  filter(Metric=='human') %>%
  dplyr::select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)

dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = plabel(pVal2humansAdjusted)) 

# print the result
for (i in seq_along(dat.binchoice.summary.withchisq$Metric)) {
  print(paste(dat.binchoice.summary.withchisq$Metric[i], ": ",
        round(dat.binchoice.summary.withchisq$AccuracyScore[i],2), 
        ", Ï‡2=", round(dat.binchoice.summary.withchisq$ChiSq[i],2), 
        ", p=", round(dat.binchoice.summary.withchisq$pVal2humansAdjusted[i],3),
        ";", sep=""))
}
```

## Plot

```{r}
label_names <- c(
  "DTFit" = "animate-inanimate, unlikely",
  "AI" = "animate-inanimate, impossible",
  "AAN" = "animate-animate, unlikely",
  "AAR" = "animate-animate\n(control)"
)
```

```{r, echo=FALSE}
shorten_metric_names <- function(col) { gsub(".sentence-LL", "", col) }

dat.binchoice.active = dat.binchoice.active %>%
  mutate(Metric = shorten_metric_names(Metric))
dat.binchoice.summary = dat.binchoice.summary %>%
  mutate(Metric = shorten_metric_names(Metric))

dat.binchoice.active$Metric = factor(dat.binchoice.active$Metric, levels=short_names)
dat.binchoice.summary$Metric = factor(dat.binchoice.summary$Metric, levels=short_names)

plot.binacc = ggplot(data=subset(dat.binchoice.active, !(TrialType=="AAR")), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 2, labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))
plot.binacc

savename <- paste(experiment, "binaryAccuracy.png",sep="_")
ggsave(paste(savedir,savename,sep="/"), width=20, height=8, units='cm')
savename <- paste(experiment, "binaryAccuracy.svg",sep="_")
ggsave(paste(savedir,savename,sep="/"), width=20, height=8, units='cm')
```