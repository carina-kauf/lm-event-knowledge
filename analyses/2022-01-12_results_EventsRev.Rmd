---
title: "results_main"
output: html_document
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(ggExtra)

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)


#SET ENVIRONMENT VARIABLES
#use BOTH EventsAdapt AND EventsRev OR only EventsAdapt
use_eventsrev <- TRUE #set to FALSE if you don't want to also use EventsRev data for analyses
which_models <- "all_main" #can be "anns_all", "anns_main", "baselines", "all", "all_main"
normalization <- "min-max" #can be "min-max", "zscore"

path <- paste("results_Jan2022/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste("results_Jan2022/EventsRev=",use_eventsrev,"_Models=",which_models,"_Norm=",normalization)
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)


remove_axis <- theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#uppercase first letter of string (needed for fast_vector_sum)
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

#function for sharing legends #https://github.com/tidyverse/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)

  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))

  grid.newpage()
  grid.draw(combined)

  # return gtable invisibly
  invisible(combined)

}

# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on EventsRev dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  ## Exclude fast vector sum model for mismatch of sentences >> to be fixed! (exlusion list excludes more than in other sentences)
  if (grepl("fast_vector_sum", filename, fixed = TRUE)  == TRUE) {
    message(paste(">> Model", filename, "is mismatched in sentences >> excluding it!!"))
    return(NULL)
  }
  
  #ROWS
  #set target number of *rows* in file
  if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
    target_rownumber = 1648
  } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
    target_rownumber = 80
  } else {
    print(paste("unknown experiment for file: ", filename))
    target_rownumber = 0
  }
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==6) {
      
      # 0. PREPARATION (set #of trials per item)
      if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
        target_trialnr = 4
      } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
        target_trialnr = 2
      } else {
        print(paste("unknown experiment for file: ", filename))
        target_trialnr = 0
      }
    
    #streamline input format
    if (ncol(d)==3){
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    else if (ncol(d)==4){
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Plausibility column to streamline how it's assigned
                           #(e.g. all lowercase etc))
      rename(SentenceNum=V1, Sentence=V2, Score=V4) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == FALSE)) { #If surprisal_scores_tinylstm
      #Add sentence number from 0 to len(dataset) -1
      tgt_len = nrow(d)-1
      sentnums = c(0:tgt_len)
      d = d  %>%
      select(V1,V5) %>% #select relevant columns (others include UNKification for EventsAdapt dataset)
      rename(Sentence=V1, Score=V5) %>%
      mutate(SentenceNum = sentnums) %>% #This adds a column SentenceNum from 0 to len(dataframe)-1
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == TRUE)) {
      d = d  %>%
      select(V1,V3,V6) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V6) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    d = d %>%
      mutate(Plausibility = ifelse(SentenceNum%%2==0, 'Plausible', 'Implausible')) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip EventsAdapt dataset prefix
      mutate(Metric = str_replace(Metric, "new[_-][Ee]ventsAdapt[_\\.]", "")) %>%
      mutate(Metric = str_replace(Metric, "newsentences[_-]EventsAdapt[_\\.]", "")) %>%
      #strip EventsRev dataset prefix
      mutate(Metric = str_replace(Metric, "ev1[_\\.]", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline1_ppmi", "syntax-based_PPMI")) %>% #EventsAdapt
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline2_ppmi", "order-based_PPMI")) %>% #EventsAdapt
      ##Assimilate thematic fit model names between datasets
      mutate(Metric = str_replace(Metric, "update-model.TF-add", "thematicFit_addition")) %>%
      mutate(Metric = str_replace(Metric, "update-model.TF-prod", "thematicFit_product")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale
      #Note: One entire quadruplet (SentenceNum 1516, The editor overheard the proofreader .) has score 0.0
      # >> add 1e-30 to enable log scaling & ask Emmanuele why this is 0!
    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all models
    if (grepl("min-max", normalization) == TRUE){
      d = d %>%
      mutate(NormScore = min_max(Score))
    } else {
      d = d %>%
      mutate(NormScore = scale_this(Score))
    }

      # 4. PROCESS SENTENCES
      d = d  %>%
      #strip space before final period for alignment with TrialTypes etc below
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", ".")) %>%
      #for some reason fast_vector_sum does not have final periods now.
      mutate(Sentence = ifelse(endsWith(Sentence, "."),Sentence,paste(Sentence, ".", sep=""))) %>%
      #uppercase first word in sentence to align with other model sentence sets
      mutate(Sentence = ifelse(Metric == "fast_vector_sum",firstup(Sentence),Sentence))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

### Print environment variables 
```{r}
print(paste(savedir))
message("Running with the following environment variables:")
message(paste("use_eventsrev: ", use_eventsrev))
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization))

#for plotting the dotted reference line
if (grepl("min-max", normalization) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
```

# Read sentence freq data

```{r}
dirname = '../word_frequency_info'

sentence_file_er = paste(dirname, 'EventsRev_freqs.csv', sep='/')

# read in & normalize predictors
dat_er_sentence = read.csv(sentence_file_er) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))
dat_er_sentence$ItemNum = dat_er_sentence$ItemNum + 900 
dat_er_sentence$TrialType = 'AAN'         
dat_er_sentence$Voice = 'active'

dat_er_sentence$Plausibility = recode(dat_er_sentence$Plausibility, 
                             implausible='Implausible', plausible='Plausible',
                             implaus="Implausible", plaus="Plausible") 

# randomly assign plausibility values to AAR, use even/odd heuristic for the rest
dat_er_sentence$Index = c(1:nrow(dat_er_sentence))
dat_er_sentence = dat_er_sentence %>%
  mutate(Plausibility = ifelse((Index%%2==0), 'Implausible', 'Plausible')) %>%
  select(-Index)
```

# Read human data

```{r}

# READ
dirname = '../human_ratings'
ratings_eventsRev = paste(dirname, '1_EventsRev_plausibility', 'analyses', 'longform_data.csv', sep='/')

dat_er = read.csv(ratings_eventsRev) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence) %>%
  filter(ItemNum<41)    # exclude attention checks

# CLEAN
dat_er$Plausibility = recode(dat_er$Plausibility, implaus='Implausible', plaus='Plausible')
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'
dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap

human_dat = dat_er

human_dat$TrialType = factor(human_dat$TrialType, levels=c('AAN'))
```

Initial mean, but to be filtered later on!

```{r pressure, echo=FALSE}
human_dat.mean = human_dat %>% 
  group_by(ItemNum, Sentence, Plausibility, TrialType, Voice, Experiment) %>%
  summarize(MeanScore = mean(Score, na.rm=TRUE)) %>%
  ungroup() 
human_dat.mean$ItemNum = as.factor(human_dat.mean$ItemNum)

#scale scores

#3.2 Normalize scores for all models
if (grepl("min-max", normalization) == TRUE){
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = min_max(MeanScore))
} else {
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = scale_this(MeanScore))
}
```

# Read model info

```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_eventsRev = paste(dirname, 'EventsRev', sep='/')

message("Loading EventsRev data ...")
files_er = list.files(path=dirname_eventsRev, pattern="*.txt")
dat_er = do.call(rbind, lapply(files_er, function(x) read_data(dirname_eventsRev, x)))
dat_er_sentence$ItemNum = as.factor(dat_er_sentence$ItemNum)

```

## Merge with sentence info

Load & merge with original sentence file for information about TrialType & Voice

NOTE: Some of the sentences were different from the human experiment! >> take from dropbox, NOT behavioral study!
Maybe we should also just exclude these sentences
The file we use is downloaded from Dropbox and named here as 'newsentences_EventsAdapt_smallfixes.csv' to avoid confusion with the 'newsentences_EventsAdapt.csv' file in the behavioral study folder

ANYA: currently excluding all the sentences where the match isn't exact


## EventsRev
```{r read data, echo=FALSE}

dat_er = merge(dat_er, dat_er_sentence, by=c("Sentence"))
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'          
dat_er$Voice = 'active'

dat_er = dat_er %>%
  select(-SentenceNum) %>%
  select(-ItemNum.x) %>%
  select(-Plausibility.x) %>%
  rename(ItemNum=ItemNum.y) %>%
  rename(Plausibility=Plausibility.y)

# check which sentences don't have a plausibility pair
sentence_pairnum = dat_er %>%
  group_by(ItemNum, TrialType, Voice, Metric) %>%
  summarize(NumSentences = length(ItemNum))
single_sentences = sentence_pairnum %>%
  filter(NumSentences==1)

# filter them out
dat_er = dat_er %>%
  filter(!(ItemNum %in% single_sentences$ItemNum))

dat = dat_er
```

## Add human data
```{r}
# only include sentences present in model data
human_dat.mean = human_dat.mean %>%
  select(Sentence, Experiment, MeanScore, NormScore) %>%
  rename(Score=MeanScore)
human_dat.mean = merge(human_dat.mean, dat_er_sentence, by=c("Sentence"))

# Use ItemNum from model data
dat_sentencenum = dat %>%
  group_by(Sentence) %>%
  summarize(ItemNum = ItemNum[1])
human_dat.mean = merge(human_dat.mean %>% select(-ItemNum), dat_sentencenum)

# combine
human_dat.mean$Metric = "human"
dat = rbind(dat, human_dat.mean)
```


# Select which models/metrics to plot

NOTE: the current version will fail to include all ANNs, might need to fix

```{r, echo=FALSE}

human <- c("human")
anns <- c("gpt2","bert","roberta", "tinylstm")

anns_main <- c("gpt2-xl_full","bert-large-cased.sentence-probs-pseudolog","roberta-large.sentence-probs-pseudolog","surprisal_scores_tinylstm")
baselines <- c("syntax-based_PPMI","thematicFit_addition","v2_sdm")
all_main <- c(human, anns_main, baselines)

# select which to use
chosen_models = eval(parse(text=which_models))
model_dat = dat %>% filter(Metric %in% chosen_models)

# simplify ANN names
if (which_models %in% c("all_main", "anns_main")){
  model_dat = model_dat %>% 
    mutate(Metric = str_replace(Metric, ".sentence-probs-pseudolog", ""))
}

message("Using these models/metrics:")
print(unique(model_dat$Metric))
```

# PLAUSIBILITY

We need to define for which models a higher score for plausible sentences is desirable:

1. HIGHER = MORE PLAUSIBLE
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a plausible sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE PLAUSIBLE
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Plausible is more positive!)
# Take out/plot NormScore below if confusing!
model_dat = model_dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore+1, NormScore))
```

# Sentence exclusion
We have to exclude certain items because there's not a full overlap of stimuli between human and model data. We cannot exclude based on item number or synonym pair number because of general misalignment (some numbers are missing)
The below exclusion keywords are obtained from running the check that finds differences in the datasets for models and humans below

```{r}
exclusion_keywords = c('sportsstar', #922 no synonym
                       'applauding' #927 no synonym
                       )
```

## exclude from dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset
message(paste(length(unique(model_dat$Sentence))))
model_dat = model_dat %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(model_dat$Sentence))))

#order by itemNum for correct assignment
model_dat = arrange(model_dat, ItemNum)

models = unique(model_dat$Metric)
df = NA

for(i in seq_along(models)){ 
  print(paste(models[i]))
  curr_dataset = subset(model_dat, Metric == models[i])
  # add info about synonym pairs for dat_ea items
  num_ER = length(subset(curr_dataset, Experiment=='EventsRev')$Sentence) %/% 2
  print(paste(num_ER, ',', num_ER%*%2))
  itemNums_forSynsets <- c(rep(901:(900+num_ER), each=2))
  
  curr_dataset = curr_dataset %>%
    rename(ItemNum_OLD=ItemNum) %>%
    mutate(ItemNum = itemNums_forSynsets)

  if (is.na(df)){
    df = curr_dataset
  } else {
    df = merge(df,curr_dataset, all=TRUE)
  }
}

savename <- "clean_eventsRev_df.csv"
write.csv(x=df, file=paste(savedir,savename,sep="/"))
```

```{r}
#Setting final model names
model_dat = model_dat %>%
  mutate(Metric = str_replace(Metric, "gpt2-xl_full", "GPT2-XL")) %>%
  mutate(Metric = str_replace(Metric, "bert-large-cased", "BERT-large")) %>%
  mutate(Metric = str_replace(Metric, "roberta-large", "RoBERTa-large")) %>%
  mutate(Metric = str_replace(Metric, "surprisal_scores_tinylstm", "tinyLSTM")) %>%
  mutate(Metric = str_replace(Metric, "thematicFit_addition", "ThematicFit")) %>%
  mutate(Metric = str_replace(Metric, "syntax-based_PPMI", "PPMI-syntax")) %>%
  mutate(Metric = str_replace(Metric, "v2_sdm", "SDM"))

ann_group = c("GPT2-XL","BERT-large","RoBERTa-large","tinyLSTM")
baseline_group = c("SDM","ThematicFit","PPMI-syntax")
```

# CONTRASTS

```{r}
#dat = read.csv('dat_all_models_EventsAdaptRev.csv')

dat = model_dat

dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
models_metrics = c("human",ann_group,baseline_group)
nr_models = length(models_metrics)
ncols=nr_models
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}
```

```{r, echo=FALSE, fig.height=12, fig.width=15}
dat = subset(dat, Metric%in%models_metrics)

dat.binchoice = dat %>%
  #add color
  mutate(Category = ifelse(Metric%in%ann_group, "ANNs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category)) %>%
  #create filter criterion
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup()

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=models_metrics)
dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "ANNs", "baselines"))
```

### Stats

```{r}
calculate_binom_pval <- function(numCorrect, numTotal) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ binom.test(numCorrect[i], numTotal[i])$p.value}))
}

# get p values
dat.binchoice.summary = dat.binchoice %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  ungroup()
```

```{r}
# get the X-square and p-value of the X-square 2 sample test. Assume the total num is the same
calculate_chisq <- function(a, b, total) {
  result = prop.test(c(a, total), c(b, total))
  return(c(as.numeric(result$statistic), result$p.value))
}
calculate_chisq_vectorized_chi <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[1]}))
}
calculate_chisq_vectorized_p <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[2]}))
}
# get human responses separately
human.results = dat.binchoice.summary %>%
  filter(Metric=='human') %>%
  select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)
dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = ifelse(pVal2humansAdjusted<0.001, "***", 
                         ifelse(pVal2humansAdjusted<0.01, "**",
                                ifelse(pVal2humansAdjusted<0.05, "*", "")))) 
```

### Plot 

CARINA: There's a slight difference between the length-normalized score GPT2 model and the unnormalized model: For AAN, 5 more sentences are correctly classified by the length-normalized model than the unnormalized one. Why?

"However, PPPLs are only comparable under the same subword vocabulary, which differs between e.g., BERT and RoBERTa. Normalizing with N as the number of *words* mitigates this. " from Salazar et al. 2020

Q: Should we normalize by #words instead?

```{r, echo=FALSE}

plot.binchoice = ggplot(data=dat.binchoice, 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun.y='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x='Model')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
plot.binchoice

savename <- "1_binaryAccuracy_split.png"
ggsave(paste(savedir,savename,sep="/"), width=15, height=12, units='cm')
```

# MODEL HUMAN COMPARISON

```{r}
#subset to only ann data
# Plotting only humans and ANNs for now - will put baselines in the appendix
metrics2plot = c(human, ann_group)

dat = dat %>% 
  filter(Metric %in% metrics2plot)

```

## Calculate human 2 human response correlation 
(each human with the average of all the rest)
```{r}
#omit NAs
human_dat = human_dat %>% drop_na()

if (grepl("min-max", normalization) == TRUE){
  human_dat.comp = human_dat %>%
  mutate(FinalNormScore = min_max(Score)) %>%
  select(WorkerId, ItemNum, TrialType, Plausibility, Voice, Sentence, FinalNormScore)
} else {
  human_dat.comp = human_dat %>%
  mutate(FinalNormScore = scale_this(Score)) %>%
  select(WorkerId, ItemNum, TrialType, Plausibility, Voice, Sentence, FinalNormScore)
}

workerIDs = unique(human_dat.comp$WorkerId)
df_correlation = data.frame()

for (i in seq_along(workerIDs)) {
  human_dat.this = human_dat.comp %>% filter(WorkerId==workerIDs[i])
  human_dat.rest = human_dat.comp %>% filter(WorkerId!=workerIDs[i]) %>%
    filter(ItemNum %in% unique(human_dat.this$ItemNum)) %>%
    group_by(ItemNum, Plausibility, Voice, Sentence) %>% 
    summarize(MeanNormScore=mean(FinalNormScore))
  
  human_dat.side2side = merge(human_dat.this, human_dat.rest,
                              by=c("ItemNum", "Plausibility", "Voice"))
  corval= cor(human_dat.side2side$FinalNormScore, human_dat.side2side$MeanNormScore)
  # add vector to a dataframe
  df <- data.frame(workerIDs[i], corval)
  df_correlation <- rbind(df_correlation,df)
}

human2human_corr = mean(df_correlation$corval)
print(paste("The human2human overall correlation value is", human2human_corr))
```

## Sample a random human response for each item 
And compute the avg of all other responses 

```{r}
set.seed(21)
human_dat.sampled = human_dat.comp %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Sentence) %>%
  summarize(RandomScore = sample(FinalNormScore,1),
            MeanScoreRest = (sum(FinalNormScore)-RandomScore)/(length(FinalNormScore)-1)) %>%
  ungroup()
```

## Prep data

```{r}
dat.human = dat %>% filter(Metric=="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore)
dat.model = dat %>% filter(Metric!="human") %>%
  select(ItemNum, TrialType, Sentence, Plausibility, FinalNormScore, Metric)
dat.model2human = merge(dat.human, dat.model, 
                        by=c("ItemNum", "TrialType", "Sentence", "Plausibility")) %>%
  rename(HumanScore=FinalNormScore.x, ModelScore=FinalNormScore.y)

# add human2human data
human_dat.sampled = human_dat.sampled %>%
  select(-Voice) %>%
  filter(Sentence %in% unique(dat.model2human$Sentence)) %>%  # make sure it's the same sents
  rename(HumanScore=MeanScoreRest, ModelScore=RandomScore)    # renaming for convenience 
human_dat.sampled$Metric = 'singleHuman'
dat.model2human = rbind(dat.model2human, human_dat.sampled)
```

Formatting:

```{r, echo=FALSE, fig.height=12, fig.width=15}
dat.model2human = dat.model2human %>%
  #add color info
  mutate(Category = ifelse(Metric%in%ann_group, "ANNs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="singleHuman", "human", Category))

# specify order for plotting
metric_names = metrics2plot
metric_names[1] = "singleHuman"
dat.model2human$Metric = factor(dat.model2human$Metric, levels=metric_names)
dat.model2human$Category = factor(dat.model2human$Category, levels=c("human", "ANNs", "baselines"))
dat.model2human$TrialType = factor(dat.model2human$TrialType, levels=c("AI", "AAN", "AAR"))
```

## Calculate correlations for the plot

```{r}

df_correlation = data.frame()
trialTypes = c("AAN")

for (t in seq_along(trialTypes)){
  dat.model2human.tt = dat.model2human %>%
    filter(TrialType==trialTypes[t])
  
  # get pairwise correlation values for humans and each model
  val1.human = dat.model2human.tt$ModelScore[dat.model2human.tt$Metric=="singleHuman"]
  val2.human = dat.model2human.tt$HumanScore[dat.model2human.tt$Metric=="singleHuman"]
  
  for(i in seq_along(metric_names)){
      current_metric=metric_names[i]
      val1.model = dat.model2human.tt$ModelScore[dat.model2human.tt$Metric==current_metric]
      val2.model = dat.model2human.tt$HumanScore[dat.model2human.tt$Metric==current_metric]
      pval2zero = cor.test(val1.model, val2.model, method="pearson")$p.value
      corvals = c(
        cor(val1.human, val2.human, method="pearson"),
        cor(val1.model, val2.model, method="pearson"),
        cor(val1.human, val1.model, method="pearson"),
        cor(val1.human, val2.model, method="pearson"),
        cor(val2.human, val1.model, method="pearson"),
        cor(val2.human, val2.model, method="pearson"))
      test2humans = cocor.dep.groups.nonoverlap(corvals[1], corvals[2], corvals[3],
                                         corvals[4], corvals[5], corvals[6], 
                                         n=length(val1.model),
                                         test='raghunathan1996')
      pval2humans= test2humans@raghunathan1996$p.value
      
      # add vector to a dataframe
      df <- data.frame(current_metric, trialTypes[t], corvals[2], pval2zero, pval2humans)
      df_correlation <- rbind(df_correlation,df)
  }
}
colnames(df_correlation) = c("Metric", "TrialType", "Correlation", "pVal2zero", "pVal2humans")

# adjust for multiple comparisons
df_correlation = df_correlation %>%
  mutate(pVal2zeroAdjusted = p.adjust(pVal2zero, method="fdr", n=length(pVal2zero)),
         pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans))) %>%
  mutate(pVal2zeroLabel = ifelse(pVal2zeroAdjusted<0.001, "***", 
                                 ifelse(pVal2zeroAdjusted<0.01, "**",
                                        ifelse(pVal2zeroAdjusted<0.05, "*", ""))),
         pVal2humansLabel = ifelse(pVal2humansAdjusted<0.001, "***", 
                                 ifelse(pVal2humansAdjusted<0.01, "**",
                                        ifelse(pVal2humansAdjusted<0.05, "*", ""))))
```

## Plot

```{r}

dat.model2human$TrialType = factor(dat.model2human$TrialType, levels=c("AI","AAN","AAR"))
dat.model2human$Metric = factor(dat.model2human$Metric, levels=metric_names)
df_correlation$TrialType = factor(df_correlation$TrialType, levels=c("AI","AAN","AAR"))
df_correlation$Metric = factor(df_correlation$Metric, levels=metric_names)

# text size 
cortext_size = 3

plot.model2human = ggplot(dat=dat.model2human)+  
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=ModelScore, y=HumanScore, color=Category), 
             size=0.1, position=position_jitter(width=0.01))+
  geom_text(mapping=aes(x=0.05, y=1.05, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.05), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  theme_classic()+
  theme(panel.spacing.x = unit(0.8, "lines"))+
  theme(legend.position="none")
plot.model2human

savename <- "model2human_byTrialType_ANNs.png"
ggsave(paste(savedir,savename,sep="/"), height=12,width=24, units='cm')
```


# SCATTERPLOTS

## General settings
```{r}
# color scheme
color_plaus = '#1b9e77'
```

## PLAUSIBLE VS IMPLAUSIBLE

### Prep data
```{r, echo=FALSE}
dat.plaus = dat %>% 
  dplyr::select(ItemNum, TrialType, Plausibility, Voice, Metric, NormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(NormScore, na.rm=TRUE)) %>%
  spread(Plausibility, meanScore)  
```

### Calculate correlations


```{r}
print(paste(metrics2plot))
``` 

```{r}
df_correlation.plaus = data.frame()

dat.plaus$Metric = factor(dat.plaus$Metric, levels=metrics2plot)

# get pairwise correlation values for humans and each model
val1.human = dat.plaus$Plausible[dat.plaus$Metric=="human"]
val2.human = dat.plaus$Implausible[dat.plaus$Metric=="human"]

for(i in seq_along(metrics2plot)){
    val1.model = dat.plaus$Plausible[dat.plaus$Metric==metrics2plot[i]]
    val2.model = dat.plaus$Implausible[dat.plaus$Metric==metrics2plot[i]]
    pval2zero = cor.test(val1.model, val2.model, method="pearson")$p.value
    corvals = c(
      cor(val1.human, val2.human, method="pearson"),
      cor(val1.model, val2.model, method="pearson"),
      cor(val1.human, val1.model, method="pearson"),
      cor(val1.human, val2.model, method="pearson"),
      cor(val2.human, val1.model, method="pearson"),
      cor(val2.human, val2.model, method="pearson"))
    test2humans = cocor.dep.groups.nonoverlap(corvals[1], corvals[2], corvals[3],
                                       corvals[4], corvals[5], corvals[6], 
                                       n=length(val1.model),
                                       test='raghunathan1996')
    pval2humans= test2humans@raghunathan1996$p.value
    
    # add vector to a dataframe
    df <- data.frame(metrics2plot[i], corvals[2], pval2zero, pval2humans)
    df_correlation.plaus <- rbind(df_correlation.plaus,df)
}
colnames(df_correlation.plaus) = c("Metric", "Correlation", "pVal2zero", "pVal2humans")

# adjust for multiple comparisons (-1 to exclude humans, *3 for 3 metrics we're comparing)
df_correlation.plaus = df_correlation.plaus %>%
  mutate(pVal2zeroAdjusted = p.adjust(pVal2zero, method="fdr", n=length(pVal2zero)*3),
         pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=(length(pVal2humans)-1)*3)) %>%
  mutate(pVal2zeroLabel = ifelse(pVal2zeroAdjusted<0.001, "***", 
                                 ifelse(pVal2zeroAdjusted<0.01, "**",
                                        ifelse(pVal2zeroAdjusted<0.05, "*", ""))),
         pVal2humansLabel = ifelse(pVal2humansAdjusted<0.001, "***", 
                                 ifelse(pVal2humansAdjusted<0.01, "**",
                                        ifelse(pVal2humansAdjusted<0.05, "*", ""))))
```

### Plot
```{r}
#determine plotting order
df_correlation.plaus$Metric = factor(df_correlation.plaus$Metric, levels=metrics2plot)
dat.plaus$Metric = factor(dat.plaus$Metric, levels=metrics2plot)

plot.plaus = ggplot(data=dat.plaus)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  geom_text(mapping=aes(x=0.05, y=1, label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.plaus, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1), xlim=c(0,1))+
  theme(panel.spacing.x = unit(1, "lines"))+
  theme_classic()
plot.plaus
#paste("r",round(Correlation,2),sep=" = ")
```
```{r, fig.height=2,fig.width=5}

df_correlation$Metric = factor(df_correlation$Metric, levels=models_metrics)
dat$Metric = factor(dat$Metric, levels=models_metrics)
dat$Plausibility = factor(dat$Plausibility, levels = c("Implausible","Plausible"))

plot.plaus.density.sub = ggplot()+
  geom_density(data=dat, mapping=aes(NormScore, fill = Plausibility), alpha = 0.2)+
  facet_wrap(~Metric,nrow=nrows, ncol=ncols)+
  geom_text(mapping=aes(x=0.1, y=10, label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)),  data=df_correlation.plaus, size=cortext_size, hjust = 0)+
  #coord_flip()+
  theme_classic()+
  xlab("Normed score") +
  ylab("Density")+
  theme(legend.title = element_blank())
plot.plaus.density.sub
```

## COMBINE

```{r}
(plot.binchoice | plot_spacer() | 
   (plot.model2human / plot.plaus) ) + 
  plot_layout(widths=c(1,0.05, 3))

savename <- "composite.png"
ggsave(paste(savedir,savename,sep="/"), height=15,width=30, units='cm')
```
```{r}
(plot.binchoice | plot_spacer() | 
   (plot.model2human / plot.plaus.density.sub) ) + 
  plot_layout(widths=c(1.5,0.05, 3))

savename <- "composite_density.png"
ggsave(paste(savedir,savename,sep="/"), height=15,width=35, units='cm')
```

```{r, echo=FALSE}

df_correlation = data.frame()
# get p values for t test
for(i in seq_along(models_metrics)){
  curr_dat = subset(dat.binchoice,Metric==models_metrics[i])
  curr_score_diff = c(curr_dat$FinalScoreDiff)
  print(paste(t.test(curr_score_diff)$p.value))
  ttestPval2zero = t.test(curr_score_diff)$p.value
  print(paste(models_metrics[i], ttestPval2zero))
  curr_cat = unique(curr_dat$Category)
  # add vector to a dataframe
    df <- data.frame(models_metrics[i], ttestPval2zero,curr_cat)
    df_correlation <- rbind(df_correlation,df)
}

# adjust for multiple comparisons within each category
df_correlation = df_correlation %>%
  mutate(pLabel = ifelse(ttestPval2zero<0.001, "***", 
                         ifelse(ttestPval2zero<0.01, "**",
                                ifelse(ttestPval2zero<0.05, "*",
                                       ifelse(ttestPval2zero>=0.05, "n.s.", "")))))

colnames(df_correlation) = c("Metric", "ttestPval2zero", "Category", "pLabel")

df_correlation$Metric = factor(df_correlation$Metric, levels=models_metrics)
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=models_metrics)

plot.score_difference = ggplot(data=dat.binchoice,mapping=aes(x=TrialType, y=FinalScoreDiff, fill=Category))+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  stat_summary(geom='col', fun.y='mean',
               color='black', width=0.5)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=1, y=0.8, label=pLabel), data=df_correlation, size=cortext_size)+
  ggtitle('SCORE DIFFERENCE')+
  theme_classic()+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  xlab("")+
  ylab("Difference (Plausible - Implausible)")
plot.score_difference
savename <- "scoreDifference.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=6, units='cm')
```

#REGRESSION


## Stats

Below cell from DTFit analysis (by Anya)
```{r regression, echo=FALSE}

metrics = unique(dat$Metric)
results = data.frame()

for(i in seq_along(metrics)){
  metric = metrics[i]
  dat.metric = dat %>% filter(Metric==metric)
  m = lmer(FinalNormScore ~ Plausibility + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + (1|ItemNum), 
           data=dat.metric, REML=FALSE, 
           control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))
 d = data.frame(coef(summary(m)))
 d = d %>% rename(pVal=Pr...t..)
 d$Metric = metric 
 d$Parameter = rownames(d)
 d = d[,c(6,7,1:5)]
 results = rbind(results, d)
}

# split up
results.ANN = results %>%
  filter(Metric %in% models_metrics)
results.baselines = results %>%
  filter(!(Metric %in% models_metrics))

# adjust for multiple comparisons
results.ANN = results.ANN %>%
  group_by(Parameter) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal))) %>%
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  mutate(pValFinal = ifelse(pValAdjusted<0.001, "<0.001", as.character(round(pValAdjusted,3)))) %>%
  ungroup()

results.baselines = results.baselines %>%
  group_by(Parameter) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)+1)) %>%   # account for humans too
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  mutate(pValFinal = ifelse(pValAdjusted<0.001, "<0.001", as.character(round(pValAdjusted,3)))) %>%
  ungroup()

savename <- "regression_by_metric_ANN.csv"
write.csv(results.ANN, paste(savedir,savename,sep="/"), row.names=FALSE)
```

```{r regression, echo=FALSE}
m0 = lmer(NormScore ~ Plausibility*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1|ItemNum), 
         data=dat, REML=FALSE)
savename = "regression_m0.txt"
sink(paste(savedir,savename,sep="/"), append=FALSE)
options(max.print=1000000)
summary(m0)
sink()
```

```{r regression, echo=FALSE}
m1 = lmer(NormScore ~ Plausibility*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1+Plausibility|ItemNum), 
         data=dat, REML=FALSE, control=lmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e6)))
savename = "regression_m1.txt"
sink(paste(savedir,savename,sep="/"), append=FALSE)
options(max.print=1000000)
summary(m1)
sink()
```

```{r}
anova(m0,m1)  # result: m1 is better
```

```{r regression, echo=FALSE}
m2 = lmer(NormScore ~ Plausibility*Voice*Metric + agent_freq_norm*Metric + patient_freq_norm*Metric + verb_freq_norm*Metric + sentence_freq_norm*Metric + (1+Plausibility+Metric|ItemNum), 
         data=dat, REML=FALSE)
savename = "regression_m2.txt"
sink(paste(savedir,savename,sep="/"), append=FALSE)
options(max.print=1000000)
summary(m2)
sink()
```

```{r}
anova(m1,m2)
```








