---
title: "results_main"
output: html_document
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(ggh4x)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(gtools)
library(cowplot)
library(png)

source('dataloader_utils.R') #includes normalizations, read_data functions
source('stats_utils.R')

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
```

### Print environment variables 
```{r}
#SET ENVIRONMENT VARIABLES

# can be: "EventsAdapt", "DTFit_EventsRev"
experiment <- "EventsAdapt"

#can be:
# llms_main = human + main LLMs
# all_main = human + main LLMs + baselines
# baselines = human + baselines
# llms_all = human + all LLMs
# llms_bidirectional = human + BERT + RoBERTa, all metrics
which_models <- "all_main" 

#can be "min-max", "zscore", "none"
normalization_type <- "min-max" 

path <- paste("results/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste(path,experiment,"_Models=",which_models,sep='')
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

print(paste(savedir))
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization_type))

#for plotting the dotted reference line
if (grepl("min-max", normalization_type) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
normalization <- get_normalization_fn(normalization_type)
```

# READ DATA
```{r}
#created via preprocess_scores.Rmd
if (grepl("EventsAdapt", experiment)) {
  dat = read.csv('clean_data/clean_EventsAdapt_df.csv')
  human_dat = read.csv('clean_data/clean_EventsAdapt_human_dat.csv')
} else {
  dat = rbind(read.csv('clean_data/clean_EventsRev_df.csv'), read.csv('clean_data/clean_DTFit_df.csv'))
  #human_dat = read.csv('clean_data/clean_EventsRev_human_dat.csv')
  #human_dat$TrialType = "AAN"
}

```

# Select which models/metrics to plot

```{r, utils.choose_models, echo=FALSE}
human <- c("human")
llms_main <- c("RoBERTa-large.sentence-PLL", "BERT-large.sentence-PLL", "GPT-J.sentence-LL", "GPT-2-xl.sentence-LL", "MPT-30b.sentence-LL")
baselines <- c("tinyLSTM.surprisal","SDM", "thematicFit.prod", "syntax-PPMI")

llms_main <- c(human, llms_main)
all_main <- c(llms_main, baselines)
baselines <- c(human, baselines)

all_metrics = unique(dat$Metric) 
llm_metrics = as.character(all_metrics[str_detect(all_metrics, 'BERT|GPT|MPT')])
llms_all <- c(human,llm_metrics)

llm_bert_metrics = as.character(all_metrics[str_detect(all_metrics, 'BERT')])
llms_bidirectional <- c(human,llm_bert_metrics)

# select which to use
chosen_models = eval(parse(text=which_models))
dat = dat %>% filter(Metric %in% chosen_models)

if (grepl("llms_all", which_models) == FALSE && grepl("llms_bidirectional", which_models) == FALSE){
# shorten model names
llms_main_short <- c("MPT", "GPT-J", "GPT-2", "RoBERTa", "BERT")
baselines_short <- c("tinyLSTM","SDM", "thematicFit", "syntax-PPMI")
models_short_order = c(human, llms_main_short, baselines_short)

shorten_metric_names <- function(col) { gsub("-large|-xl|-xxlarge|-30b", "", col) }

dat = dat %>%
  mutate(MetricModel = Metric, Metric=as.character(Metric)) %>%
  separate(Metric, into=c("Metric"), sep="\\.", extra="drop") %>%
  mutate(Metric = shorten_metric_names(Metric)) 
chosen_models = unique(dat$Metric)
chosen_models = chosen_models[order(match(chosen_models,models_short_order))]
dat$Metric = factor(dat$Metric, levels = chosen_models)
}else{
  models_order = c("human", "RoBERTa-large.sentence-PLL", "RoBERTa-large.verb-PLL", "RoBERTa-large.last-word-PLL", "RoBERTa-large.sentence-l2r-PLL",
                   "BERT-large.sentence-PLL", "BERT-large.verb-PLL", "BERT-large.last-word-PLL", "BERT-large.sentence-l2r-PLL")
  chosen_models = chosen_models[order(match(chosen_models,models_order))]
  dat$Metric = relevel(factor(dat$Metric), levels = chosen_models, ref="human")
}
message("Using these models/metrics:")
print(chosen_models)
```

## Avg word frequency

```{r}
dat.sentfreq = dat %>% 
  filter(Voice=='active') %>%
  dplyr::select(TrialType, Sentence, sentence_freq) %>%
  distinct() %>%
  group_by(TrialType) %>%
  summarize(avg_sent_freq = mean(sentence_freq))
```

# CONTRASTS
```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

if (grepl("EventsAdapt", experiment)) {
  dat$TrialType = factor(dat$TrialType, levels=c("AAN", "AAR", "AI"))    # dummy coding by default
  dat$Voice = as.factor(dat$Voice)
  contrasts(dat$Voice) = c(0.5, -0.5)
  colnames(attr(dat$Voice, "contrasts")) = c("A>P")
} else {
  dat$TrialType = factor(dat$TrialType, levels=c("DTFit", "AAN")) 
}
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
nr_models = length(chosen_models)
ncols=nr_models #models plus human
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}

cortext_size = 4
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

#add Category color
if (which_models == "llms_all" || which_models == "llms_bidirectional") {
  dat.binchoice = dat %>%
    mutate(Metric = as.character(Metric)) %>%
    mutate(Category = ifelse(startsWith(Metric, "BERT"), "BERT", "RoBERTa")) %>%
    mutate(Category = ifelse(startsWith(Metric, "GPT"), "GPT", Category)) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category))
} else {
  dat.binchoice = dat %>%
    mutate(Category = ifelse(Metric%in%llms_main_short, "LLMs", "baselines")) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category)) 
}

dat.sentences = dat.binchoice %>%
  filter(Plausibility=="Plausible") %>% 
  dplyr::select(ItemNum, TrialType, Voice, Metric, LowerBetter, Category, Sentence)

dat.binchoice = dat.binchoice %>%
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup() %>%
  inner_join(dat.sentences)

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=chosen_models)
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("DTFit", "AI", "AAN", "AAR"))

if (which_models=="llms_all" || which_models == "llms_bidirectional") {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "RoBERTa", "BERT", "GPT"))
} else {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "LLMs", "baselines"))
}
```


```{r}
# leave only active sentences here (since SDM & PPMI-syntax cannot deal with passive structures, put full plot in SI)
dat.binchoice.active = dat.binchoice %>% filter(Voice=="active")
```

## Stats

```{r}
# get p values
dat.binchoice.summary = dat.binchoice.active %>%
  filter(!(TrialType=="AAR")) %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  ungroup()
```

```{r}
# get human responses separately
human.results = dat.binchoice.summary %>%
  filter(Metric=='human') %>%
  dplyr::select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)

dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = plabel(pVal2humansAdjusted)) 

# print the result
for (i in seq_along(dat.binchoice.summary.withchisq$Metric)) {
  print(paste(dat.binchoice.summary.withchisq$Metric[i], ": ",
        round(dat.binchoice.summary.withchisq$AccuracyScore[i],2), 
        ", χ2=", round(dat.binchoice.summary.withchisq$ChiSq[i],2), 
        ", p=", round(dat.binchoice.summary.withchisq$pVal2humansAdjusted[i],3),
        ";", sep=""))
}
```

## Plot

```{r}
label_names <- c(
  "DTFit" = "animate-inanimate, unlikely",
  "AI" = "animate-inanimate, impossible",
  "AAN" = "animate-animate, unlikely",
  "AAR" = "animate-animate\n(control)"
)
```

```{r, echo=FALSE}

plot.binacc = ggplot(data=subset(dat.binchoice.active, !(TrialType=="AAR")), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 2, labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))
plot.binacc

savename <- "1_binaryAccuracy_split.png"
plt_width = ifelse(which_models%in%c("llms_all","llms_bidirectional"), 20, 15)
plt_height = ifelse(which_models%in%c("llms_all","llms_bidirectional"), 12, 9)
ggsave(paste(savedir,savename,sep="/"), width=plt_width, height=plt_height, units='cm')
```
### Add logos
```{r}
# source: https://www.markhw.com/blog/logos

get_png <- function(filename) {
  grid::rasterGrob(png::readPNG(filename), interpolate = TRUE)
}

img_ai <- get_png("./animate-inanimate.png")
img_aa <- get_png("./animate-animate.png")

(p3 <- ggplot(mapping = aes(x = 0:1, y = 1)) +
  theme_void() +
  annotation_custom(img_ai, xmin = .14, xmax = .34) +
  annotation_custom(img_aa, xmin = .57, xmax = .77))

plot.binacc.logos <- gridExtra::grid.arrange(p3, plot.binacc, heights = c(.09, .91))

savename <- "1_binaryAccuracy_split_logos.png"
ggsave(paste(savedir,savename,sep="/"), plot.binacc.logos, width=20, height=8, units='cm')
```

## Difference between AI & AAN conditions

### Stats

Hypothesis test of proportions (Two proportions z test):
prop.test(c(nr_successes1, nr_successes2), c(nr_total1, nr_total2), p = NULL, alternative = "two.sided", correct = TRUE)
          
The function returns:
- the value of Pearson’s chi-squared test statistic.
- a p-value
- a 95% confidence intervals
- an estimated probability of success (the proportion of smokers in the two groups)

```{r, echo=FALSE}
dat.aivsaan = dat.binchoice.summary %>% 
  dplyr::select(TrialType, Metric, NumCorrect, NumTotal)

df_ai.vs.aan = data.frame()

message(paste("Hypothesis test for proportions | AI vs. AAN accuracy"))
for (i in seq_along(chosen_models)) {
  curr_data = subset(dat.aivsaan, Metric==chosen_models[i])
  cnt_successes = curr_data$NumCorrect
  cnt_total = curr_data$NumTotal
  
  proptest <- prop.test(x = cnt_successes, n = cnt_total)
  
  proptest.stat = proptest$statistic
  proptest.pval = proptest$p.value
#  print(paste(chosen_models[i], "|", proptest.stat, "|", proptest.pval, "|", plabel(proptest.pval)))
  
  # get score difference
  score_diff = curr_data %>% 
    mutate(Accuracy=NumCorrect/NumTotal) %>%
    summarize(Diff = Accuracy[TrialType=="AI"]-Accuracy[TrialType=="AAN"]) 
  
  df <- data.frame(chosen_models[i], score_diff[1,1], proptest.stat, proptest.pval)
  df_ai.vs.aan <- rbind(df_ai.vs.aan,df)
}

colnames(df_ai.vs.aan) = c("Metric", "Difference", "ChiSquared", "pVal")
```

### Correct for MC within LLMs and baselines

```{r adjust pVals, echo=FALSE}
results = df_ai.vs.aan %>%
  mutate(Category = ifelse(Metric%in%llms_main_short, "LLMs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category))

results = results %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal))) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  mutate(Difference = round(Difference, 2),
    ChiSquared = round(ChiSquared, 2),
    pValAdjusted = ifelse(pValAdjusted<0.001, "<0.001 ***", 
                          paste(as.character(round(pValAdjusted,3)), pLabel))) %>%
  ungroup() %>%
  dplyr::select(Category, Metric, Difference, ChiSquared, pValAdjusted)

savename <- "AI_AA_difference_stats.csv"
write.csv(results, paste(savedir,savename,sep="/"), row.names=FALSE)
```

## Mean model results
```{r}
dat.binchoice.meanbycategory = dat.binchoice.summary %>%
  group_by(Category, TrialType) %>%
  summarize(meanAccuracy = mean(AccuracyScore))
```


# DETAILED PLOT

## General settings
```{r}
# color scheme
color_plaus = '#1b9e77'
color_voice = '#d95f02'
color_syn = '#7570b3'
cortext_size = 3
```


## HISTOGRAMS 

```{r, fig.height=2,fig.width=5}
if (grepl("all_main", which_models) == TRUE){
plotting_models = c('human',llms_main_short)
}else{
  plotting_models = chosen_models
}

dat.sub = dat %>%
  filter(Voice=="active") %>%
  filter(!TrialType=="AAR") %>%
  filter(Metric%in%plotting_models)

dat.sub$Metric = factor(dat.sub$Metric, levels=chosen_models)
dat.sub$TrialType = factor(dat.sub$TrialType, levels=c("DTFit", "AI", "AAN", "AAR"))

plot.plaus.density.sub = ggplot()+
  geom_density(data=dat.sub, mapping=aes(FinalNormScore, fill = factor(Plausibility, levels=c("Implausible","Plausible"))), alpha = 0.2)+
  facet_grid(TrialType~Metric, labeller = labeller(TrialType = as_labeller(label_names)))+
  geom_vline(xintercept=reference_value, linetype='dotted')+
  theme_classic()+
  scale_x_continuous(breaks=c(0,0.5,1))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  xlab("Normed score") +
  ylab(" ")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title=element_blank())
plot.plaus.density.sub
```

### Mean diff scores by category
```{r mean diff, echo=FALSE}
dat.meanDiff = dat.binchoice.active %>% 
  filter(!(TrialType=="AAR")) %>%
  group_by(Category, TrialType, Metric) %>% 
  summarize(meanDiff = mean(FinalScoreDiff),
            meanDiffSD = sd(FinalScoreDiff)) %>%
  ungroup() %>%
  group_by(Category, TrialType) %>%
  summarize(meanDiffSD = ifelse(!(Category == "human"), round(sd(meanDiff), 2), meanDiffSD),
            meanDiff = round(mean(meanDiff), 2)) %>%
  ungroup() %>%
  distinct(Category, TrialType, .keep_all = TRUE) %>%
  select(Category, TrialType,  meanDiff, meanDiffSD)
```

### Plot
```{r singular binacc plots}

if(experiment=="EventsAdapt") {
  AI_TrialType = "AI"
} else {
  AI_TrialType = "DTFit"
}

dat.sub.AI = dat.sub %>%
  filter(TrialType%in%c(AI_TrialType))
dat.sub.AI$Plausibility = recode(dat.sub.AI$Plausibility, 
                                 "Implausible"="implausible", "Plausible"="plausible")

plot.plaus.density.AI = ggplot()+
  geom_density(data=dat.sub.AI, mapping=aes(FinalNormScore, fill = factor(Plausibility, levels=c("implausible","plausible"))), alpha = 0.2)+
  facet_nested(~TrialType + Metric, labeller = labeller(TrialType = as_labeller(label_names)))+
  geom_vline(xintercept=reference_value, linetype='dotted')+
  theme_classic()+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c('0','0.5','1'))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  xlab("Normalized score") +
  ylab("Density")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title=element_blank(),
        legend.position="none",
        axis.title = element_text(size=9))
plot.plaus.density.AI

dat.sub.AAN = dat.sub %>%
  filter(TrialType%in%c("AAN"))
dat.sub.AAN$Plausibility = recode(dat.sub.AAN$Plausibility, 
                                 "Implausible"="implausible", "Plausible"="plausible")

plot.plaus.density.AAN = ggplot()+
  geom_density(data=dat.sub.AAN, mapping=aes(FinalNormScore, fill = factor(Plausibility, levels=c("implausible","plausible"))), alpha = 0.2)+
  facet_nested(~TrialType + Metric, labeller = labeller(TrialType = as_labeller(label_names)))+
  geom_vline(xintercept=reference_value, linetype='dotted')+
  theme_classic()+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c('0','0.5','1'))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  xlab("Normalized score") +
  ylab("")+
  labs(fill="Distribution")+
  theme(plot.title = element_text(hjust=0.5,face='bold'))+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_text(size=9))
plot.plaus.density.AAN

legend.plaus <- cowplot::get_legend(plot.plaus.density.AAN)
```

## PLAUSIBILITY

### Prep data

```{r, echo=FALSE}
if (grepl("all_main", which_models) == TRUE){
plotting_models = c('human',llms_main_short)
}else{
  plotting_models = chosen_models
}

dat.plaus = dat.sub %>%
  filter(Metric %in% plotting_models) %>%
  dplyr::select(ItemNum, TrialType, Plausibility, Voice, Metric, FinalNormScore) %>%
  group_by(ItemNum, TrialType, Plausibility, Voice, Metric) %>%
  summarize(meanScore = mean(FinalNormScore, na.rm=TRUE)) %>%
  spread(Plausibility, meanScore)
dat.plaus$Metric = factor(dat.plaus$Metric, levels=plotting_models)
```

### Calculate correlations

```{r}
df_correlation.plaus.AI = get_correlation_df("plausibility", "human", subset(dat.plaus, TrialType==AI_TrialType), plotting_models)
df_correlation.plaus.AAN = get_correlation_df("plausibility", "human", subset(dat.plaus, TrialType=="AAN"), plotting_models)
```

### Plot 

```{r}
df_correlation.plaus.AI$Metric = factor(df_correlation.plaus.AI$Metric, levels=plotting_models)
df_correlation.plaus.AAN$Metric = factor(df_correlation.plaus.AAN$Metric, levels=plotting_models)
dat.plaus$Metric = factor(dat.plaus$Metric, levels=plotting_models)

plot.plaus.AI = ggplot(data=subset(dat.plaus, TrialType==AI_TrialType))+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  geom_text(mapping=aes(x=0, y=1.10, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.plaus.AI, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c('0','0.5','1'))+
  scale_y_continuous(breaks=c(0,0.5,1), labels=c('0','0.5','1'))+
  theme_classic()+
  facet_grid(~Metric)+
  xlab("Plausible score")+
  ylab("Implausible score")+
  theme(plot.title = element_text(hjust=0.5,face='bold'),
        axis.title = element_text(size=9))
plot.plaus.AI

plot.plaus.AAN = ggplot(data=subset(dat.plaus, TrialType=="AAN"))+
  facet_wrap(~Metric, ncol=ncols, nrow=nrows)+
  geom_abline(slope=1, intercept=0, size=0.2)+
  geom_point(mapping=aes(x=Plausible, y=Implausible), size=0.2, color=color_plaus)+
  geom_text(mapping=aes(x=0, y=1.10, 
                        label=sprintf("r = %.2f%s", round(Correlation,2), pVal2zeroLabel)), 
            data=df_correlation.plaus.AAN, size=cortext_size, hjust = 0)+
  coord_cartesian(ylim=c(0,1.15), xlim=c(0,1))+
  scale_x_continuous(breaks=c(0,0.5,1), labels=c('0','0.5','1'))+
  scale_y_continuous(breaks=c(2))+
  theme_classic()+
  facet_grid(~Metric)+
  xlab("Plausible score")+
  ylab(" ")+
  theme(plot.title = element_text(hjust=0.5,face='bold'),
        axis.title = element_text(size=9))
plot.plaus.AAN
```

## SCORE DISTRIBUTION PLOTS

```{r, echo=FALSE}
(p3 <- ggplot(mapping = aes(x = 0:1, y = 1)) +
  theme_void() +
  annotation_custom(img_ai, xmin = .18, xmax = .28) +
  annotation_custom(img_aa, xmin = .73, xmax = .83))

main_plot <- p3 / (plot.plaus.density.AI + plot.plaus.density.AAN ) /
  (plot.plaus.AI + plot.plaus.AAN) /
  plot_layout(heights=c(.2, .5, .5)) + 
  plot_annotation(tag_levels = list(c('', 'A', '', 'B', ''))) & 
  theme(plot.tag.position = c(0, 0.98), plot.tag = element_text(face='bold', size=18))
main_plot 

savename <- "score_distributions.png"
ggsave(paste(savedir,savename,sep="/"), height=11, width=28, units='cm')
```

## OLD MAIN FIGURE

```{r, echo=FALSE}
(p3 <- ggplot(mapping = aes(x = 0:1, y = 1)) +
  theme_void() +
  annotation_custom(img_ai, xmin = .18, xmax = .28) +
  annotation_custom(img_aa, xmin = .73, xmax = .83))

main_plot <- p3 / plot.binacc / (plot.plaus.density.AI + plot.plaus.density.AAN ) /
  (plot.plaus.AI + plot.plaus.AAN) /
  plot_layout(heights=c(.2, 1.2, .5, .5)) + 
  plot_annotation(tag_levels = list(c('', 'A', 'B', '', 'C', ''))) & 
  theme(plot.tag.position = c(0, 0.98), plot.tag = element_text(face='bold', size=18))
main_plot

savename <- "alternative_main.png"
ggsave(paste(savedir,savename,sep="/"), height=18, width=25, units='cm')
```

#REGRESSION 

EventsAdapt:
Full model, all trial types

DTFit & EventsRev:
Run stats separately for the two sentence sets
No active/passive, only one trial type -> simple model

```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) 
dat$ItemNum = as.factor(dat$ItemNum)

if (experiment=="EventsAdapt") {
  dat$TrialType = factor(dat$TrialType, levels=c("AAN", "AAR", "AI"))    
  dat$Voice = as.factor(dat$Voice)
  contrasts(dat$Voice) = c(0.5, -0.5)
  colnames(attr(dat$Voice, "contrasts")) = c("A>P")
  datasets = c("EventsAdapt")
  f = FinalNormScore ~ Plausibility*TrialType*Voice + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + cNumTokens + (1+Plausibility|ItemNum)
} else {
  datasets = c("DTFit", "EventsRev")
  f = FinalNormScore ~ Plausibility + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + cNumTokens + (1|ItemNum)
}
```

## Determine whether to include Plausibility by ItemNum random slope (EventsAdapt)
```{r regression, echo=FALSE}
# metrics = c('human',llms_main_short)
# pvals_likelihoodratiotest = c()
# 
# for(i in seq_along(metrics)){
#   metric = metrics[i]
#   dat.metric = dat %>% filter(Metric==metric)
    # if (!(metric %in% llms_main_short)) {
    #   dat.metric = dat.metric %>% mutate(NumTokens=NumWords)
    # }
    # dat.metric = dat.metric %>% mutate(cNumTokens=scale(NumTokens, scale=FALSE))
#   m1 = lmer(FinalNormScore ~ Plausibility*TrialType*Voice + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + NumTokens + (1+Plausibility|ItemNum), 
#            data=dat.metric, REML=FALSE, 
#            control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))
#   m0 = lmer(FinalNormScore ~ Plausibility*TrialType*Voice + agent_freq_norm + patient_freq_norm + verb_freq_norm + sentence_freq_norm + NumTokens +(1|ItemNum), 
#            data=dat.metric, REML=FALSE, 
#            control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))
#   pvals_likelihoodratiotest = c(pvals_likelihoodratiotest, anova(m1, m0)[2,8])
# }
# print(pvals_likelihoodratiotest)
# print(lapply(pvals_likelihoodratiotest, function(x) {x<0.05/length(pvals_likelihoodratiotest)}))  
```

Conclusion: the model with the Plausibility x ItemNum slope is a better fit for all metrics, so will stick to it.


## Stats main

```{r regression, echo=FALSE}
if (which_models=="all_main") {
  metrics = c('human',llms_main_short)
} else {
  metrics = chosen_models
}

results = data.frame()

for(i in seq_along(metrics)){
  for (j in seq_along(datasets)) {
    metric = metrics[i]
    dataset = datasets[j]
    dat.metric = dat %>% filter(Metric==metric, Experiment==dataset)
    if (!(metric %in% llms_main_short)) {
      dat.metric = dat.metric %>% mutate(NumTokens=NumWords)
    }
    dat.metric = dat.metric %>% mutate(cNumTokens=scale(NumTokens, scale=FALSE))
    m = lmer(f, 
             data=dat.metric, REML=FALSE, 
             control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))
   d = data.frame(coef(summary(m)))
   d = d %>% rename(pVal=Pr...t..)
   d$Metric = metric 
   d$Experiment = dataset
   d$Parameter = rownames(d)
   d = d[,c(6,7,8,1:5)]
   results = rbind(results, d)
  }
}
```


## Correct for MC within LLMs and baselines

```{r adjust pVals, echo=FALSE}
results = results %>%
  mutate(Category = ifelse(Metric%in%llm_metrics, "LLMs", "baselines")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category))

results = results %>%
  group_by(Category, Parameter) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal))) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  mutate(pValFinal = ifelse(pValAdjusted<0.001, "<0.001", as.character(round(pValAdjusted,3)))) %>%
  ungroup()

savename <- "regression_by_metric.csv"
write.csv(results, paste(savedir,savename,sep="/"), row.names=FALSE)
```

## Format

```{r format results, echo=FALSE}
if (experiment=="EventsAdapt") {
  reported_params = c("PlausibilityImplausible:TrialTypeAAR",
                      "PlausibilityImplausible:TrialTypeAI",
                      "VoiceA>P",
                      "agent_freq_norm",
                      "patient_freq_norm",
                      "verb_freq_norm",
                      "sentence_freq_norm",
                      "cNumTokens",
                      "TrialTypeAAR:VoiceA>P",
                      "TrialTypeAI:VoiceA>P",
                      "PlausibilityImplausible:TrialTypeAAR:VoiceA>P",
                      "PlausibilityImplausible:TrialTypeAI:VoiceA>P")
} else {
  reported_params = c("PlausibilityImplausible",
                    "agent_freq_norm",
                    "patient_freq_norm",
                    "verb_freq_norm",
                    "sentence_freq_norm",
                    "cNumTokens")
}

# get mean model effect size
results.mean = results %>%
  filter(Parameter %in% reported_params) %>%
  filter(Metric!="human") %>%
  group_by(Experiment, Parameter) %>%
  summarize(MeanByModel = round(mean(Estimate), 2))

# restructure 
results.clean = results %>% 
  filter(Parameter %in% reported_params) %>%
  mutate(Value = ifelse(pValAdjusted<0.05, 
                        paste(round(Estimate, 2), pLabel, sep=" "),
                        "")) %>%
  dplyr::select(Experiment, Metric, Parameter, Value) %>% 
  spread(Metric, Value)
results.clean = merge(results.clean, results.mean)

# order and names
results.clean = results.clean %>%
  mutate(Category = ifelse(Parameter %in% c("PlausibilityImplausible:TrialTypeAAR",
                                            "PlausibilityImplausible:TrialTypeAI",
                                            "PlausibilityImplausible"),
                           "Core effects", "Surface-level effects")) %>%
  dplyr::select(c(Experiment, Category, Parameter, metrics, 'MeanByModel')) %>%
  mutate(Parameter = factor(Parameter, levels=reported_params)) %>%
  arrange(Experiment, Parameter) %>%
  mutate(Parameter = recode(Parameter, 
                    "PlausibilityImplausible:TrialTypeAAR"="Plausible AA > Implausible AA",
                    "PlausibilityImplausible:TrialTypeAI"="Implausible AI > Implausible AA",
                    "PlausibilityImplausible"="Plausible > Implausible",
                    "VoiceA>P"="Voice",
                    "agent_freq_norm"="Agent frequency",
                    "patient_freq_norm"="Patient frequency",
                    "verb_freq_norm"="Verb frequency",
                    "sentence_freq_norm"="Avg. word frequency",
                    "cNumTokens"="Sentence length",
                    "TrialTypeAAR:VoiceA>P"="Voice x Sentence (AAN>control)",
                    "TrialTypeAI:VoiceA>P"="Voice x Sentence (AI>AAN)",
                    "PlausibilityImplausible:TrialTypeAAR:VoiceA>P"="Plausibility x Voice x Sentence (AAN>control)",
                    "PlausibilityImplausible:TrialTypeAI:VoiceA>P"="Plausibility x Voice x Sentence (AI>AAN)"))

savename <- "regression_by_metric_formatted.csv"
write.csv(results.clean, paste(savedir,savename,sep="/"), row.names=FALSE)  
```


# ERROR ANALYSIS

## Compare performance across models

```{r}
acc_by_sentence.model = dat.binchoice.active %>% 
  filter(!(TrialType=="AAR")) 

if (which_models!="baselines") {
  acc_by_sentence.model = acc_by_sentence.model %>% filter(Category=="LLMs")
} else{
  acc_by_sentence.model = acc_by_sentence.model %>% filter(Category=="baselines")
}

acc_by_sentence.model = acc_by_sentence.model %>%
  group_by(TrialType,ItemNum, Voice) %>% 
  mutate(meanAccuracy=mean(Accuracy)) %>%
  ungroup()
```

## Compare with human scores

```{r}
dat.binchoice.human = dat.binchoice %>% filter(Metric=='human')
acc_by_sentence = inner_join(dat.binchoice.human, acc_by_sentence.model %>% dplyr::select(ItemNum, TrialType, Voice, meanAccuracy)) %>%
  distinct()

# calculate correlation
cor_df = acc_by_sentence %>% 
  group_by(TrialType) %>%
  summarize(Correlation = cor(meanAccuracy, FinalScoreDiff),
            pVal = cor.test(meanAccuracy, FinalScoreDiff, method="pearson")$p.value) %>%
  mutate(pLabel= plabel(pVal))

# plot
ggplot(acc_by_sentence, aes(x=meanAccuracy, y=FinalScoreDiff)) + 
  facet_wrap(~TrialType, ncol = 2, labeller = as_labeller(label_names))+
  stat_summary(geom='col', fun='mean', fill='gray80')+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  geom_jitter(width=0.02, size=0.5, color='gray50')+
  stat_summary(geom='errorbar', fun.data='mean_se',
             color = 'gray10', size = 0.5, width=0.04)+
  geom_text(mapping=aes(x=0, y=1.10, 
                      label=sprintf("r = %.2f%s", round(Correlation,2), pLabel)), 
          data=cor_df, size=cortext_size, hjust = 0)+
  scale_x_continuous(breaks=c(0,.2,.4,.6,.8, 1), labels=c(0,1,2,3,4,5))+
  scale_y_continuous(breaks=c(0,0.5,1))+
  labs(x="# of LLMs that evaluated sentence pair correctly", y="Human score difference")+
  theme_classic() + theme(strip.text = element_text(size=11))

savename <- "models correct vs human score diff.png"
ggsave(paste(savedir,savename,sep="/"), width=15, height=8, units='cm')
```

## Save hard sentences

```{r}
threshold = 0.5

hard_sents = acc_by_sentence %>%
  filter(meanAccuracy<threshold) %>%
  dplyr::select(TrialType, ItemNum, Voice, meanAccuracy, FinalScoreDiff)

hard_sents = inner_join(hard_sents, dat) %>%
  dplyr::select(ItemNum, Sentence, TrialType, Plausibility, meanAccuracy, FinalScoreDiff) %>% 
  distinct() %>%
  spread(Plausibility, Sentence) %>%
  dplyr::select(-ItemNum) %>% 
  arrange(meanAccuracy, desc(TrialType)) %>%
  rename(number_ANNs_correct = meanAccuracy, human_score_difference = FinalScoreDiff) %>%
  mutate(human_score_difference = round(human_score_difference, 2))
hard_sents$Index = 1:nrow(hard_sents)
hard_sents = dplyr::select(hard_sents, Index, everything())

savename <- "hard_sentences_models.csv"
write.csv(hard_sents, paste(savedir,savename,sep="/"), row.names=FALSE)
```


## Human errors 
```{r}

dat.human.error.prep = subset(dat, Metric=='human') %>%
  filter(Voice=='active') %>%
  filter(!(TrialType=='AAR')) %>%
  group_by(TrialType, ItemNum, Voice, LowerBetter) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, ifelse(FinalScoreDiff<0, 0, NA))) %>%
  ungroup()

human.error.itemnum <- subset(dat.human.error.prep, Accuracy==0) %>%
  dplyr::select(ItemNum)

dat.human.error.sent = subset(dat, Metric=='human') %>% 
  filter(Voice=='active') %>%
  inner_join(human.error.itemnum, by = c("ItemNum")) %>%
  inner_join(dat.human.error.prep, by = c("ItemNum")) %>%
  mutate(Diff = round(FinalScoreDiff, 2)) %>%
  dplyr::select(ItemNum,Sentence,Plausibility,Diff) %>%
  distinct() %>%
  spread(Plausibility, Sentence)

savename <- "human_wrong_binacc_sentences_AAN.csv"
write.csv(dat.human.error.sent, paste(savedir,savename,sep="/"), row.names=FALSE)

```

# ```{r}
# Compute average score difference between plausible and implausible sentences for AI vs. AA sentences in humans (& LLMs)
# dat.avg.scorediff = dat.binchoice.active %>% 
#   filter(!(TrialType=="AAR")) %>%
#   group_by(Category, TrialType, Metric) %>% 
#   summarize(meanDiffSD = round(sd(FinalScoreDiff), 2),
#             meanDiff = round(mean(FinalScoreDiff), 2)) %>%
#   ungroup()
# ```