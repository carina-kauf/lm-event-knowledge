---
title: "results_main"
output: html_document
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(ggExtra)
library(gtools)

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)


#SET ENVIRONMENT VARIABLES
#use BOTH EventsAdapt AND EventsRev OR only EventsAdapt
use_eventsrev <- FALSE #set to FALSE if you don't want to also use EventsRev data for analyses
which_models <- "mlm_all"
normalization <- "min-max" #can be "min-max", "zscore"
normalized_GPT <- FALSE #can be FALSE or TRUE

path <- paste("results_Jan2022/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste("results_Jan2022/EventsRev=",use_eventsrev,"_Models=",which_models,"_Norm=",normalization)
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)


remove_axis <- theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())

#helper function #https://stackoverflow.com/questions/35775696/trying-to-use-dplyr-to-group-by-and-apply-scale
scale_this <- function(x) as.vector(scale(x))

min_max <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#uppercase first letter of string (needed for fast_vector_sum)
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

#function for sharing legends #https://github.com/tidyverse/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)

  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))

  grid.newpage()
  grid.draw(combined)

  # return gtable invisibly
  invisible(combined)

}

# custom function to read a datatable
read_data <- function(directory, filename) {
  d = read.delim(paste(directory, filename, sep='/'), 
                 header=FALSE, sep='\t')
  
  #DATA EXCLUSION
  ## Exclude ngram model for having too many UNK tokens in EventsAdapt sentence set
  if (grepl("ngram", filename, fixed = TRUE) == TRUE) {
    message(">> Ngram model has too many UNK tokens for EventsAdapt sentence set! Excluding the model for now!")
    return(NULL)
  }
  ## Exclude GRNN & gpt2 surprisal model for only having been run on EventsRev dataset
  if ((grepl("surprisal_scores_GRNN", filename, fixed = TRUE)  == TRUE) |
    (grepl("surprisal_scores_gpt2", filename, fixed = TRUE)  == TRUE)) {
    message(paste(">> Model", filename, "not run for EventsAdapt! Excluding the model!"))
    return(NULL)
  }
  ## Exclude fast vector sum model for mismatch of sentences >> to be fixed! (exlusion list excludes more than in other sentences)
  if (grepl("fast_vector_sum", filename, fixed = TRUE)  == TRUE) {
    message(paste(">> Model", filename, "is mismatched in sentences >> excluding it!!"))
    return(NULL)
  }
  
  #ROWS
  #set target number of *rows* in file
  if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
    target_rownumber = 1648
  } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
    target_rownumber = 80
  } else {
    print(paste("unknown experiment for file: ", filename))
    target_rownumber = 0
  }
  #check for target number of rows
  if (nrow(d)!=target_rownumber) {
    print(paste('unexpected number of sentences in file: ', filename, 'number of sentences: ', nrow(d)))
    return(NULL)
  }
  
  #COLUMNS
  #check for target number of *columns* in file
  if (ncol(d)==3 | ncol(d)==4 | ncol(d)==6) {
      
      # 0. PREPARATION (set #of trials per item)
      if (grepl("Adapt", filename, fixed = TRUE) == TRUE) { #if EventsAdapt
        target_trialnr = 4
      } else if (grepl("ev1", filename, fixed = TRUE) == TRUE){ #if EventsRev
        target_trialnr = 2
      } else {
        print(paste("unknown experiment for file: ", filename))
        target_trialnr = 0
      }
    
    #streamline input format
    if (ncol(d)==3){
      d = d  %>%
      rename(SentenceNum=V1, Sentence=V2, Score=V3) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    else if (ncol(d)==4){
      d = d  %>%
      select(V1,V2,V4) %>% #select relevant columns (do not choose Plausibility column to streamline how it's assigned
                           #(e.g. all lowercase etc))
      rename(SentenceNum=V1, Sentence=V2, Score=V4) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == FALSE)) { #If surprisal_scores_tinylstm
      #Add sentence number from 0 to len(dataset) -1
      tgt_len = nrow(d)-1
      sentnums = c(0:tgt_len)
      d = d  %>%
      select(V1,V5) %>% #select relevant columns (others include UNKification for EventsAdapt dataset)
      rename(Sentence=V1, Score=V5) %>%
      mutate(SentenceNum = sentnums) %>% #This adds a column SentenceNum from 0 to len(dataframe)-1
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    else if ((ncol(d)==6) & (grepl("smooth", filename) == TRUE)) {
      d = d  %>%
      select(V1,V3,V6) %>% #select relevant columns (others include grammatical tags)
      rename(SentenceNum=V1, Sentence=V3, Score=V6) %>%
      mutate(ItemNum = SentenceNum %/% target_trialnr)
    }
    
    d = d %>%
      mutate(Plausibility = ifelse(SentenceNum%%2==0, 'Plausible', 'Implausible')) %>%
      
      # 2. NAME MODEL_METRIC
      #add metric column
      mutate(Metric = substr(filename,1,nchar(filename)-4)) %>%
      
      #strip EventsAdapt dataset prefix
      mutate(Metric = str_replace(Metric, "new[_-][Ee]ventsAdapt[_\\.]", "")) %>%
      mutate(Metric = str_replace(Metric, "newsentences[_-]EventsAdapt[_\\.]", "")) %>%
      #strip EventsRev dataset prefix
      mutate(Metric = str_replace(Metric, "ev1[_\\.]", "")) %>%
      
      ##Assimilate ppmi model names between datasets
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline1", "syntax-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline1_ppmi", "syntax-based_PPMI")) %>% #EventsAdapt
      mutate(Metric = str_replace(Metric, "deps.smooth.baseline2", "order-based_PPMI")) %>% #EventsRev
      mutate(Metric = str_replace(Metric, "scores_baseline2_ppmi", "order-based_PPMI")) %>% #EventsAdapt
      ##Assimilate thematic fit model names between datasets
      mutate(Metric = str_replace(Metric, "update-model.TF-add", "thematicFit_addition")) %>%
      mutate(Metric = str_replace(Metric, "update-model.TF-prod", "thematicFit_product")) %>%
      ##Assimilate gpt2 model names between datasets
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2-xl", "gpt2-xl_full")) %>%
      mutate(Metric = str_replace(Metric, "deps.prob_sent_gpt2", "gpt2_full")) %>%
      ##Simplify ANN names
      mutate(Metric = str_replace(Metric, ".sentence-probs-pseudolog", ".PLL"))
      
      # 3. PREPROCESS SCORES
    
      #3.1 Log-Scale GPT2 probabilities
      #the GPT2 models are the only ones with probability scores and not log probability! > log-scale
      #Note: One entire quadruplet (SentenceNum 1516, The editor overheard the proofreader .) has score 0.0
      # >> add 1e-30 to enable log scaling & ask Emmanuele why this is 0!
    if ((grepl("gpt2", filename) == TRUE) & (grepl("surprisal", filename) == FALSE)) {
      print(paste("log scaling scores in file: ", filename))
      d = d %>%
      mutate(Score = log(Score))
    }
      #3.2 Normalize scores for all models
    if (grepl("min-max", normalization) == TRUE){
      d = d %>%
      mutate(NormScore = min_max(Score))
    } else {
      d = d %>%
      mutate(NormScore = scale_this(Score))
    }

      # 4. PROCESS SENTENCES
      d = d  %>%
      #strip space before final period for alignment with TrialTypes etc below
      mutate(Sentence = str_replace(Sentence, " [.]", ".")) %>%
      #for some reason, some baseline files have multiple periods at end > strip to just one!
      mutate(Sentence = str_replace(Sentence, "[.]{2,}", ".")) %>%
      #for some reason fast_vector_sum does not have final periods now.
      mutate(Sentence = ifelse(endsWith(Sentence, "."),Sentence,paste(Sentence, ".", sep=""))) %>%
      #uppercase first word in sentence to align with other model sentence sets
      mutate(Sentence = ifelse(Metric == "fast_vector_sum",firstup(Sentence),Sentence))
    
    return(d)
  } 
  else {
    print(paste('unexpected number of columns in file: ', 'number of columns: ', filename, ncol(d)))
    return(NULL)
  }
}
```

### Print environment variables 
```{r}
print(paste(savedir))
message("Running with the following environment variables:")
message(paste("use_eventsrev: ", use_eventsrev))
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization))

#for plotting the dotted reference line
if (grepl("min-max", normalization) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
```

# Read sentence freq data

```{r}
dirname = '../word_frequency_info'

sentence_file_er = paste(dirname, 'EventsRev_freqs.csv', sep='/')
sentence_file_ea = paste(dirname, 'EventsAdapt_freqs.csv', sep='/')

# read in & normalize predictors
dat_er_sentence = read.csv(sentence_file_er) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))
dat_er_sentence$ItemNum = dat_er_sentence$ItemNum + 900 
dat_er_sentence$TrialType = 'AAN'         
dat_er_sentence$Voice = 'active'
dat_ea_sentence = read.csv(sentence_file_ea) %>%
  mutate(agent_freq_norm = (agent_freq - mean(agent_freq))/sd(agent_freq),
         patient_freq_norm = (patient_freq - mean(patient_freq))/sd(patient_freq),
         verb_freq_norm = (verb_freq - mean(verb_freq))/sd(verb_freq),
         sentence_freq_norm = (sentence_freq - mean(sentence_freq))/sd(sentence_freq))

# combine if needed
if (use_eventsrev){
  dat_sentence = rbind(dat_er_sentence, dat_ea_sentence) 
} else {
  dat_sentence = dat_ea_sentence
}
dat_sentence$Plausibility = recode(dat_sentence$Plausibility, 
                             implausible='Implausible', plausible='Plausible',
                             implaus="Implausible", plaus="Plausible") 

# randomly assign plausibility values to AAR, use even/odd heuristic for the rest
dat_sentence$Index = c(1:nrow(dat_sentence))
dat_sentence = dat_sentence %>%
  mutate(Plausibility = ifelse((Index%%2==0), 'Implausible', 'Plausible')) %>%
  select(-Index)
```

## Add synonym info
```{r}
# Items after the new ItemNum 278 do not have synonyms!
dat_sentence = dat_sentence %>%
    mutate(SynonymPair = ifelse(ItemNum>278, NA, (ItemNum+1)%/%2)) %>% 
    mutate(NumSyn = ifelse(ItemNum>278, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) 
```

# Read human data

```{r}

# READ
dirname = '../human_ratings'
ratings_eventsRev = paste(dirname, '1_EventsRev_plausibility', 'analyses', 'longform_data.csv', sep='/')
ratings_eventsAdapt = paste(dirname, '2_EventsAdapt_plausibility', 'analyses', 'longform_data.csv', sep='/')

dat_er = read.csv(ratings_eventsRev) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence) %>%
  filter(ItemNum<41)    # exclude attention checks

dat_ea = read.csv(ratings_eventsAdapt) %>% rename(Score=Answer.Rating) %>%
  rename(Sentence=Input.trial) %>%
  rename(ItemNum=Item) %>%
  select(WorkerId, Score, Plausibility, ItemNum, Sentence, TrialType, Voice)

# CLEAN
dat_er$Plausibility = recode(dat_er$Plausibility, implaus='Implausible', plaus='Plausible')
dat_er$Experiment = 'EventsRev'
dat_er$TrialType = 'AAN'           # can change to a different value to show EventsRev results separately
dat_er$Voice = 'active'
dat_er$ItemNum = dat_er$ItemNum + 900    # so that the numbering doesn't overlap

# arbitrarily assign plausibility labels to AAR
dat_ea$Plausibility = recode(dat_ea$Plausibility, plausible0='Implausible', plausible1='Plausible')    
dat_ea$Plausibility = recode(dat_ea$Plausibility, implausible='Implausible', plausible='Plausible') 
dat_ea$Experiment = 'EventsAdapt'

# combine with EventsRev if needed
if (use_eventsrev){
  human_dat = rbind(dat_er, dat_ea)
} else {
  human_dat = dat_ea
}
human_dat$TrialType = factor(human_dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
```

Initial mean, but to be filtered later on!

```{r pressure, echo=FALSE}
human_dat.mean = human_dat %>% 
  group_by(ItemNum, Sentence, Plausibility, TrialType, Voice, Experiment) %>% #, SynonymPair, NumSyn) %>% 
  summarize(MeanScore = mean(Score, na.rm=TRUE)) %>%
  ungroup() 
human_dat.mean$ItemNum = as.factor(human_dat.mean$ItemNum)

#scale scores

#3.2 Normalize scores for all models
if (grepl("min-max", normalization) == TRUE){
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = min_max(MeanScore))
} else {
  human_dat.mean = human_dat.mean %>%
  mutate(NormScore = scale_this(MeanScore))
}
```

# Read model info

```{r read data, echo=FALSE}
dirname = '../model_scores'

dirname_eventsAdapt = paste(dirname, 'new-EventsAdapt', sep='/')
dirname_eventsRev = paste(dirname, 'EventsRev', sep='/')

message("Loading EventsAdapt data ...")
files_ea = list.files(path=dirname_eventsAdapt, pattern="*.txt")
dat_ea = do.call(rbind, lapply(files_ea, function(x) read_data(dirname_eventsAdapt, x)))

cat("\n")
message("Loading EventsRev data ...")
files_er = list.files(path=dirname_eventsRev, pattern="*.txt")
dat_er = do.call(rbind, lapply(files_er, function(x) read_data(dirname_eventsRev, x)))
dat_sentence$ItemNum = as.factor(dat_sentence$ItemNum)

```

## Merge with sentence info

Load & merge with original sentence file for information about TrialType & Voice

NOTE: Some of the sentences were different from the human experiment! >> take from dropbox, NOT behavioral study!
Maybe we should also just exclude these sentences
The file we use is downloaded from Dropbox and named here as 'newsentences_EventsAdapt_smallfixes.csv' to avoid confusion with the 'newsentences_EventsAdapt.csv' file in the behavioral study folder

ANYA: currently excluding all the sentences where the match isn't exact
```{r read data, echo=FALSE}
dat_ea = dat_ea %>% select(-ItemNum, -Plausibility, -SentenceNum)
dat_ea = merge(dat_ea, dat_sentence, by=c("Sentence"))
dat_ea$Experiment = 'EventsAdapt'

# check which sentences don't have a plausibility pair
sentence_pairnum = dat_ea %>%
  group_by(ItemNum, TrialType, Voice, Metric) %>%
  summarize(NumSentences = length(ItemNum))
single_sentences = sentence_pairnum %>%
  filter(NumSentences==1)

# filter them out
dat_ea = dat_ea %>%
  filter(!(ItemNum %in% single_sentences$ItemNum))
```


## EventsRev (combine if needed)
```{r read data, echo=FALSE}

if (use_eventsrev == TRUE){
  dat_er = merge(dat_er, dat_sentence, by=c("Sentence"))
  dat_er$Experiment = 'EventsRev'
  dat_er$TrialType = 'AAN'          
  dat_er$Voice = 'active'
  
  dat_er = dat_er %>%
    select(-SentenceNum) %>%
    select(-ItemNum.x) %>%
    select(-Plausibility.x) %>%
    rename(ItemNum=ItemNum.y) %>%
    rename(Plausibility=Plausibility.y)

  # check which sentences don't have a plausibility pair
  sentence_pairnum = dat_er %>%
    group_by(ItemNum, TrialType, Voice, Metric) %>%
    summarize(NumSentences = length(ItemNum))
  single_sentences = sentence_pairnum %>%
    filter(NumSentences==1)

  # filter them out
  dat_er = dat_er %>%
    filter(!(ItemNum %in% single_sentences$ItemNum))
  dat = rbind(dat_er, dat_ea)
} else if (use_eventsrev == FALSE){
  dat = dat_ea
} else {
  stop("Please specify whether to use EventsRev for analysis or not!")
}

dat$TrialType = factor(dat$TrialType, levels=c('AI', 'AAN', 'AAR'))
```

## Add human data
```{r}
# only include sentences present in model data
human_dat.mean = human_dat.mean %>%
  filter(ItemNum %in% unique(dat$ItemNum)) %>%
  select(Sentence, Experiment, MeanScore, NormScore) %>%
  rename(Score=MeanScore)
human_dat.mean = merge(human_dat.mean, dat_sentence, by=c("Sentence"))

# Use ItemNum from model data
dat_sentencenum = dat %>%
  group_by(Sentence) %>%
  summarize(ItemNum = ItemNum[1])
human_dat.mean = merge(human_dat.mean %>% select(-ItemNum), dat_sentencenum)

# combine
human_dat.mean$Metric = "human"
dat = rbind(dat, human_dat.mean)
```


# Select which models/metrics to plot

NOTE: the current version will fail to include all ANNs, might need to fix

```{r, echo=FALSE}

human <- c("human")
anns <- c("bert","roberta")

unique_metrics = unique(dat$Metric)
mlm_all <- unique_metrics[grepl(paste(anns, collapse = "|"), unique_metrics)]
mlm_all = mixedsort(mlm_all)

mlm_all <- c(human,mlm_all)

# select which to use
chosen_models = eval(parse(text=which_models))
model_dat = dat %>% filter(Metric %in% chosen_models)

model_dat$Metric = factor(model_dat$Metric, levels = mlm_all)

message("Using these models/metrics:")
print(unique(model_dat$Metric))
```

# PLAUSIBILITY

We need to define for which models a higher score for plausible sentences is desirable:

1. HIGHER = MORE PLAUSIBLE
    * ANNs >> metric is probability
    * PPMI >> higher PMI indicates a plausible sentence
    * Vector similarity
    * TFit similarity (sum of vector similarities to prototype vector)
    * SDM (sum of some vector similarities)
2. LOWER = MORE PLAUSIBLE
    * Surprisal

```{r pressure, echo=FALSE}

lower_better = c("surprisal")
lower_better_pat <- paste(lower_better, collapse = '|')

# Add FinalNormScore for NormScores plotting in the same direction for all metrics (i.e., Plausible is more positive!)
# Take out/plot NormScore below if confusing!
model_dat = model_dat %>%
  mutate(LowerBetter = ifelse(grepl(lower_better_pat, Metric),TRUE,FALSE)) %>%
  mutate(FinalNormScore = ifelse(LowerBetter==TRUE, -NormScore+1, NormScore))
```

# Sentence exclusion
We have to exclude certain items because there's not a full overlap of stimuli between human and model data. We cannot exclude based on item number or synonym pair number because of general misalignment (some numbers are missing)
The below exclusion keywords are obtained from running the check that finds differences in the datasets for models and humans below

```{r}
exclusion_keywords = c('designer', 
                       'decorator',
                       #
                       'pants',
                       'trousers',
                       #
                       'owner',
                       'proprietor',
                       #
                       'spectators',
                       'audience',
                       #
                       'illusionist',
                       'magician',
                       #
                       'ancestors',
                       'serf',
                       #
                       'dictator',
                       'townspeople',
                       #
                       'runner',
                       'jogger',
                       #
                       'officer',
                       'deputy',
                       #
                       'hunter',
                       'perpetrator',
                       #
                       'closed', # doesn't come with a synonym
                       'cushion', # doesn't come with a synonym
                       'pullover',  # doesn't come with a synonym
                       'soda', # doesn't come with a synonym
                       'woodworker', # doesn't come with a synonym
                       'target', # doesn't come with a synonym
                       'pine',  # doesn't come with a synonym
                       'smuggler',  # doesn't come with a synonym
                       'fired',  # doesn't come with a synonym
                       'beverage', #7 doesn't come with a synonym
                       'box-office', #282 no synonym
                       'counselor', #293 no synonym
                       'laundress', #306 no synonym
                       'twins', #328 no synonym
                       'fashionista', #379 no synonym
                       'sportsstar', #922 no synonym
                       'applauding' #927 no synonym
                       )
```

## exclude from dataset
```{r}
#exclude sentences based on those that are not shared between models and humans in the dataset
message(paste(length(unique(model_dat$Sentence))))
model_dat = model_dat %>% 
  filter(!grepl(paste(exclusion_keywords, collapse = "|"), Sentence))
message(paste(length(unique(model_dat$Sentence))))

#order by itemNum for correct assignment
model_dat = arrange(model_dat, ItemNum)

models = unique(model_dat$Metric)
df = NA

for(i in seq_along(models)){ 
  print(paste(models[i]))
  curr_dataset = subset(model_dat, Metric == models[i])
  # add info about synonym pairs for dat_ea items
  num_EA = length(subset(curr_dataset, Experiment=='EventsAdapt')$Sentence) %/% 4
  print(paste(num_EA, ',', num_EA%*%4))
  if (use_eventsrev){
    num_ER = length(subset(curr_dataset, Experiment=='EventsRev')$Sentence) %/% 2
    print(paste(num_ER, ',', num_ER%*%2))
    itemNums_forSynsets <- c(rep(1:num_EA, each=4), rep(901:(900+num_ER), each=2))
  } else {
    itemNums_forSynsets <- c(rep(1:num_EA, each=4))
  }
  
  curr_dataset = curr_dataset %>%
    rename(ItemNum_OLD=ItemNum) %>%
    mutate(ItemNum = itemNums_forSynsets) %>%
    mutate(SynonymPair = ifelse(ItemNum>248, NA, (ItemNum+1)%/%2)) %>% #changed from 900 to 248 CK
    mutate(NumSyn = ifelse(ItemNum>248, NA, paste('Version', (ItemNum+1)%%2+1, sep=""))) #changed from 900 to 248 CK

  if (is.na(df)){
    df = curr_dataset
  } else {
    df = merge(df,curr_dataset, all=TRUE)
  }
}

write.csv(x=df, file="clean_eventsAdapt_df.csv")
```

# CONTRASTS

```{r}
#dat = read.csv('dat_all_models_EventsAdaptRev.csv')

dat = model_dat
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

dat$Voice = as.factor(dat$Voice)
contrasts(dat$Voice) = c(0.5, -0.5)
colnames(attr(dat$Voice, "contrasts")) = c("A>P")

dat$TrialType = factor(dat$TrialType, levels=c("AAR", "AAN", "AI"))    # dummy coding by default
dat$Metric = as.factor(dat$Metric)
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
models_metrics = chosen_models
nr_models = length(models_metrics)
ncols=5
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

dat.binchoice = dat %>%
  mutate(Metric = as.character(Metric)) %>%
  #add color
  mutate(Category = ifelse(startsWith(Metric, "bert"), "BERT", "RoBERTa")) %>%
  mutate(Category = ifelse(Metric=="human", "human", Category)) %>%
  #create filter criterion
  group_by(ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"]) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup()

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=models_metrics)
dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "BERT", "RoBERTa"))
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("AI", "AAN", "AAR"))
```

### Stats

```{r}
calculate_binom_pval <- function(numCorrect, numTotal) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ binom.test(numCorrect[i], numTotal[i])$p.value}))
}

# get p values
dat.binchoice.summary = dat.binchoice %>%
  filter(TrialType %in% c("AI", "AAN")) %>%
  group_by(Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))

# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel = ifelse(pValAdjusted<0.001, "***", 
                         ifelse(pValAdjusted<0.01, "**",
                                ifelse(pValAdjusted<0.05, "*", "")))) %>%
  ungroup()
```
```{r}
# get the X-square and p-value of the X-square 2 sample test. Assume the total num is the same
calculate_chisq <- function(a, b, total) {
  result = prop.test(c(a, total), c(b, total))
  return(c(as.numeric(result$statistic), result$p.value))
}
calculate_chisq_vectorized_chi <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[1]}))
}
calculate_chisq_vectorized_p <- function(numCorrect, numTotal, num_correct_human) {
  return(sapply(c(1:length(numCorrect)), 
                function(i){ calculate_chisq(numCorrect[i], num_correct_human[i], numTotal[i])[2]}))
}
# get human responses separately
human.results = dat.binchoice.summary %>%
  filter(Metric=='human') %>%
  select(TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)
dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = ifelse(pVal2humansAdjusted<0.001, "***", 
                         ifelse(pVal2humansAdjusted<0.01, "**",
                                ifelse(pVal2humansAdjusted<0.05, "*", "")))) 
```

### Plot 

CARINA: There's a slight difference between the length-normalized score GPT2 model and the unnormalized model: For AAN, 5 more sentences are correctly classified by the length-normalized model than the unnormalized one. Why?

"However, PPPLs are only comparable under the same subword vocabulary, which differs between e.g., BERT and RoBERTa. Normalizing with N as the number of *words* mitigates this. " from Salazar et al. 2020

Q: Should we normalize by #words instead?

```{r, echo=FALSE}

dat.binchoice$Metric = factor(dat.binchoice$Metric, levels = mlm_all)
dat.binchoice.summary$Metric = factor(dat.binchoice.summary$Metric, levels = mlm_all)

ggplot(data=subset(dat.binchoice, TrialType%in%c("AI","AAN")), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 1)+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun.y='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x='Model')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

savename <- "1_binaryAccuracy_split.png"
ggsave(paste(savedir,savename,sep="/"), width=15, height=12, units='cm')
```


