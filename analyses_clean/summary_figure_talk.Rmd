---
title: "results_main"
output: html_document
---

# SETUP

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(grid)
library(gridExtra)
library(operator.tools)
library(lme4)
library(lmerTest)
library(cocor)
library(patchwork)
library(gtools)
library(cowplot)
library(png)
library(RColorBrewer)

source('dataloader_utils.R') #includes normalizations, read_data functions
source('stats_utils.R')

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
```

### Print environment variables 
```{r}
#SET ENVIRONMENT VARIABLES

# can be: "EventsAdapt", "DTFit_EventsRev" "DTFit_EventsRev_EventsAdapt"
experiment <- "DTFit_EventsRev_EventsAdapt"

#can be:
# llms_main = human + main LLMs
# all_main = human + main LLMs + baselines
# baselines = human + baselines
# llms_all = human + all MLMs 
which_models <- "all_main" 

#can be "min-max", "zscore", "none"
normalization_type <- "min-max" 

path <- paste("results/")
ifelse(!dir.exists(path), dir.create(path), FALSE)
savedir <- paste(path,experiment,"_Models=",which_models,sep='')
ifelse(!dir.exists(savedir), dir.create(savedir), FALSE)

print(paste(savedir))
message("Running with the following environment variables:")
message(paste("which_models: ", which_models))
message(paste("normalization: ", normalization_type))

#for plotting the dotted reference line
if (grepl("min-max", normalization_type) == TRUE){
  reference_value = 0.5
} else {
  reference_value = 0
}
normalization <- get_normalization_fn(normalization_type)
```

# READ DATA
```{r}
#created via preprocess_scores.Rmd
if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
  ER_dat = read.csv('clean_data/clean_EventsRev_df.csv') %>%
    mutate(Voice = "active") %>%
    mutate(TrialType = "AA,unlikely") %>%
    mutate(SynonymPair = NA) %>%
    mutate(NumSyn = NA)
  DT_dat = read.csv('clean_data/clean_DTFit_df.csv') %>%
    mutate(Voice = "active") %>%
    mutate(TrialType = "AI,unlikely") %>%
    mutate(SynonymPair = NA) %>%
    mutate(NumSyn = NA)
  EA_dat = read.csv('clean_data/clean_EventsAdapt_df.csv')
  dat = rbind(EA_dat, DT_dat, ER_dat)
} else if (grepl("EventsAdapt", experiment)) {
  dat = read.csv('clean_data/clean_EventsAdapt_df.csv')
  human_dat = read.csv('clean_data/clean_EventsAdapt_human_dat.csv')
} else {
  dat = rbind(read.csv('clean_data/clean_EventsRev_df.csv'), read.csv('clean_data/clean_DTFit_df.csv'))
  #human_dat = read.csv('clean_data/clean_EventsRev_human_dat.csv')
  #human_dat$TrialType = "AAN"
}

```

# Select which models/metrics to plot

```{r, utils.choose_models, echo=FALSE}
human <- c("human")
llms_main <- c("RoBERTa-large.sentence-PLL", "BERT-large.sentence-PLL", "GPT-J.sentence-LL", "GPT-2-xl.sentence-LL")
baselines <- c("tinyLSTM.surprisal","SDM", "thematicFit.prod", "syntax-PPMI")

llms_main <- c(human, llms_main)
all_main <- c(llms_main, baselines)
baselines <- c(human, baselines)

all_metrics = unique(dat$Metric) 
llm_metrics = as.character(all_metrics[str_detect(all_metrics, 'BERT|GPT')])
llms_all <- c(human,llm_metrics)

# select which to use
chosen_models = eval(parse(text=which_models))
dat = dat %>% filter(Metric %in% chosen_models)

if (grepl("llms_all", which_models) == FALSE){
# shorten model names
llms_main_short <- c("RoBERTa", "BERT", "GPT-J", "GPT-2")
baselines_short <- c("tinyLSTM","SDM", "thematicFit", "syntax-PPMI")
models_short_order = c(human, llms_main_short, baselines_short)

shorten_metric_names <- function(col) { gsub("-large|-xl", "", col) }

dat = dat %>%
  mutate(MetricModel = Metric, Metric=as.character(Metric)) %>%
  separate(Metric, into=c("Metric"), sep="\\.", extra="drop") %>%
  mutate(Metric = shorten_metric_names(Metric)) 
chosen_models = unique(dat$Metric)
chosen_models = chosen_models[order(match(chosen_models,models_short_order))]
dat$Metric = factor(dat$Metric, levels = chosen_models)
}else{
  dat$Metric = relevel(dat$Metric, ref="human")
  chosen_models = levels(dat$Metric)
}
message("Using these models/metrics:")
print(chosen_models)
```

## Avg word frequency

```{r}
dat.sentfreq = dat %>% 
  filter(Voice=='active') %>%
  select(TrialType, Sentence, sentence_freq) %>%
  distinct() %>%
  group_by(TrialType) %>%
  summarize(avg_sent_freq = mean(sentence_freq))
```

# CONTRASTS
```{r}
dat$Plausibility = factor(dat$Plausibility, levels=c("Plausible", "Implausible")) # dummy coding by default

if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
  dat$TrialType = factor(dat$TrialType, levels=c("AI,unlikely", "AA,unlikely", "AAN", "AAR", "AI"))    # dummy coding by default
  dat$Voice = as.factor(dat$Voice)
  contrasts(dat$Voice) = c(0.5, -0.5)
  colnames(attr(dat$Voice, "contrasts")) = c("A>P")
} else if (grepl("EventsAdapt", experiment)) {
  dat$TrialType = factor(dat$TrialType, levels=c("AAN", "AAR", "AI"))    # dummy coding by default
  dat$Voice = as.factor(dat$Voice)
  contrasts(dat$Voice) = c(0.5, -0.5)
  colnames(attr(dat$Voice, "contrasts")) = c("A>P")
} else {
  dat$TrialType = factor(dat$TrialType, levels=c("DTFit", "AAN")) 
}
dat = within(dat, Metric <- relevel(Metric, ref = "human"))    # set humans as the reference 
dat$ItemNum = as.factor(dat$ItemNum)
```


# BINARY ACCURACY

## General plotting settings

```{r}
#Set plotting options for grid plots
nr_models = length(chosen_models)
ncols=nr_models #models plus human
nrows=round(nr_models/ncols)
if (nrows * ncols < nr_models) {
  nrows = nrows + 1
}

cortext_size = 4
```

```{r, echo=FALSE, fig.height=12, fig.width=15}

#add Category color
if (which_models == "llms_all") {
  dat.binchoice = dat %>%
    mutate(Metric = as.character(Metric)) %>%
    mutate(Category = ifelse(startsWith(Metric, "BERT"), "BERT", "RoBERTa")) %>%
    mutate(Category = ifelse(startsWith(Metric, "GPT"), "GPT", Category)) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category))
} else {
  dat.binchoice = dat %>%
    mutate(Category = ifelse(Metric%in%llms_main_short, "LLMs", "baselines")) %>%
    mutate(Category = ifelse(Metric=="human", "human", Category)) 
}

dat.binchoice = dat.binchoice %>%
  group_by(Experiment, ItemNum, TrialType, Voice, Metric, LowerBetter, Category) %>%
  summarize(ScoreDiff = NormScore[Plausibility=="Plausible"]-NormScore[Plausibility=="Implausible"],
            Sentence=Sentence) %>%
  mutate(FinalScoreDiff = ifelse(LowerBetter==TRUE, -ScoreDiff, ScoreDiff)) %>%
  mutate(Accuracy = ifelse(FinalScoreDiff>0, 1, 0)) %>%
  ungroup()

# specify order for plotting
dat.binchoice$Metric = factor(dat.binchoice$Metric, levels=chosen_models)
dat.binchoice$TrialType = factor(dat.binchoice$TrialType, levels=c("AI", "AAN", "AAR", "AI,unlikely", "AA,unlikely"))
dat.binchoice$Experiment = factor(dat.binchoice$Experiment, levels=c("EventsAdapt", "DTFit", "EventsRev"))

if (which_models=="llms_all") {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "RoBERTa", "BERT", "GPT"))
} else {
  dat.binchoice$Category = factor(dat.binchoice$Category, levels=c("human", "LLMs", "baselines"))
}
```


```{r}
# leave only active sentences here (since SDM & PPMI-syntax cannot deal with passive structures, put full plot in SI)
dat.binchoice.active = dat.binchoice %>% filter(Voice=="active")
```

### Stats

```{r}
# get p values
  
if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
    dat.binchoice.summary = dat.binchoice.active %>%
      filter(!(TrialType=="AAR")) %>%
      group_by(Experiment, Category, TrialType) %>%
      summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
      mutate(AccuracyScore = NumCorrect/NumTotal) %>%
      ungroup() %>%
      mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))
    
} else {
  dat.binchoice.summary = dat.binchoice.active %>%
  filter(!(TrialType=="AAR")) %>%
  group_by(Experiment, Category, Metric, TrialType) %>%
  summarize(NumCorrect=sum(Accuracy), NumTotal=length(Accuracy)) %>%
  mutate(AccuracyScore = NumCorrect/NumTotal) %>%
  ungroup() %>%
  mutate(pVal = calculate_binom_pval(NumCorrect, NumTotal))
}
# adjust for multiple comparisons within each category
dat.binchoice.summary = dat.binchoice.summary %>%
  group_by(Category) %>%
  mutate(pValAdjusted = p.adjust(pVal, method="fdr", n=length(pVal)),
         ntoadjust = length(pVal)) %>%
  mutate(pLabel= plabel(pValAdjusted)) %>%
  ungroup()
```

```{r}
# get human responses separately > WHY?
human.results = dat.binchoice.summary %>%
  filter(Category=='human') %>%
  select(Experiment, TrialType, NumCorrect, NumTotal) %>%
  rename(NumCorrectHuman=NumCorrect, NumTotalHuman=NumTotal)

if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) { #compared to 4 LLMs/BMs (now all grouped into one category)
  human.results = human.results %>%
    mutate(NumCorrectHuman = 4*NumCorrectHuman) %>%
    mutate(NumTotalHuman = 4*NumTotalHuman)
}

dat.binchoice.summary.withchisq = merge(dat.binchoice.summary, human.results)
dat.binchoice.summary.withchisq = dat.binchoice.summary.withchisq %>%
  mutate(ChiSq = calculate_chisq_vectorized_chi(NumCorrect, NumTotal, NumCorrectHuman),
         pVal2humans = calculate_chisq_vectorized_p(NumCorrect, NumTotal, NumCorrectHuman)) %>%
  group_by(Experiment, Category) %>%
  mutate(pVal2humansAdjusted = p.adjust(pVal2humans, method="fdr", n=length(pVal2humans)),
         ntoadjust = length(pVal2humans)) %>%
  mutate(pLabel2humans = plabel(pVal2humansAdjusted)) 

if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
    # print the result
  for (i in seq_along(dat.binchoice.summary.withchisq$Category)) {
    print(paste(dat.binchoice.summary.withchisq$Category[i], ": ",
          round(dat.binchoice.summary.withchisq$AccuracyScore[i],2), 
          ", χ2=", round(dat.binchoice.summary.withchisq$ChiSq[i],2), 
          ", p=", round(dat.binchoice.summary.withchisq$pVal2humansAdjusted[i],3),
          ";", sep=""))
  }
} else {
  # print the result
  for (i in seq_along(dat.binchoice.summary.withchisq$Metric)) {
    print(paste(dat.binchoice.summary.withchisq$Metric[i], ": ",
          round(dat.binchoice.summary.withchisq$AccuracyScore[i],2), 
          ", χ2=", round(dat.binchoice.summary.withchisq$ChiSq[i],2), 
          ", p=", round(dat.binchoice.summary.withchisq$pVal2humansAdjusted[i],3),
          ";", sep=""))
  }
}
```

### Plot

```{r}
label_names <- c(
  "AI,unlikely" = "animate-inanimate,\nunlikely",
  "AA,unlikely" = "animate-animate,\nunlikely (easy)",
  #"DTFit" = "animate-inanimate, unlikely",
  "AI" = "animate-inanimate,\nimpossible",
  "AAN" = "animate-animate,\nunlikely",
  "AAR" = "animate-animate\n(control)"
)
```

```{r, echo=FALSE}

if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
  dat.binchoice.active$TrialType = factor(dat.binchoice.active$TrialType, levels=c("AI", "AAR", "AI,unlikely", "AA,unlikely", "AAN"))
  dat.binchoice.summary$TrialType = factor(dat.binchoice.summary$TrialType, levels=c("AI", "AAR", "AI,unlikely", "AA,unlikely", "AAN"))
  
  label_names_cat <- c(
  "human" = "human",
  "LLMs" = "LLM (average)",
  "baselines" = "baseline (average)"
  )
  
  dat.binchoice.active = dat.binchoice.active %>%
    mutate(Category=recode(Category, !!!label_names_cat))
  dat.binchoice.summary = dat.binchoice.summary %>%
    mutate(Category=recode(Category, !!!label_names_cat))
  
  plot.binacc = ggplot(data=subset(dat.binchoice.active, !(TrialType=="AAR")), 
         mapping=aes(x=Category, y=Accuracy, fill=Category))+
    facet_wrap(~TrialType, ncol = 4, labeller = as_labeller(label_names))+
    geom_hline(yintercept=1, color='gray50', linetype='dotted')+
    stat_summary(geom='col', fun='mean',
                 color='black', width=0.8)+
    stat_summary(geom='errorbar', fun.data='mean_se',
                 color = 'black', size = 0.5, width=0.1)+
    geom_text(mapping=aes(x=Category, y=0.05, label=pLabel), data=dat.binchoice.summary)+
    coord_cartesian(ylim=c(0.45,1))+
    geom_hline(yintercept=.5, linetype='dotted')+
    theme_classic()+
    labs(x=NULL)+
    theme(axis.text.x=element_blank(),
          axis.title = element_text(size = 9),
          axis.title.x=element_blank(),
          axis.ticks.x=element_blank())
} else {
  plot.binacc = ggplot(data=subset(dat.binchoice.active, !(TrialType=="AAR")), 
       mapping=aes(x=Metric, y=Accuracy, fill=Category))+
  facet_wrap(~TrialType, ncol = 2, labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))
}
  plot.binacc

if (grepl("DTFit_EventsRev_EventsAdapt", experiment)) {
  savename <- "1_binaryAccuracy_averagePerCategory.png"
  ggsave(paste(savedir,savename,sep="/"), width=30, height=7, units='cm')
} else{
  savename <- "1_binaryAccuracy_split.png"
  ggsave(paste(savedir,savename,sep="/"), width=15, height=9, units='cm')
}
```

```{r}
# source: https://www.markhw.com/blog/logos

get_png <- function(filename) {
  grid::rasterGrob(png::readPNG(filename), interpolate = TRUE)
}

img_ai <- get_png("./animate-inanimate.png")
img_aa <- get_png("./animate-animate.png")

(p3 <- ggplot(mapping = aes(x = 0:1, y = 1)) +
  theme_void() +
  annotation_custom(img_ai, xmin = 0.055, xmax = .155) +
  annotation_custom(img_ai, xmin = .26, xmax = .36) +
  annotation_custom(img_aa, xmin = .46, xmax = .56) +
  annotation_custom(img_aa, xmin = .67, xmax = .77))

plot.binacc.logos <- gridExtra::grid.arrange(p3, plot.binacc, heights = c(.1, .9))

savename <- "1_binaryAccuracy_averagePerCategory_logos.png"
ggsave(paste(savedir,savename,sep="/"), width=20, height=8, units='cm', plot.binacc.logos)
```

```{r}

dat.binchoice.active = subset(dat.binchoice.active, !(TrialType=="AAR"))

dat.binchoice.active$TrialType = factor(dat.binchoice.active$TrialType, levels=c("AI", "AAR", "AI,unlikely", "AA,unlikely", "AAN"))
#dat.binchoice.summary$TrialType = factor(dat.binchoice.summary$TrialType, levels=c("AI", "AAR", "AI,unlikely", "AA,unlikely", "AAN"))

label_names_tt <- c(
  "AI,unlikely" = "animate-inanimate, unlikely",
  "AA,unlikely" = "animate-animate, unlikely (easy)",
  #"DTFit" = "animate-inanimate, unlikely",
  "AI" = "animate-inanimate, impossible",
  "AAN" = "animate-animate, unlikely",
  "AAR" = "animate-animate\n(control)"
)

dat.binchoice.active = dat.binchoice.active %>%
  mutate(TrialType=recode(TrialType, !!!label_names_tt))

label_names <- c(
  "animate-animate" = "animate-animate",
  "animate-inanimate" = "animate-inanimate",
  "human" = "human",
  "average LLM model" = "average LLM model",
  "average baseline model" = "average baseline model"
)
```

```{r}
dat.binplot = dat.binchoice.active %>%
  mutate(Animacy = ifelse(grepl("animate-animate", dat.binchoice.active$TrialType), "animate-animate", "animate-inanimate"))
dat.binplot$Animacy = factor(dat.binplot$Animacy, levels=c("animate-inanimate", "animate-animate"))

mypalette = c(brewer.pal(6, "Paired")[2],brewer.pal(6, "Paired")[1], brewer.pal(6, "Paired")[6], brewer.pal(6, "Paired")[5])
plot.binacc2 = ggplot(data=dat.binplot, 
       mapping=aes(x=TrialType, y=Accuracy, fill=TrialType))+
  facet_nested(~Animacy+Category, scales = "free", labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  #geom_text(mapping=aes(x=Category, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))+
  scale_fill_manual(values=mypalette, name = "Dataset")+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
  #scale_fill_brewer(palette="Paired")+
  #scale_x_discrete(labels=c("impossible","unlikely","unlikely (easy)","unlikely"))
plot.binacc2
savename <- "1_binaryAccuracy_averagePerCategory_AnimacyXCategory.png"
ggsave(paste(savedir,savename,sep="/"), width=30, height=10, units='cm')
```
### Add logos
```{r}
# source: https://www.markhw.com/blog/logos

(p3 <- ggplot(mapping = aes(x = 0:1, y = 1)) +
  theme_void() +
  annotation_custom(img_ai, xmin = .15, xmax = .25) +
  annotation_custom(img_aa, xmin = .57, xmax = .67))

plot.binacc.logos <- gridExtra::grid.arrange(p3, plot.binacc2, heights = c(.075, .925))

savename <- "1_binaryAccuracy_averagePerCategory_CategoryXAnimacy_logos.png"
ggsave(paste(savedir,savename,sep="/"), width=30, height=10, units='cm', plot.binacc.logos)
```


```{r}
plot.binacc3 = ggplot(data=dat.binplot, 
       mapping=aes(x=TrialType, y=Accuracy, fill=TrialType))+
  facet_nested(~Category+Animacy, scales = "free")+#, labeller = as_labeller(label_names))+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
  #geom_text(mapping=aes(x=Category, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))+
  scale_fill_brewer(palette="Set2")
plot.binacc3
savename <- "1_binaryAccuracy_averagePerCategory_CategoryXAnimacy.png"
ggsave(paste(savedir,savename,sep="/"), width=30, height=10, units='cm')
```

```{r}
dat.mean = dat.binchoice %>%
  filter(Voice=="active", !(TrialType=="AAR")) %>%
  group_by(Category, TrialType) %>%
  summarize(Accuracy=mean(Accuracy))

  plot.binacc4 = ggplot(data=dat.mean,
       mapping=aes(x=TrialType, y=Accuracy, fill=Category))+
  facet_wrap(~Category, nrow=1)+
  geom_hline(yintercept=1, color='gray50', linetype='dotted')+
  stat_summary(geom='col', fun='mean',
               color='black', width=0.8)+
  stat_summary(geom='errorbar', fun.data='mean_se',
               color = 'black', size = 0.5, width=0.1)+
#  geom_text(mapping=aes(x=Metric, y=0.05, label=pLabel), data=dat.binchoice.summary)+
  coord_cartesian(ylim=c(0,1))+
  geom_hline(yintercept=.5, linetype='dotted')+
  theme_classic()+
  labs(x=NULL)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 9))
  plot.binacc4
#savename <- "1_binaryAccuracy_averagePerCategory_CategoryXAnimacy.png"
#ggsave(paste(savedir,savename,sep="/"), width=30, height=10, units='cm')
```